{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a3f834-60b7-4c21-bfb4-ad16d30fd3f7",
   "metadata": {},
   "source": [
    "# Amazon Comprehend Moderation Chain\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c4236d8-4054-473d-84a4-87a4db278a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /Users/pijain/projects/langchain-dev/langchain/.venv/lib/python3.9/site-packages (from nltk) (8.1.6)\n",
      "Collecting joblib (from nltk)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/fe/61/3e64c3a4eb93d70d09e1f32185bbc9c15631d294471835f542222bd5a01d/regex-2023.8.8-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/pijain/projects/langchain-dev/langchain/.venv/lib/python3.9/site-packages (from nltk) (4.65.0)\n",
      "Downloading regex-2023.8.8-cp39-cp39-macosx_11_0_arm64.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.3.2 nltk-3.8.1 regex-2023.8.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain\n",
    "%pip install nltk\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82190723-5f4e-404d-8c01-251577f48fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8518ad-c762-413c-b8c9-f1c211fc311d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove this - this is for Private Preview APIs only\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "#os.environ['AWS_DATA_PATH'] = 'botodata'\n",
    "comprehend_client = boto3.client('comprehend', region_name='us-east-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2ded1-1bb2-415b-9144-996c155f7127",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> This is a Beta Cookbook. </div>\n",
    "\n",
    "Copy the `langchain_cm` directory to your project's workspace or where your Python Notebook resides. \n",
    "\n",
    "Install `langchain`, `nltk`, and `boto3` if you do not have them installed already. Import the Amazon Comprehend Moderation modules into your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74550d74-3c01-4ba7-ad32-ca66d955d001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import AmazonComprehendModerationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c338b-de9f-40e5-9295-93c9e26058e3",
   "metadata": {},
   "source": [
    "Initialize an instance of the Amazon Comprehend Moderation Chain to be used with your LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde58cc6-ff83-493a-9aed-93d755f984a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comprehend_moderation = AmazonComprehendModerationChain(client=comprehend_client, #optional\n",
    "                                                        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad646d01-82d2-435a-939b-c450693857ab",
   "metadata": {},
   "source": [
    "Use it with your LLM chain. _Note_: The example below uses the _Fake LLM_ from LangChain as an example usage, same concept applies to all other LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0efa1946-d4a9-467a-920a-a8fb78720fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AmazonComprehendModerationChain chain...\u001b[0m\n",
      "Running AmazonComprehendModerationChain...\n",
      "Running pii validation...\n",
      "Found PII content..stopping..\n"
     ]
    },
    {
     "ename": "ModerationPiiError",
     "evalue": "The prompt contains PII entities and cannot be processed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModerationPiiError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(prompt\u001b[39m=\u001b[39mprompt, llm\u001b[39m=\u001b[39mllm)\n\u001b[1;32m     18\u001b[0m chain \u001b[39m=\u001b[39m prompt \u001b[39m|\u001b[39m comprehend_moderation \u001b[39m|\u001b[39m {llm_chain\u001b[39m.\u001b[39minput_keys[\u001b[39m0\u001b[39m]: \u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m] }  \u001b[39m|\u001b[39m llm_chain \u001b[39m|\u001b[39m { \u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] } \u001b[39m|\u001b[39m comprehend_moderation \n\u001b[0;32m---> 20\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mA sample SSN number looks like this 123-456-7890. Can you give me some more samples?\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/schema/runnable/base.py:830\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps:\n\u001b[0;32m--> 830\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m    831\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    832\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m    833\u001b[0m             patch_config(config, run_manager\u001b[39m.\u001b[39;49mget_child()),\n\u001b[1;32m    834\u001b[0m         )\n\u001b[1;32m    835\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/base.py:66\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     61\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     62\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     64\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m     65\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m     67\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m     68\u001b[0m         callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     69\u001b[0m         tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     70\u001b[0m         metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     71\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     72\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    277\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/amazon_comprehend_moderation.py:187\u001b[0m, in \u001b[0;36mAmazonComprehendModerationChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    180\u001b[0m     run_manager\u001b[39m.\u001b[39mon_text(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning AmazonComprehendModerationChain...\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m moderation \u001b[39m=\u001b[39m BaseModeration(client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient, \n\u001b[1;32m    182\u001b[0m                             config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoderation_config, \n\u001b[1;32m    183\u001b[0m                             force_base_exception\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_base_exception,\n\u001b[1;32m    184\u001b[0m                             moderation_callback\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoderation_callback,\n\u001b[1;32m    185\u001b[0m                             unique_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munique_id,\n\u001b[1;32m    186\u001b[0m                             run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 187\u001b[0m response \u001b[39m=\u001b[39m moderation\u001b[39m.\u001b[39;49mmoderate(prompt\u001b[39m=\u001b[39;49minputs[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_keys[\u001b[39m0\u001b[39;49m]])\n\u001b[1;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: response}\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/comprehend_moderation/base_moderation.py:141\u001b[0m, in \u001b[0;36mBaseModeration.moderate\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mexcept\u001b[39;00m ModerationPiiError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message_for_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound PII content..stopping..\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    142\u001b[0m \u001b[39mexcept\u001b[39;00m ModerationToxicityError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message_for_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound Toxic content..stopping..\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/comprehend_moderation/base_moderation.py:112\u001b[0m, in \u001b[0;36mBaseModeration.moderate\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message_for_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning pii validation...\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m pii_validate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_moderation_class(moderation_class\u001b[39m=\u001b[39mComprehendPII)\n\u001b[0;32m--> 112\u001b[0m output_text \u001b[39m=\u001b[39m pii_validate(prompt_value\u001b[39m=\u001b[39;49minput_text) \n\u001b[1;32m    114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message_for_verbose(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning toxicity validation...\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m toxicity_validate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_moderation_class(moderation_class\u001b[39m=\u001b[39mComprehendToxicity)\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/comprehend_moderation/pii.py:31\u001b[0m, in \u001b[0;36mComprehendPII.validate\u001b[0;34m(self, prompt_value, config)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_contains_pii(prompt_value\u001b[39m=\u001b[39mprompt_value, config\u001b[39m=\u001b[39mconfig) \u001b[39mif\u001b[39;00m action \u001b[39m==\u001b[39m BaseModerationActions\u001b[39m.\u001b[39mSTOP \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_pii(prompt_value\u001b[39m=\u001b[39mprompt_value, config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m     30\u001b[0m \u001b[39melse\u001b[39;00m:            \n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_contains_pii(prompt_value\u001b[39m=\u001b[39;49mprompt_value)\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/comprehend_moderation/pii.py:66\u001b[0m, in \u001b[0;36mComprehendPII._contains_pii\u001b[0;34m(self, prompt_value, config)\u001b[0m\n\u001b[1;32m     64\u001b[0m     asyncio\u001b[39m.\u001b[39mcreate_task(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback\u001b[39m.\u001b[39mon_after_pii(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoderation_beacon, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munique_id))\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m pii_found:\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mraise\u001b[39;00m ModerationPiiError \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_base_exception \u001b[39melse\u001b[39;00m BaseModerationError        \n\u001b[1;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m prompt_value\n",
      "\u001b[0;31mModerationPiiError\u001b[0m: The prompt contains PII entities and cannot be processed"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "responses = [\n",
    "             \"Final Answer: A credit card number looks like 1289-2321-1123-2387. A fake SSN number looks like 323-22-9980. John Doe's phone number is (999)253-9876.\", \n",
    "             \"Final Answer: This is a really shitty way of constructing a birdhouse. This is fucking insane to think that any birds would actually create their motherfucking nests here.\"\n",
    "            ]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "chain = prompt | comprehend_moderation | {llm_chain.input_keys[0]: lambda x: x['output'] }  | llm_chain | { \"input\": lambda x: x['text'] } | comprehend_moderation \n",
    "\n",
    "response = chain.invoke({\"question\": \"A sample SSN number looks like this 123-456-7890. Can you give me some more samples?\"})\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da25d96-0d96-4c01-94ae-a2ead17f10aa",
   "metadata": {},
   "source": [
    "## Using `moderation_config` to customize your moderation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd550e7-5012-41fa-9546-8b78ddf1c673",
   "metadata": {},
   "source": [
    "Use Amazon Comprehend Moderation with a configuration to control what moderations you wish to perform. There are three different moderations that happen when no configuration is passed as demonstrated above. These moderations are-\n",
    "\n",
    "- PII (Personally Identifiable Information) checks \n",
    "- Toxicity content detection\n",
    "- Intention detection _(TBD)_\n",
    "\n",
    "For each of these moderation fucntions you can control the behaviour via a configuration object which serves as a `kwargs` for each of the moderation models. Below is an example of specifying a configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a25ae049-79c1-4e53-8431-c168dc549691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import BaseModerationActions, BaseModerationFilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e8900a-44ef-4967-bde8-b88af282139d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moderation_config = { \n",
    "        \"filters\":[ BaseModerationFilters.PII, \n",
    "                    BaseModerationFilters.TOXICITY,\n",
    "                    BaseModerationFilters.INTENT],\n",
    "        \"pii\":{ \"action\": BaseModerationActions.ALLOW, \n",
    "                \"threshold\":0.5, \n",
    "                \"labels\":[\"SSN\"], \n",
    "                \"mask_character\": \"X\"},\n",
    "        \"toxicity\":{ \"action\": BaseModerationActions.STOP, \n",
    "                     \"threshold\":0.5},\n",
    "        \"intent\":{ \"action\": BaseModerationActions.STOP, \n",
    "                   \"threshold\":0.5}\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634376b-5938-43df-9ed6-70ca7e99290f",
   "metadata": {},
   "source": [
    "At the core of the configuration you have three filters specified in the `filters` key-\n",
    "\n",
    "1. `BaseModerationFilters.PII`\n",
    "2. `BaseModerationFilters.TOXICITY`\n",
    "3. `BaseModerationFilters.INTENT`\n",
    "\n",
    "And two actions specified in the `action` key for each moderation function -\n",
    "\n",
    "1. `BaseModerationActions.ALLOW` - `allows` the prompt to pass through but masks detected PII in case of PII check. Allows prompt to pass through except for labels specified in the `labels` key.\n",
    "2. `BaseModerationActions.STOP` - `stops` the prompt from passing through to the next step in case any PII, Toxicity, or incorrect Intent is detected. The action of `BaseModerationActions.STOP` will raise a Python `Exception` essentially stopping the chain in progress.\n",
    "\n",
    "The configuration above specifies the Amazon Comprehend Moderation chain to perform two levels of moderations- The chain will perform PII checks. It will allow the prompt to pass through however it will mask any SSN numbers present in either the prompt or the LLM output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4f7e65-f733-4863-ae6d-34c9faffd849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comp_moderation_with_config = AmazonComprehendModerationChain(moderation_config=moderation_config, #specify the configuration\n",
    "                                                              client=comprehend_client,            #optionally pass the Boto3 Client\n",
    "                                                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25e6f93-765b-4f99-8c1c-929157dbd4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AmazonComprehendModerationChain chain...\u001b[0m\n",
      "Running AmazonComprehendModerationChain...\n",
      "Running pii Validation...\n",
      "Running toxicity Validation...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Comprehend' object has no attribute 'detect_toxic_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(prompt\u001b[39m=\u001b[39mprompt, llm\u001b[39m=\u001b[39mllm)\n\u001b[1;32m     16\u001b[0m chain \u001b[39m=\u001b[39m prompt \u001b[39m|\u001b[39m comp_moderation_with_config \u001b[39m|\u001b[39m {llm_chain\u001b[39m.\u001b[39minput_keys[\u001b[39m0\u001b[39m]: \u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m] }  \u001b[39m|\u001b[39m llm_chain \u001b[39m|\u001b[39m { \u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] } \u001b[39m|\u001b[39m comp_moderation_with_config \n\u001b[0;32m---> 18\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mA sample SSN number looks like this 123-456-7890. Can you give me some more samples?\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/schema/runnable/base.py:830\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps:\n\u001b[0;32m--> 830\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m    831\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    832\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m    833\u001b[0m             patch_config(config, run_manager\u001b[39m.\u001b[39;49mget_child()),\n\u001b[1;32m    834\u001b[0m         )\n\u001b[1;32m    835\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/base.py:66\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     61\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     62\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     64\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m     65\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m     67\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m     68\u001b[0m         callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     69\u001b[0m         tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     70\u001b[0m         metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     71\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     72\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    277\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/amazon_comprehend_moderation.py:187\u001b[0m, in \u001b[0;36mAmazonComprehendModerationChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    180\u001b[0m     run_manager\u001b[39m.\u001b[39mon_text(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning AmazonComprehendModerationChain...\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m moderation \u001b[39m=\u001b[39m BaseModeration(client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient, \n\u001b[1;32m    182\u001b[0m                             config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoderation_config, \n\u001b[1;32m    183\u001b[0m                             force_base_exception\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_base_exception,\n\u001b[1;32m    184\u001b[0m                             moderation_callback\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoderation_callback,\n\u001b[1;32m    185\u001b[0m                             unique_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munique_id,\n\u001b[1;32m    186\u001b[0m                             run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 187\u001b[0m response \u001b[39m=\u001b[39m moderation\u001b[39m.\u001b[39;49mmoderate(prompt\u001b[39m=\u001b[39;49minputs[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_keys[\u001b[39m0\u001b[39;49m]])\n\u001b[1;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: response}\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/comprehend_moderation/base_moderation.py:149\u001b[0m, in \u001b[0;36mBaseModeration.moderate\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/comprehend_moderation/base_moderation.py:132\u001b[0m, in \u001b[0;36mBaseModeration.moderate\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    130\u001b[0m             validation_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_moderation_class(moderation_class\u001b[39m=\u001b[39mfilter_functions[filter_name])                    \n\u001b[1;32m    131\u001b[0m             input_text \u001b[39m=\u001b[39m input_text \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_text \u001b[39melse\u001b[39;00m output_text\n\u001b[0;32m--> 132\u001b[0m             output_text \u001b[39m=\u001b[39m validation_fn(prompt_value\u001b[39m=\u001b[39;49minput_text, \n\u001b[1;32m    133\u001b[0m                                         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[filter_name] \u001b[39mif\u001b[39;49;00m filter_name \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    134\u001b[0m \u001b[39m# convert text to prompt and return\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_text_to_prompt(prompt\u001b[39m=\u001b[39mprompt,text\u001b[39m=\u001b[39moutput_text)\n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/libs/langchain/langchain/chains/comprehend_moderation/toxicity.py:120\u001b[0m, in \u001b[0;36mComprehendToxicity.validate\u001b[0;34m(self, prompt_value, config)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m sentence_list \u001b[39min\u001b[39;00m chunks:\n\u001b[1;32m    119\u001b[0m     segments \u001b[39m=\u001b[39m [ {\u001b[39m\"\u001b[39m\u001b[39mText\u001b[39m\u001b[39m\"\u001b[39m: sentence} \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentence_list ]            \n\u001b[0;32m--> 120\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mdetect_toxic_content(TextSegments\u001b[39m=\u001b[39msegments, LanguageCode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m)    \n\u001b[1;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback\u001b[39m.\u001b[39mtoxicity_callback:\n\u001b[1;32m    122\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoderation_beacon[\u001b[39m'\u001b[39m\u001b[39mmoderation_input\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m segments \n",
      "File \u001b[0;32m~/projects/langchain-dev/langchain/.venv/lib/python3.9/site-packages/botocore/client.py:888\u001b[0m, in \u001b[0;36mBaseClient.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m event_response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mreturn\u001b[39;00m event_response\n\u001b[0;32m--> 888\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    889\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Comprehend' object has no attribute 'detect_toxic_content'"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "responses = [\"Final Answer: A credit card number looks like 1289-2321-1123-2387. A fake SSN number looks like 323-22-9980. John Doe's phone number is (999)253-9876.\", \n",
    "             \"Final Answer: This is a really shitty way of constructing a birdhouse. This is fucking insane to think that any birds would actually create their motherfucking nests here.\"]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "chain = prompt | comp_moderation_with_config | {llm_chain.input_keys[0]: lambda x: x['output'] }  | llm_chain | { \"input\": lambda x: x['text'] } | comp_moderation_with_config \n",
    "\n",
    "response = chain.invoke({\"question\": \"A sample SSN number looks like this 123-456-7890. Can you give me some more samples?\"})\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba890681-feeb-43ca-a0d5-9c11d2d9de3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exception modes, Unique ID, and Moderation Callbacks\n",
    "---\n",
    "\n",
    "In addition to the moderation configuration, the `AmazonComprehendModerationChain` can also be initialized with the following parameters\n",
    "\n",
    "- `force_base_exception` [Optional] a boolean parameter to force the chain to raise `BaseModerationError`, this is a generic moderation error that is raised throughout all moderation checks. This defaults to `False`. When not set to `True` and moderation action is `STOP`, the chain will raise one of the following exceptions-\n",
    "    - `ModerationPiiError`, for PII checks\n",
    "    - `ModerationToxicityError`, for Toxicity checks \n",
    "    - `ModerationIntentionError` for Intent checks\n",
    "- `unique_id` [Optional] a string parameter. This parameter can be used to pass any string value or ID. For example, in a chat application you may want to keep track of abusive users, in this case you can pass the user's username/email id etc. This defaults to `None`.\n",
    "\n",
    "- `moderation_callback` [Optional] the `BaseModerationCallbackHandler` that will be called asynchronously (non-blocking to the chain). Callback functions are useful when you want to perform additional actions when the moderation functions are executed, for example logging into a database, or writing a log file. You can override three functions by subclassing `BaseModerationCallbackHandler` - `on_after_pii()`, `on_after_toxicity()`, and `on_after_intent()`. Note that all three functions must be `async` functions. These callback functions receive two arguments:\n",
    "    - `moderation_beacon` a dictionary that will contain information about the moderation function, the full response from Amazon Comprehend model, a unique chain id, the moderation status, and the input string which was validated. The dictionary is of the following schema-\n",
    "    \n",
    "    ```\n",
    "    { \n",
    "        'moderation_chain_id': 'xxx-xxx-xxx', # Unique chain ID\n",
    "        'moderation_type': 'Toxicity' | 'PII' | 'Intent', \n",
    "        'moderation_status': 'LABELS_FOUND' | 'LABELS_NOT_FOUND',\n",
    "        'moderation_input': 'A sample SSN number looks like this 123-456-7890. Can you give me some more samples?',\n",
    "        'moderation_output': {...} #Full Amazon Comprehend PII, Toxicity, or Intent Model Output\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    - `unique_id` if passed to the `AmazonComprehendModerationChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c178835-0264-4ac6-aef4-091d2993d06c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> <code>moderation_callback</code> is different from LangChain Chain Callbacks. You can still use LangChain Chain callbacks with <code>AmazonComprehendModerationChain</code> via the callbacks parameter. Example: <br/>\n",
    "<pre>\n",
    "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
    "comp_moderation_with_config = AmazonComprehendModerationChain(verbose=True, callbacks=[StdOutCallbackHandler()])\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec38536-8cc9-408e-860b-e4a439283643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import BaseModerationCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be744c7-3f99-4165-bf7f-9c5c249bbb53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define callback handlers by subclassing BaseModerationCallbackHandler\n",
    "\n",
    "class MyModCallback(BaseModerationCallbackHandler):\n",
    "    \n",
    "    async def on_after_pii(self, output_beacon, unique_id):\n",
    "        import json\n",
    "        moderation_type = output_beacon['moderation_type']\n",
    "        chain_id = output_beacon['moderation_chain_id']\n",
    "        with open(f'output-{moderation_type}-{chain_id}.json', 'w') as file:\n",
    "            data = { 'beacon_data': output_beacon, 'unique_id': unique_id }\n",
    "            json.dump(data, file)\n",
    "    \n",
    "    '''\n",
    "    async def on_after_toxicity(output_beacon, unique_id):\n",
    "        pass\n",
    "    \n",
    "    async def on_after_intent(output_beacon, unique_id):\n",
    "        pass\n",
    "    '''\n",
    "\n",
    "my_callback = MyModCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a3fe0-f09f-411e-9df1-d79b3e87510c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moderation_config = { \n",
    "        \"filters\":[ BaseModerationFilters.PII, \n",
    "                    BaseModerationFilters.TOXICITY],\n",
    "        \"pii\":{ \"action\": BaseModerationActions.STOP, \n",
    "                \"threshold\":0.5, \n",
    "                \"labels\":[\"SSN\"], \n",
    "                \"mask_character\": \"X\" },\n",
    "        \"toxicity\":{ \"action\": BaseModerationActions.STOP, \n",
    "                     \"threshold\":0.5 }\n",
    "   }\n",
    "\n",
    "comp_moderation_with_config = AmazonComprehendModerationChain(moderation_config=moderation_config, # specify the configuration\n",
    "                                                              client=comprehend_client,            # optionally pass the Boto3 Client\n",
    "                                                              force_base_exception=True,           # Force BaseModerationError\n",
    "                                                              unique_id='john.doe@email.com',      # A unique ID\n",
    "                                                              moderation_callback=my_callback,     # BaseModerationCallbackHandler\n",
    "                                                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af07937-67ea-4738-8343-c73d4d28c2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "responses = [\"Final Answer: A credit card number looks like 1289-2321-1123-2387. A fake SSN number looks like 323-22-9980. John Doe's phone number is (999)253-9876.\", \n",
    "             \"Final Answer: This is a really shitty way of constructing a birdhouse. This is fucking insane to think that any birds would actually create their motherfucking nests here.\"]\n",
    "\n",
    "llm = FakeListLLM(responses=responses)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "chain = prompt | comp_moderation_with_config | {llm_chain.input_keys[0]: lambda x: x['output'] }  | llm_chain | { \"input\": lambda x: x['text'] } | comp_moderation_with_config \n",
    "\n",
    "response = chain.invoke({\"question\": \"A sample SSN number looks like this 123-456-7890. Can you give me some more samples?\"})\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706454b2-2efa-4d41-abc8-ccf2b4e87822",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `moderation_config` and moderation execution order\n",
    "---\n",
    "\n",
    "If `AmazonComprehendModerationChain` is not initialized with any `moderation_config` then the default action is `STOP` and default order of moderation check is as follows.\n",
    "\n",
    "```\n",
    "AmazonComprehendModerationChain\n",
    "│\n",
    "└──Check PII with Stop Action\n",
    "    ├── Callback (if available)\n",
    "    ├── Label Found ⟶ [Error Stop]\n",
    "    └── No Label Found \n",
    "        └──Check Toxicity with Stop Action\n",
    "            ├── Callback (if available)\n",
    "            ├── Label Found ⟶ [Error Stop]\n",
    "            └── No Label Found\n",
    "                └──Check Intent with Stop Action\n",
    "                    ├── Callback (if available)\n",
    "                    ├── Label Found ⟶ [Error Stop]\n",
    "                    └── No Label Found\n",
    "                        └── Return Prompt\n",
    "```\n",
    "\n",
    "If any of the check raises exception then the subsequent checks will not be performed. If a `callback` is provided in this case, then it will be called for each of the checks that have been performed. For example, in the case above, if the Chain fails due to presence of PII then the Toxicity and Intent checks will not be performed.\n",
    "\n",
    "You can override the execution order by passing `moderation_config` and simply specifying the desired order in the `filters` key of the configuration. In case you use `moderation_config` then the order of the checks as specified in the `filters` key will be maintained. For example, in the configuration below, first Toxicity check will be performed, then PII, and finally Intent validation will be performed. In this case, `AmazonComprehendModerationChain` will perform the desired checks in the specified order with default values of each model `kwargs`.\n",
    "\n",
    "```python\n",
    "moderation_config = { \n",
    "        \"filters\":[ BaseModerationFilters.TOXICITY, \n",
    "                    BaseModerationFilters.PII, \n",
    "                    BaseModerationFilters.INTENT]\n",
    "   }\n",
    "```\n",
    "\n",
    "Model `kwargs` are specified by the `pii`, `toxicity`, and `intent` keys within the `moderation_config` dictionary. For example, in the `moderation_config` below, the default order of moderation is overriden and the `pii` & `toxicity` model `kwargs` have been overriden. For `intent` the chain's default `kwargs` will be used.\n",
    "\n",
    "```python\n",
    " moderation_config = { \n",
    "        \"filters\":[ BaseModerationFilters.TOXICITY, \n",
    "                    BaseModerationFilters.PII, \n",
    "                    BaseModerationFilters.INTENT],\n",
    "        \"pii\":{ \"action\": BaseModerationActions.ALLOW, \n",
    "                \"threshold\":0.5, \n",
    "                \"labels\":[\"SSN\"], \n",
    "                \"mask_character\": \"X\" },\n",
    "        \"toxicity\":{ \"action\": BaseModerationActions.STOP, \n",
    "                     \"threshold\":0.5 }\n",
    "   }\n",
    "```\n",
    "\n",
    "1. For a list of PII labels see Amazon Comprehend Universal PII entity types - https://docs.aws.amazon.com/comprehend/latest/dg/how-pii.html#how-pii-types\n",
    "2. Following are the list of available Toxicity labels-\n",
    "    - `HATE_SPEECH`: Speech that criticizes, insults, denounces or dehumanizes a person or a group on the basis of an identity, be it race, ethnicity, gender identity, religion, sexual orientation, ability, national origin, or another identity-group.\n",
    "    - `GRAPHIC`: Speech that uses visually descriptive, detailed and unpleasantly vivid imagery is considered as graphic. Such language is often made verbose so as to amplify an insult, discomfort or harm to the recipient.\n",
    "    - `HARASSMENT_OR_ABUSE`: Speech that imposes disruptive power dynamics between the speaker and hearer, regardless of intent, seeks to affect the psychological well-being of the recipient, or objectifies a person should be classified as Harassment.\n",
    "    - `SEXUAL`: Speech that indicates sexual interest, activity or arousal by using direct or indirect references to body parts or physical traits or sex is considered as toxic with toxicityType \"sexual\". \n",
    "    - `VIOLENCE_OR_THREAT`: Speech that includes threats which seek to inflict pain, injury or hostility towards a person or group.\n",
    "    - `INSULT`: Speech that includes demeaning, humiliating, mocking, insulting, or belittling language.\n",
    "    - `PROFANITY`: Speech that contains words, phrases or acronyms that are impolite, vulgar, or offensive is considered as profane.\n",
    "3. For a list of Intent labels refer to documentation [link here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78905aec-55ae-4fc3-a23b-8a69bd1e33f2",
   "metadata": {},
   "source": [
    "# Examples\n",
    "---\n",
    "\n",
    "## With HuggingFace Hub Models\n",
    "\n",
    "Get your API Key from Huggingface hub - https://huggingface.co/docs/api-inference/quicktour#get-your-api-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b9627-769b-46ce-8be2-c8a5cf7728ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c0172-2a8c-4a74-b563-2e42f5c39430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7ea98-ad16-4454-8f12-c03c17113a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b235427-cc06-4c07-874b-1f67c2d1f924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "repo_id = \"google/flan-t5-xxl\"  \n",
    "# repo_id = \"amazon/FalconLite\"  #quantized version of the Falcon 40B\n",
    "# repo_id = \"tiiuae/falcon-7b\"\n",
    "# repo_id = \"tiiuae/falcon-40b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86e256-34fb-4c8e-8092-1a4f863a5c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 256}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad603796-ad8b-4599-9022-a486f1c1b89a",
   "metadata": {},
   "source": [
    "Create a configuration and initialize an Amazon Comprehend Moderation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc3409-5be5-433d-b6da-38b9e5c5ee3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import AmazonComprehendModerationChain, BaseModerationActions, BaseModerationFilters\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "os.environ['AWS_DATA_PATH'] = 'botodata'\n",
    "comprehend_client = boto3.client('comprehend', region_name='us-east-2')\n",
    "\n",
    "moderation_config = { \n",
    "        \"filters\":[ BaseModerationFilters.PII, BaseModerationFilters.TOXICITY, BaseModerationFilters.INTENT ],\n",
    "        \"pii\":{\"action\": BaseModerationActions.ALLOW, \"threshold\":0.5, \"labels\":[\"SSN\",\"CREDIT_DEBIT_NUMBER\"], \"mask_character\": \"X\"},\n",
    "        \"toxicity\":{\"action\": BaseModerationActions.STOP, \"threshold\":0.5},\n",
    "        \"intent\":{\"action\": BaseModerationActions.ALLOW, \"threshold\":0.5,},\n",
    "   }\n",
    "\n",
    "# without any callback\n",
    "amazon_comp_moderation = AmazonComprehendModerationChain(moderation_config=moderation_config, \n",
    "                                                         client=comprehend_client,\n",
    "                                                         verbose=True)\n",
    "\n",
    "# with callback\n",
    "amazon_comp_moderation_out = AmazonComprehendModerationChain(moderation_config=moderation_config, \n",
    "                                                         client=comprehend_client,\n",
    "                                                         moderation_callback=my_callback,\n",
    "                                                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1256bc8-1321-4624-9e8a-a2d4a8df59bf",
   "metadata": {},
   "source": [
    "The `moderation_config` will now prevent any inputs and model outputs containing obscene words or sentences, bad intent, or Pii with entities other than SSN with score above threshold or 0.5 or 50%. If it finds Pii entities - SSN - it will redact them before allowing the call to proceed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337becc-7c3c-483e-a55c-a225226cb9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | amazon_comp_moderation | {llm_chain.input_keys[0]: lambda x: x['output'] }  | llm_chain | { \"input\": lambda x: x['text'] } | amazon_comp_moderation_out\n",
    "response = chain.invoke({\"question\": \"My AnyCompany Financial Services, LLC credit card account 1111-0000-1111-0008 has 24$ due by July 31st. Can you give me some more credit car number samples?\"})\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52c7b8-6526-4f68-a2b3-b5ad3cf82489",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## With Amazon SageMaker Jumpstart\n",
    "\n",
    "The exmaple below shows how to use Amazon Comprehend Moderation chain with an Amazon SageMaker Jumpstart hosted LLM. You should have an Amazon SageMaker Jumpstart hosted LLM endpoint within your AWS Account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49d075-bc23-4ab8-a92c-0ddbbc436c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPONT_NAME = \"ENDPOINT_NAME\" # replace with your endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978a5e6-667d-4926-842c-d965f88e5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import load_prompt, PromptTemplate\n",
    "import json\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt,  **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json['generated_texts'][0]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "#prompt template for input text\n",
    "llm_prompt = PromptTemplate(input_variables=[\"input_text\"], template=\"{input_text}\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=SagemakerEndpoint(\n",
    "        endpoint_name=ENDPONT_NAME, \n",
    "        region_name='us-east-1',\n",
    "        model_kwargs={\"temperature\":0.97,\n",
    "                      \"max_length\": 200,\n",
    "                      \"num_return_sequences\": 3,\n",
    "                      \"top_k\": 50,\n",
    "                      \"top_p\": 0.95,\n",
    "                      \"do_sample\": True},\n",
    "        content_handler=content_handler\n",
    "    ),\n",
    "    prompt=llm_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577b036-99a4-47fe-9a8e-4a34aa4cd88d",
   "metadata": {},
   "source": [
    "Create a configuration and initialize an Amazon Comprehend Moderation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859da135-94d3-4a9c-970e-a873913592e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import AmazonComprehendModerationChain, BaseModerationActions, BaseModerationFilters\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "os.environ['AWS_DATA_PATH'] = 'botodata'\n",
    "comprehend_client = boto3.client('comprehend', region_name='us-east-2')\n",
    "\n",
    "moderation_config = { \n",
    "        \"filters\":[ BaseModerationFilters.PII, BaseModerationFilters.TOXICITY ],\n",
    "        \"pii\":{\"action\": BaseModerationActions.ALLOW, \"threshold\":0.5, \"labels\":[\"SSN\"], \"mask_character\": \"X\"},\n",
    "        \"toxicity\":{\"action\": BaseModerationActions.STOP, \"threshold\":0.5},\n",
    "        \"intent\":{\"action\": BaseModerationActions.ALLOW, \"threshold\":0.5,},\n",
    "   }\n",
    "\n",
    "amazon_comp_moderation = AmazonComprehendModerationChain(moderation_config=moderation_config, \n",
    "                                                         client=comprehend_client ,\n",
    "                                                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb191f-7a96-4077-8c30-b9ddc225bd6b",
   "metadata": {},
   "source": [
    "The `moderation_config` will now prevent any inputs and model outputs containing obscene words or sentences, bad intent, or Pii with entities other than SSN with score above threshold or 0.5 or 50%. If it finds Pii entities - SSN - it will redact them before allowing the call to proceed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5aa2a-9c00-42a0-8e24-c5ba39994f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | amazon_comp_moderation | {llm_chain.input_keys[0]: lambda x: x['output'] }  | llm_chain | { \"input\": lambda x: x['text'] } | amazon_comp_moderation \n",
    "response = chain.invoke({\"question\": \"My AnyCompany Financial Services, LLC credit card account 1111-0000-1111-0008 has 24$ due by July 31st. Can you give me some more samples?\"})\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfedf9-1a0a-4a9f-a6b0-d9ed2dbaa5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
