{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use Late Chunk in RAG\n",
    "\n",
    "Based on the [Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models](https://arxiv.org/abs/2409.04701) paper and the [Late Chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models) blog post\n",
    "\n",
    "This notebooks explains how the `Late chunk Embedding` support with `LangChain`.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- Can combine with any `text splitting` used in LangChain or you can custom with the [Chunk](https://github.com/jina-ai/late-chunking/blob/main/chunked_pooling/chunking.py) used in the paper.\n",
    "\n",
    "- Support by Late Chunk Qdrant vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-community qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import JinaLateChunkEmbeddings\n",
    "from langchain_community.vectorstores import LateChunkQdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Custom Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "from typing import List, Optional, Tuple, Iterable, Sequence, Any\n",
    "from langchain_core.documents import BaseDocumentTransformer, Document\n",
    "\n",
    "\n",
    "class JinaTextSplitter(BaseDocumentTransformer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        jina_huggingface_model_name,\n",
    "        strategies: str = 'sentences',\n",
    "        chunk_size: int = 1024,\n",
    "        number_of_sentences:int = 2,\n",
    "        sentence_split_regex: str = r\"(?<=[.?!])\\s+\",\n",
    "        add_start_index: bool = False,\n",
    "    ):  \n",
    "        try:\n",
    "            from transformers import AutoTokenizer\n",
    "        \n",
    "        except ImportError:\n",
    "            raise ValueError(\"Could not import transformers python package.\"\n",
    "                             \"Please install it with `pip install transformers`.\"\n",
    "                            )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(jina_huggingface_model_name, trust_remote_code=True)\n",
    "        self.strategies = strategies\n",
    "        self.chunk_size = chunk_size\n",
    "        self.number_of_sentences = number_of_sentences\n",
    "        self.sentence_split_regex = sentence_split_regex\n",
    "        self._add_start_index = add_start_index\n",
    "        \n",
    "    def create_documents(\n",
    "        self, texts: List[str], metadatas: Optional[List[dict]] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Create documents from a list of texts.\"\"\"\n",
    "        _metadatas = metadatas or [{}] * len(texts)\n",
    "        documents = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            start_index = 0\n",
    "            for chunk in self.split_text(text):\n",
    "                metadata = copy.deepcopy(_metadatas[i])\n",
    "                if self._add_start_index:\n",
    "                    metadata[\"start_index\"] = start_index\n",
    "                new_doc = Document(page_content=chunk, metadata=metadata)\n",
    "                documents.append(new_doc)\n",
    "                start_index += len(chunk)\n",
    "        return documents\n",
    "    \n",
    "    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents.\"\"\"\n",
    "        texts, metadatas = [], []\n",
    "        for doc in documents:\n",
    "            texts.append(doc.page_content)\n",
    "            metadatas.append(doc.metadata)\n",
    "        return self.create_documents(texts, metadatas=metadatas)\n",
    "    \n",
    "    def transform_documents(\n",
    "        self, documents: Sequence[Document], **kwargs: Any\n",
    "    ) -> Sequence[Document]:\n",
    "        \"\"\"Transform sequence of documents by splitting them.\"\"\"\n",
    "        return self.split_documents(list(documents))\n",
    "    \n",
    "    def split_text(\n",
    "        self,\n",
    "        text: str,\n",
    "    ) -> List[str]:\n",
    "        \n",
    "        if self.strategies == \"tokenize\":\n",
    "            if self.chunk_size < 4:\n",
    "                raise ValueError(\"Chunk size must be >= 4.\")\n",
    "            return self._chunk_by_tokens(text, self.chunk_size)\n",
    "        elif self.strategies == \"sentences\":\n",
    "            return self._chunk_by_sentences(text, self.number_of_sentences)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported chunking strategy {self.strategies}\")\n",
    "        \n",
    "    def _get_single_sentences_list(self, text: str) -> List[str]:\n",
    "        return re.split(self.sentence_split_regex, text)\n",
    "\n",
    "    def _chunk_by_sentences(\n",
    "        self,\n",
    "        text:str ,\n",
    "        n_sentences: int\n",
    "    ):\n",
    "        single_sentences_list = self._get_single_sentences_list(text)\n",
    "        \n",
    "        # Calculate how many chunks we need\n",
    "        num_chunks = len(single_sentences_list) // n_sentences\n",
    "        if len(single_sentences_list) % n_sentences != 0:\n",
    "            num_chunks += 1  # If there are leftovers, add an extra chunk\n",
    "        \n",
    "        # Create a list of lists to hold the sentence chunks\n",
    "        chunk_text = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for _ in range(num_chunks):\n",
    "            end_idx = start_idx + n_sentences\n",
    "            chunk = ' '.join(x for x in single_sentences_list[start_idx:end_idx])\n",
    "            chunk_text.append(chunk)\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        return chunk_text\n",
    "    \n",
    "    def _chunk_by_tokens(\n",
    "        self,\n",
    "        text: str,\n",
    "        chunk_size: int,\n",
    "    ) -> List[Tuple[int, int, int]]:\n",
    "        tokens = self.tokenizer.encode_plus(\n",
    "            text, return_offsets_mapping=True, add_special_tokens=False\n",
    "        )\n",
    "        token_offsets = tokens.offset_mapping\n",
    "\n",
    "        chunk_spans = []\n",
    "        for i in range(0, len(token_offsets), chunk_size):\n",
    "            chunk_end = min(i + chunk_size, len(token_offsets))\n",
    "            if chunk_end - i > 0:\n",
    "                chunk_spans.append((i, chunk_end))\n",
    "\n",
    "        # get sub-text\n",
    "        chunk_text = self._tokens_to_text(text, chunk_spans)    \n",
    "    \n",
    "        return chunk_text\n",
    "    \n",
    "    def _tokens_to_text(self, text: str, annotations: List[Tuple[int, int]]):\n",
    "        tokens = self.tokenizer.encode_plus(\n",
    "            text, return_offsets_mapping=True, add_special_tokens=False\n",
    "        )\n",
    "        token_offsets = tokens.offset_mapping\n",
    "        chunks = []\n",
    "        for start, end in annotations:\n",
    "            chunk = text[token_offsets[start][0]:token_offsets[end-1][1]]\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    def _get_token_length_of_text(self, text):\n",
    "        input_ids = self.tokenizer(text)['input_ids']\n",
    "        \n",
    "        return len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Split Text\n",
    "text_splitter = JinaTextSplitter(\n",
    "        jina_huggingface_model_name='jinaai/jina-embeddings-v3',\n",
    "        strategies = 'sentences',\n",
    "        chunk_size = 1024,\n",
    "        number_of_sentences = 3,\n",
    "    )\n",
    "\n",
    "# Text embedding\n",
    "text_embeddings = JinaLateChunkEmbeddings(jina_api_key=\"jina_*\", model_name=\"jina-embeddings-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Vectorestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ROOT = 'demo-qdrant'\n",
    "\n",
    "\n",
    "def init_vectorstore(text_embeddings, text_splitter, collection_name='latechunk', topK=5):\n",
    "    client = QdrantClient()\n",
    "    vectorstore = LateChunkQdrant(\n",
    "        client, collection_name=collection_name,\n",
    "        embeddings=text_embeddings, text_splitter=text_splitter\n",
    "    )\n",
    "\n",
    "    if os.path.isdir(os.path.join(ROOT, 'collection', collection_name)):\n",
    "        print(f\"===== Load exits collection: {collection_name} ======\")\n",
    "        vectorstore = vectorstore.from_existing_collection(\n",
    "            embedding=text_embeddings, path=ROOT,\n",
    "            collection_name=collection_name, text_splitter=text_splitter\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(f\"===== Create new collection: {collection_name} ======\")\n",
    "\n",
    "        loader = WebBaseLoader(\"https://github.com/hwchase17/chroma-langchain/blob/master/state_of_the_union.txt\")\n",
    "        data = loader.load()\n",
    "\n",
    "        vectorstore = vectorstore.from_documents(\n",
    "            documents=data, embedding=text_embeddings, text_splitter=text_splitter,\n",
    "            path=ROOT, collection_name=collection_name\n",
    "        )\n",
    "\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": topK})\n",
    "\n",
    "vectordb = init_vectorstore(text_embeddings, text_splitter, collection_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"what did the president say about ketanji brown jackson?\"\n",
    "\n",
    "docs = vectordb.invoke(query)\n",
    "len(docs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
