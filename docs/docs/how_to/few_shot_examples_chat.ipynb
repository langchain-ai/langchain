{
 "cells": [
  {
   "cell_type": "raw",
   "id": "beba2e0e",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0735c0",
   "metadata": {},
   "source": [
    "# How to use few shot examples in chat models\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "- [Prompt templates](/docs/concepts/#prompt-templates)\n",
    "- [Example selectors](/docs/concepts/#example-selectors)\n",
    "- [Chat models](/docs/concepts/#chat-model)\n",
    "- [Vectorstores](/docs/concepts/#vector-stores)\n",
    "\n",
    ":::\n",
    "\n",
    "This guide covers how to prompt a chat model with example inputs and outputs. Providing the model with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\n",
    "\n",
    "There does not appear to be solid consensus on how best to do few-shot prompting, and the optimal prompt compilation will likely vary by model. Because of this, we provide few-shot prompt templates like the [FewShotChatMessagePromptTemplate](https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate.html?highlight=fewshot#langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate) as a flexible starting point, and you can modify or replace them as you see fit.\n",
    "\n",
    "The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.\n",
    "\n",
    "**Note:** The following code examples are for chat models only, since `FewShotChatMessagePromptTemplates` are designed to output formatted [chat messages](/docs/concepts/#message-types) rather than pure strings. For similar few-shot prompt examples for pure string templates compatible with completion models (LLMs), see the [few-shot prompt templates](/docs/how_to/few_shot_examples/) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716f2de-cc29-4823-9360-a808c7bfdb86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fixed Examples\n",
    "\n",
    "The most basic (and common) few-shot prompting technique is to use fixed prompt examples. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production.\n",
    "\n",
    "The basic components of the template are:\n",
    "- `examples`: A list of dictionary examples to include in the final prompt.\n",
    "- `example_prompt`: converts each example into 1 or more messages through its [`format_messages`](https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html?highlight=format_messages#langchain_core.prompts.chat.ChatPromptTemplate.format_messages) method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.\n",
    "\n",
    "Below is a simple demonstration. First, define the examples you'd like to include. Let's give the LLM an unfamiliar mathematical operator, denoted by the \"ðŸ¦œ\" emoji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b79e400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:08.170253Z",
     "iopub.status.busy": "2024-09-11T18:19:08.170020Z",
     "iopub.status.idle": "2024-09-11T18:19:12.423398Z",
     "shell.execute_reply": "2024-09-11T18:19:12.423105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "langchain-groq 0.2.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-google-genai 2.0.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-benchmarks 0.0.14 requires langchain-community<0.3,>=0.2, but you have langchain-community 0.3.0.dev1 which is incompatible.\r\n",
      "langchain-community 0.3.0.dev1 requires langchain<0.4.0,>=0.3.0.dev1, but you have langchain 0.2.16 which is incompatible.\r\n",
      "langchain-community 0.3.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev2, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-anthropic 0.2.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-mistralai 0.2.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-google-vertexai 2.0.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\r\n",
      "langchain-experimental 0.3.0.dev1 requires langchain-core<0.4.0,>=0.3.0.dev4, but you have langchain-core 0.2.39 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "getpass was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[0;32m----> 6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langchain/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1256\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1255\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetpass was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: getpass was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-chroma\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30856d92",
   "metadata": {},
   "source": [
    "If we try to ask the model what the result of this expression is, it will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174dec5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:12.424916Z",
     "iopub.status.busy": "2024-09-11T18:19:12.424816Z",
     "iopub.status.idle": "2024-09-11T18:19:13.687723Z",
     "shell.execute_reply": "2024-09-11T18:19:13.687054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The expression \"2 ðŸ¦œ 9\" seems to use a parrot emoji (ðŸ¦œ) in place of a mathematical operator. If you could clarify what operation you intend to represent with the parrot emoji, I would be happy to help you solve it!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 17, 'total_tokens': 72}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-fd2e2393-a140-4d24-a243-b1dc2e89ac5d-0', usage_metadata={'input_tokens': 17, 'output_tokens': 55, 'total_tokens': 72})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "model.invoke(\"What is 2 ðŸ¦œ 9?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d58385",
   "metadata": {},
   "source": [
    "Now let's see what happens if we give the LLM some examples to work with. We'll define some below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc5a02a-6249-4e92-95c3-30fff9671e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:13.691104Z",
     "iopub.status.busy": "2024-09-11T18:19:13.690839Z",
     "iopub.status.idle": "2024-09-11T18:19:13.694802Z",
     "shell.execute_reply": "2024-09-11T18:19:13.693596Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"2 ðŸ¦œ 2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2 ðŸ¦œ 3\", \"output\": \"5\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8710ecc-2aa0-4172-a74c-250f6bc3d9e2",
   "metadata": {},
   "source": [
    "Next, assemble them into the few-shot prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e72ad1-9060-47d0-91a1-bc130c8b98ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:13.697716Z",
     "iopub.status.busy": "2024-09-11T18:19:13.697359Z",
     "iopub.status.idle": "2024-09-11T18:19:13.703000Z",
     "shell.execute_reply": "2024-09-11T18:19:13.702619Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='2 ðŸ¦œ 2'), AIMessage(content='4'), HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5')]\n"
     ]
    }
   ],
   "source": [
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.invoke({}).to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5490bd59-b28f-46a4-bbdf-0191802dd3c5",
   "metadata": {},
   "source": [
    "Finally, we assemble the final prompt as shown below, passing `few_shot_prompt` directly into the `from_messages` factory method, and use it with a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f86d6d9-50de-41b6-b6c7-0f9980cc0187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:13.705186Z",
     "iopub.status.busy": "2024-09-11T18:19:13.705009Z",
     "iopub.status.idle": "2024-09-11T18:19:13.707692Z",
     "shell.execute_reply": "2024-09-11T18:19:13.707361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8029c5",
   "metadata": {},
   "source": [
    "And now let's ask the model the initial question and see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97d443b1-6fae-4b36-bede-3ff7306288a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:13.709548Z",
     "iopub.status.busy": "2024-09-11T18:19:13.709359Z",
     "iopub.status.idle": "2024-09-11T18:19:14.372600Z",
     "shell.execute_reply": "2024-09-11T18:19:14.371930Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of \\\\(2 ðŸ¦œ 9\\\\) is \\\\(11\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 60, 'total_tokens': 77}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-1d95d723-e60e-4324-b26a-a7b1d826df73-0', usage_metadata={'input_tokens': 60, 'output_tokens': 17, 'total_tokens': 77})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chain = final_prompt | model\n",
    "\n",
    "chain.invoke({\"input\": \"What is 2 ðŸ¦œ 9?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab7114-f07f-46be-8874-3705a25aba5f",
   "metadata": {},
   "source": [
    "And we can see that the model has now inferred that the parrot emoji means addition from the given few-shot examples!\n",
    "\n",
    "## Dynamic few-shot prompting\n",
    "\n",
    "Sometimes you may want to select only a few examples from your overall set to show based on the input. For this, you can replace the `examples` passed into `FewShotChatMessagePromptTemplate` with an `example_selector`. The other components remain the same as above! Our dynamic few-shot prompt template would look like:\n",
    "\n",
    "- `example_selector`: responsible for selecting few-shot examples (and the order in which they are returned) for a given input. These implement the [BaseExampleSelector](https://python.langchain.com/v0.2/api_reference/core/example_selectors/langchain_core.example_selectors.base.BaseExampleSelector.html?highlight=baseexampleselector#langchain_core.example_selectors.base.BaseExampleSelector) interface. A common example is the vectorstore-backed [SemanticSimilarityExampleSelector](https://python.langchain.com/v0.2/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html?highlight=semanticsimilarityexampleselector#langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector)\n",
    "- `example_prompt`: convert each example into 1 or more messages through its [`format_messages`](https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html?highlight=chatprompttemplate#langchain_core.prompts.chat.ChatPromptTemplate.format_messages) method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.\n",
    "\n",
    "These once again can be composed with other messages and chat templates to assemble your final prompt.\n",
    "\n",
    "Let's walk through an example with the `SemanticSimilarityExampleSelector`. Since this implementation uses a vectorstore to select examples based on semantic similarity, we will want to first populate the store. Since the basic idea here is that we want to search for and return examples most similar to the text input, we embed the `values` of our prompt examples rather than considering the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad66f06a-66fd-4fcc-8166-5d0e3c801e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:14.379907Z",
     "iopub.status.busy": "2024-09-11T18:19:14.379021Z",
     "iopub.status.idle": "2024-09-11T18:19:15.697231Z",
     "shell.execute_reply": "2024-09-11T18:19:15.696778Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"2 ðŸ¦œ 2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2 ðŸ¦œ 3\", \"output\": \"5\"},\n",
    "    {\"input\": \"2 ðŸ¦œ 4\", \"output\": \"6\"},\n",
    "    {\"input\": \"What did the cow say to the moon?\", \"output\": \"nothing at all\"},\n",
    "    {\n",
    "        \"input\": \"Write me a poem about the moon\",\n",
    "        \"output\": \"One for the moon, and one for me, who are we to talk about the moon?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e384a-2031-432b-951c-7ea8cf9262f1",
   "metadata": {},
   "source": [
    "### Create the `example_selector`\n",
    "\n",
    "With a vectorstore created, we can create the `example_selector`. Here we will call it in isolation, and set `k` on it to only fetch the two example closest to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7790303a-f722-452e-8921-b14bdf20bdff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:15.700256Z",
     "iopub.status.busy": "2024-09-11T18:19:15.700011Z",
     "iopub.status.idle": "2024-09-11T18:19:15.946361Z",
     "shell.execute_reply": "2024-09-11T18:19:15.945545Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What did the cow say to the moon?', 'output': 'nothing at all'},\n",
       " {'input': '2 ðŸ¦œ 4', 'output': '6'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "# The prompt template will load examples by passing the input do the `select_examples` method\n",
    "example_selector.select_examples({\"input\": \"horse\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77c40f-3f58-40a2-b757-a2a2ea43f24a",
   "metadata": {},
   "source": [
    "### Create prompt template\n",
    "\n",
    "We now assemble the prompt template, using the `example_selector` created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "253c255e-41d7-45f6-9d88-c7a0ced4b1bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:15.950299Z",
     "iopub.status.busy": "2024-09-11T18:19:15.949813Z",
     "iopub.status.idle": "2024-09-11T18:19:16.178307Z",
     "shell.execute_reply": "2024-09-11T18:19:16.177700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5'), HumanMessage(content='2 ðŸ¦œ 4'), AIMessage(content='6')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.invoke(input=\"What's 3 ðŸ¦œ 3?\").to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cae7d-0eb0-44a6-852f-0267c5ff72b3",
   "metadata": {},
   "source": [
    "And we can pass this few-shot chat message prompt template into another chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e731cb45-f0ea-422c-be37-42af2a6cb2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:16.181403Z",
     "iopub.status.busy": "2024-09-11T18:19:16.181124Z",
     "iopub.status.idle": "2024-09-11T18:19:16.533487Z",
     "shell.execute_reply": "2024-09-11T18:19:16.532954Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5'), HumanMessage(content='2 ðŸ¦œ 4'), AIMessage(content='6')]\n"
     ]
    }
   ],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.invoke(input=\"What's 3 ðŸ¦œ 3?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408ea69-1880-4ef5-a0fa-ffa8d2026aa9",
   "metadata": {},
   "source": [
    "### Use with an chat model\n",
    "\n",
    "Finally, you can connect your model to the few-shot prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0568cbc6-5354-47f1-ab4d-dfcc616cf583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T18:19:16.538088Z",
     "iopub.status.busy": "2024-09-11T18:19:16.537813Z",
     "iopub.status.idle": "2024-09-11T18:19:17.782851Z",
     "shell.execute_reply": "2024-09-11T18:19:17.782325Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 ðŸ¦œ 3 equals 6.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 59, 'total_tokens': 69}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-4f986e55-c4b0-4d18-9eab-679457278781-0', usage_metadata={'input_tokens': 59, 'output_tokens': 10, 'total_tokens': 69})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = final_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "chain.invoke({\"input\": \"What's 3 ðŸ¦œ 3?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fad3c",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You've now learned how to add few-shot examples to your chat prompts.\n",
    "\n",
    "Next, check out the other how-to guides on prompt templates in this section, the related how-to guide on [few shotting with text completion models](/docs/how_to/few_shot_examples), or the other [example selector how-to guides](/docs/how_to/example_selectors/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e26b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
