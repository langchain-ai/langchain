{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titan Takeoff\n",
    "\n",
    "`TitanML` helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.\n",
    "\n",
    "Our inference server, [Titan Takeoff](https://docs.titanml.co/docs/intro) enables deployment of LLMs locally on your hardware in a single command. Most embedding models are supported out of the box, if you experience trouble with a specific model, please let us know at hello@titanml.co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "Here are some helpful examples to get started using Titan Takeoff Server. You need to make sure Takeoff Server has been started in the background before running these commands. For more information see [docs page for launching Takeoff](https://docs.titanml.co/docs/Docs/launching/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Note importing TitanTakeoffPro instead of TitanTakeoff will work as well both use same object under the hood\n",
    "from langchain_community.embeddings import TitanTakeoffEmbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Basic use assuming Takeoff is running on your machine using its default ports (ie localhost:3000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = TitanTakeoffEmbed()\n",
    "output = embed.embed_query(\"What is the weather in London in August?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 \n",
    "Starting readers using TitanTakeoffEmbed Python Wrapper. If you haven't created any readers with first launching Takeoff, or you want to add another you can do so when you initialize the TitanTakeoffEmbed object. Just pass a list of models you want to start as the models parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = {\n",
    "    \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "    \"device\": \"cpu\",\n",
    "    \"consumer_group\": \"embed\",\n",
    "}\n",
    "embed = TitanTakeoffEmbed(models=[embedding_model])\n",
    "\n",
    "# The model needs time to spin up, length of time need will depend on the size of model and your network connection speed\n",
    "time.sleep(60)\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "# We specified \"embed\" consumer group so need to send request to the same consumer group so it hits our embedding model and not others\n",
    "output = embed.embed_query(prompt, consumer_group=\"embed\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
