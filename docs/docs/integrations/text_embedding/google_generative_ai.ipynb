{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8543d632",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Google Gemini\n",
    "keywords: [google gemini embeddings]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab8b36-10bb-4795-bc98-75ab2d2081bb",
   "metadata": {},
   "source": [
    "# Google Generative AI Embeddings (AI Studio & Gemini API)\n",
    "\n",
    "Connect to Google's generative AI embeddings service using the `GoogleGenerativeAIEmbeddings` class, found in the [langchain-google-genai](https://pypi.org/project/langchain-google-genai/) package.\n",
    "\n",
    "This will help you get started with Google's Generative AI embedding models (like Gemini) using LangChain. For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/v0.2/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html).\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "import { ItemTable } from \"@theme/FeatureTables\";\n",
    "\n",
    "<ItemTable category=\"text_embedding\" item=\"Google Gemini\" />\n",
    "\n",
    "## Setup\n",
    "\n",
    "To access Google Generative AI embedding models you'll need to create a Google Cloud project, enable the Generative Language API, get an API key, and install the `langchain-google-genai` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "To use Google Generative AI models, you must have an API key. You can create one in Google AI Studio. See the [Google documentation](https://ai.google.dev/gemini-api/docs/api-key) for instructions.\n",
    "\n",
    "Once you have a key, set it as an environment variable `GOOGLE_API_KEY`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47652620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67283790",
   "metadata": {},
   "source": [
    "To enable automated tracing of your model calls, set your [LangSmith](https://docs.smith.langchain.com/) API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63545b38-9d56-4312-8f61-8d4f1e7a3b1b",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6a3cd-379f-4dff-a449-d3a9f3196f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2437b22-e364-418a-8c13-490a026cb7b5",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eedc551e-a1f3-4fd8-8d65-4e0784c4441b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.024917153641581535,\n",
       " 0.012005362659692764,\n",
       " -0.003886754624545574,\n",
       " -0.05774897709488869,\n",
       " 0.0020742062479257584]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bed60-e7bd-4e48-83d6-1c87001f98bd",
   "metadata": {},
   "source": [
    "## Batch\n",
    "\n",
    "You can also embed multiple strings at once for a processing speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec53aba-404f-4778-acd9-5d6664e79ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3072)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = embeddings.embed_documents(\n",
    "    [\n",
    "        \"Today is Monday\",\n",
    "        \"Today is Tuesday\",\n",
    "        \"Today is April Fools day\",\n",
    "    ]\n",
    ")\n",
    "len(vectors), len(vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c362bfbf",
   "metadata": {},
   "source": [
    "## Indexing and Retrieval\n",
    "\n",
    "Embedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our [RAG tutorials](/docs/tutorials/).\n",
    "\n",
    "Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document in the `InMemoryVectorStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "606a7f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is the framework for building context-aware reasoning applications'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text],\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "# show the retrieved document's content\n",
    "retrieved_documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482486f-5617-498a-8a44-1974d3212dda",
   "metadata": {},
   "source": [
    "## Task type\n",
    "`GoogleGenerativeAIEmbeddings` optionally support a `task_type`, which currently must be one of:\n",
    "\n",
    "- `SEMANTIC_SIMILARITY`: Used to generate embeddings that are optimized to assess text similarity.\n",
    "- `CLASSIFICATION`: Used to generate embeddings that are optimized to classify texts according to preset labels.\n",
    "- `CLUSTERING`: Used to generate embeddings that are optimized to cluster texts based on their similarities.\n",
    "- `RETRIEVAL_DOCUMENT`, `RETRIEVAL_QUERY`, `QUESTION_ANSWERING`, and `FACT_VERIFICATION`: Used to generate embeddings that are optimized for document search or information retrieval.\n",
    "- `CODE_RETRIEVAL_QUERY`: Used to retrieve a code block based on a natural language query, such as sort an array or reverse a linked list. Embeddings of the code blocks are computed using `RETRIEVAL_DOCUMENT`.\n",
    "\n",
    "By default, we use `RETRIEVAL_DOCUMENT` in the `embed_documents` method and `RETRIEVAL_QUERY` in the `embed_query` method. If you provide a task type, we will use that for all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7acc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f077db-8eb4-49f7-8866-471a8528dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1\n",
      "Cosine similarity with query: 0.7892893360164779\n",
      "---\n",
      "Document 2\n",
      "Cosine similarity with query: 0.5438283285204146\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\", task_type=\"RETRIEVAL_QUERY\"\n",
    ")\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\", task_type=\"RETRIEVAL_DOCUMENT\"\n",
    ")\n",
    "\n",
    "q_embed = query_embeddings.embed_query(\"What is the capital of France?\")\n",
    "d_embed = doc_embeddings.embed_documents(\n",
    "    [\"The capital of France is Paris.\", \"Philipp is likes to eat pizza.\"]\n",
    ")\n",
    "\n",
    "for i, d in enumerate(d_embed):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Cosine similarity with query: {cosine_similarity([q_embed], [d])[0][0]}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ea7b1",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7857e5",
   "metadata": {},
   "source": [
    "## Additional Configuration\n",
    "\n",
    "You can pass the following parameters to ChatGoogleGenerativeAI in order to customize the SDK's behavior:\n",
    "\n",
    "- `client_options`: [Client Options](https://googleapis.dev/python/google-api-core/latest/client_options.html#module-google.api_core.client_options) to pass to the Google API Client, such as a custom `client_options[\"api_endpoint\"]`\n",
    "- `transport`: The transport method to use, such as `rest`, `grpc`, or `grpc_asyncio`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
