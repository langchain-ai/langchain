{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: Baseten\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BasetenEmbeddings\n",
        "\n",
        "This will help you get started with Baseten embedding models using LangChain. For detailed documentation on `BasetenEmbeddings` features and configuration options, please refer to the [API reference](https://python.langchain.com/api_reference/baseten/embeddings/langchain_baseten.embeddings.BasetenEmbeddings.html).\n",
        "\n",
        "## Overview\n",
        "\n",
        "### Integration details\n",
        "\n",
        "import { ItemTable } from \"@theme/FeatureTables\";\n",
        "\n",
        "<ItemTable category=\"text_embedding\" item=\"Baseten\" />\n",
        "\n",
        "## Setup\n",
        "\n",
        "To access Baseten embedding models you'll need to create a Baseten account, get an API key, deploy an embedding model, and install the `langchain-baseten` integration package.\n",
        "\n",
        "### Credentials\n",
        "\n",
        "Head to [baseten.co](https://baseten.co/) to sign up to Baseten and generate an API key. Once you've done this set the BASETEN_API_KEY environment variable:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"BASETEN_API_KEY\"):\n",
        "    os.environ[\"BASETEN_API_KEY\"] = getpass.getpass(\"Enter your Baseten API key: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "To enable automated tracing of your model calls, set your [LangSmith](https://docs.smith.langchain.com/) API key:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet langchain-baseten\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiation\n",
        "\n",
        "Now we can instantiate our embedding model. You'll need to deploy an embedding model on Baseten first and get the model URL from your dashboard:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_baseten import BasetenEmbeddings\n",
        "\n",
        "embeddings = BasetenEmbeddings(\n",
        "    model=\"your-embedding-model\",\n",
        "    model_url=\"https://model-<id>.api.baseten.co/environments/production/sync\",\n",
        "    # api_key=\"...\",  # or set BASETEN_API_KEY\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Indexing and Retrieval\n",
        "\n",
        "Embedding models are often used in retrieval augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [working with external knowledge tutorials](/docs/tutorials/#working-with-external-knowledge).\n",
        "\n",
        "Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document in the `InMemoryVectorStore`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a vector store with a sample text\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
        "\n",
        "vectorstore = InMemoryVectorStore.from_texts(\n",
        "    [text],\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "# Use the vectorstore as a retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Retrieve the most similar text\n",
        "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
        "\n",
        "# show the retrieved document's content\n",
        "retrieved_documents[0].page_content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embed multiple documents\n",
        "texts = [\n",
        "    \"Machine learning is a subset of artificial intelligence\",\n",
        "    \"Natural language processing helps computers understand text\",\n",
        "    \"Vector embeddings represent text as numerical arrays\"\n",
        "]\n",
        "\n",
        "vectors = embeddings.embed_documents(texts)\n",
        "print(f\"Generated {len(vectors)} embeddings\")\n",
        "print(f\"Each embedding has {len(vectors[0])} dimensions\")\n",
        "print(f\"First embedding sample: {vectors[0][:3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embed single query\n",
        "query = \"What is artificial intelligence?\"\n",
        "query_vector = embeddings.embed_query(query)\n",
        "print(f\"Query embedding dimension: {len(query_vector)}\")\n",
        "print(f\"Query embedding sample: {query_vector[:3]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance Client Features\n",
        "\n",
        "Baseten embeddings use the Performance Client for optimized throughput:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Large batch processing with automatic optimization\n",
        "large_text_list = [f\"Document {i} about various topics\" for i in range(100)]\n",
        "large_vectors = embeddings.embed_documents(large_text_list)\n",
        "\n",
        "print(f\"Processed {len(large_vectors)} embeddings with automatic batching\")\n",
        "print(\"Performance Client features:\")\n",
        "print(\"- Automatic batching (batch_size=32)\")\n",
        "print(\"- Concurrent requests (max_concurrent_requests=128)\")\n",
        "print(\"- Smart request sizing (max_chars_per_request=8000)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
