{
 "cells": [
  {
   "cell_type": "raw",
   "id": "afaf8039",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: GreenNode\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d6f34",
   "metadata": {},
   "source": [
    "# GreenNodeEmbeddings\n",
    "\n",
    ">[GreenNode](https://greennode.ai/) is a global AI solutions provider and a **NVIDIA Preferred Partner**, delivering full-stack AI capabilities—from infrastructure to application—for enterprises across the US, MENA, and APAC regions. Operating on **world-class infrastructure** (LEED Gold, TIA‑942, Uptime Tier III), GreenNode empowers enterprises, startups, and researchers with a comprehensive suite of AI services\n",
    "\n",
    "This notebook provides a guide to getting started with `GreenNodeEmbeddings`. It enables you to perform semantic document search using various built-in connectors or your own custom data sources by generating high-quality vector representations of text.\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "| Provider | Package |\n",
    "|:--------:|:-------:|\n",
    "| [GreenNode](/docs/integrations/providers/greennode/) | [langchain-greennode](https://python.langchain.com/v0.2/api_reference/langchain_greennode/embeddings/langchain_greennode.embeddingsGreenNodeEmbeddings.html) |\n",
    "\n",
    "## Setup\n",
    "\n",
    "To access GreenNode embedding models you'll need to create a GreenNode account, get an API key, and install the `langchain-greennode` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "GreenNode requires an API key for authentication, which can be provided either as the `api_key` parameter during initialization or set as the environment variable `GREENNODE_API_KEY`. You can obtain an API key by registering for an account on [GreenNode Serverless AI](https://aiplatform.console.greennode.ai/playground)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36521c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"GREENNODE_API_KEY\"):\n",
    "    os.environ[\"GREENNODE_API_KEY\"] = getpass.getpass(\"Enter your GreenNode API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84fb993",
   "metadata": {},
   "source": [
    "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9664366",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain GreenNode integration lives in the `langchain-greennode` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64853226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-greennode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd1724",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "The `GreenNodeEmbeddings` class can be instantiated with optional parameters for the API key and model name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea7a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_greennode import GreenNodeEmbeddings\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings = GreenNodeEmbeddings(\n",
    "    # api_key=\"YOUR_API_KEY\",  # You can pass the API key directly\n",
    "    model=\"BAAI/bge-m3\"  # The default embedding model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d271b6",
   "metadata": {},
   "source": [
    "## Indexing and Retrieval\n",
    "\n",
    "Embedding models play a key role in retrieval-augmented generation (RAG) workflows by enabling both the indexing of content and its efficient retrieval. \n",
    "Below, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document in the `InMemoryVectorStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23df9f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is the framework for building context-aware reasoning applications'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text],\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "# show the retrieved document's content\n",
    "retrieved_documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b9855",
   "metadata": {},
   "source": [
    "## Direct Usage\n",
    "\n",
    "The `GreenNodeEmbeddings` class can be used independently to generate text embeddings without the need for a vector store. This is useful for tasks such as similarity scoring, clustering, or custom processing pipelines.\n",
    "\n",
    "### Embed single texts\n",
    "\n",
    "You can embed single texts or documents with `embed_query`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2befcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01104736328125, -0.0281982421875, 0.0035858154296875, -0.0311279296875, -0.0106201171875, -0.039\n"
     ]
    }
   ],
   "source": [
    "single_vector = embeddings.embed_query(text)\n",
    "print(str(single_vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a7d03",
   "metadata": {},
   "source": [
    "### Embed multiple texts\n",
    "\n",
    "You can embed multiple texts with `embed_documents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d6e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01104736328125, -0.0281982421875, 0.0035858154296875, -0.0311279296875, -0.0106201171875, -0.039\n",
      "[-0.07177734375, -0.00017452239990234375, -0.002044677734375, -0.0299072265625, -0.0184326171875, -0\n"
     ]
    }
   ],
   "source": [
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "two_vectors = embeddings.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19dda0",
   "metadata": {},
   "source": [
    "### Async Support\n",
    "\n",
    "GreenNodeEmbeddings supports async operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d556e655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async query embedding dimension: 1024\n",
      "Async document embeddings count: 3\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def generate_embeddings_async():\n",
    "    # Embed a single query\n",
    "    query_result = await embeddings.aembed_query(\"What is the capital of France?\")\n",
    "    print(f\"Async query embedding dimension: {len(query_result)}\")\n",
    "\n",
    "    # Embed multiple documents\n",
    "    docs = [\n",
    "        \"Paris is the capital of France\",\n",
    "        \"Berlin is the capital of Germany\",\n",
    "        \"Rome is the capital of Italy\",\n",
    "    ]\n",
    "    docs_result = await embeddings.aembed_documents(docs)\n",
    "    print(f\"Async document embeddings count: {len(docs_result)}\")\n",
    "\n",
    "\n",
    "await generate_embeddings_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a7966",
   "metadata": {},
   "source": [
    "### Document Similarity Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Matrix:\n",
      "Document 1: ['1.0000', '0.6005', '0.3542', '0.5788']\n",
      "Document 2: ['0.6005', '1.0000', '0.4154', '0.6170']\n",
      "Document 3: ['0.3542', '0.4154', '1.0000', '0.3528']\n",
      "Document 4: ['0.5788', '0.6170', '0.3528', '1.0000']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Create some documents\n",
    "documents = [\n",
    "    \"Machine learning algorithms build mathematical models based on sample data\",\n",
    "    \"Deep learning uses neural networks with many layers\",\n",
    "    \"Climate change is a major global environmental challenge\",\n",
    "    \"Neural networks are inspired by the human brain's structure\",\n",
    "]\n",
    "\n",
    "# Embed the documents\n",
    "embeddings_list = embeddings.embed_documents(documents)\n",
    "\n",
    "\n",
    "# Function to calculate similarity\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "\n",
    "# Print similarity matrix\n",
    "print(\"Document Similarity Matrix:\")\n",
    "for i, emb_i in enumerate(embeddings_list):\n",
    "    similarities = []\n",
    "    for j, emb_j in enumerate(embeddings_list):\n",
    "        similarity = calculate_similarity(emb_i, emb_j)\n",
    "        similarities.append(f\"{similarity:.4f}\")\n",
    "    print(f\"Document {i + 1}: {similarities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98785c12",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "For more details about the GreenNode Serverless AI API, visit the [GreenNode Serverless AI Documentation](https://aiplatform.console.greennode.ai/api-docs/maas).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tradingagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
