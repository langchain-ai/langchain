{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Google Bigtable\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigtableByteStore\n",
    "\n",
    "This guide will help you get started with Google Bigtable as a [key-value store](/docs/concepts/key_value_stores). \n",
    "\n",
    "For more advanced examples and tutorials, please visit the [langchain-google-bigtable GitHub repository](https://github.com/googleapis/langchain-google-bigtable-python/). For a full feature reference, see the [API documentation](https://python.langchain.com/v0.2/api_reference/google_bigtable/storage/langchain_google_bigtable.storage.BigtableByteStore.html).\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Google Cloud Bigtable](https://cloud.google.com/bigtable) is a scalable, fully managed NoSQL wide-column database that is suitable for both real-time access and analytics workloads.\n",
    "\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | JS support | Package downloads | Package latest |\n",
    "| :--- | :--- | :---: | :---: | :---: | :---: |\n",
    "| [BigtableByteStore](https://python.langchain.com/v0.2/api_reference/google_bigtable/storage/langchain_google_bigtable.storage.BigtableByteStore.html) | [langchain-google-bigtable](https://pypi.org/project/langchain-google-bigtable/) | ❌ | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-google-bigtable?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-google-bigtable?style=flat-square&label=%20) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The `BigtableByteStore` stores string keys as the `row_key` and byte values in a designated column.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You need a Google Cloud project with the Bigtable API enabled and an active Bigtable instance and table. You can follow the [official setup guide](https://cloud.google.com/bigtable/docs/) for detailed instructions.\n",
    "\n",
    "### Installation\n",
    "\n",
    "The integration lives in the `langchain-google-bigtable` package. The following command also installs `langchain-google-vertexai` for the embedding cache example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-google-bigtable langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "To instantiate the store, you'll need your Google Cloud `project_id` and your Bigtable `instance_id` and `table_id`. The example below shows how to create a table if it doesn't exist and then asynchronously creates the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_bigtable import BigtableByteStore, init_key_value_store_table\n",
    "\n",
    "# Your Google Cloud project ID\n",
    "PROJECT_ID = \"<YOUR_PROJECT_ID>\"\n",
    "# Your Bigtable instance ID\n",
    "INSTANCE_ID = \"<YOUR_INSTANCE_ID>\"\n",
    "# The table to use for the key-value store\n",
    "TABLE_ID = \"my-kv-store-table\"\n",
    "\n",
    "# Helper function to create the table and column family if they don't exist\n",
    "init_key_value_store_table(\n",
    "    project_id=PROJECT_ID,\n",
    "    instance_id=INSTANCE_ID,\n",
    "    table_id=TABLE_ID,\n",
    ")\n",
    "\n",
    "# The store is created asynchronously\n",
    "kv_store = await BigtableByteStore.create(\n",
    "    project_id=PROJECT_ID, \n",
    "    instance_id=INSTANCE_ID, \n",
    "    table_id=TABLE_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "The `BigtableByteStore` supports both synchronous (e.g., `mset`, `mget`) and asynchronous (e.g., `amset`, `amget`) methods. This guide uses the async methods.\n",
    "\n",
    "You can set and get data using `amset` and `amget`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await kv_store.amset(\n",
    "    [\n",
    "        (\"key1\", b\"value1\"),\n",
    "        (\"key2\", b\"value2\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "await kv_store.amget([\"key1\", \"key2\", \"nonexistent_key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can delete data using `amdelete`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await kv_store.amdelete([\"key1\"])\n",
    "\n",
    "await kv_store.amget([\"key1\", \"key2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can iterate over keys using `ayield_keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[key async for key in kv_store.ayield_keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage: Embedding Caching\n",
    "\n",
    "A common use case for a key-value store is to cache the results of expensive operations, like computing text embeddings. This saves both time and money on subsequent requests for the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_google_vertexai.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# Initialize the underlying embeddings model\n",
    "underlying_embeddings = VertexAIEmbeddings(project=PROJECT_ID, model_name=\"textembedding-gecko@003\")\n",
    "\n",
    "# Create the cached embedder using the same store\n",
    "# The 'namespace' argument prevents key collisions with other data in the store\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings,\n",
    "    kv_store,\n",
    "    namespace=\"text-embeddings\"\n",
    ")\n",
    "\n",
    "# The first call to embed_query will compute the embeddings and store them in Bigtable.\n",
    "# Subsequent calls with the same text will be much faster as they will fetch the result directly from the cache.\n",
    "embedding_result = await cached_embedder.aembed_query(\"Hello, world!\")\n",
    "print(embedding_result[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
