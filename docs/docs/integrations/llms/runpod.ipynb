{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunPod LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get started with RunPod LLMs.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide covers how to use the LangChain `RunPod` LLM class to interact with text generation models hosted on [RunPod Serverless](https://www.runpod.io/serverless-gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. **Install the package:**\n",
    "   ```bash\n",
    "   pip install -qU langchain-runpod\n",
    "   ```\n",
    "2. **Deploy an LLM Endpoint:** Follow the setup steps in the [RunPod Provider Guide](/docs/integrations/providers/runpod#setup) to deploy a compatible text generation endpoint on RunPod Serverless and get its Endpoint ID.\n",
    "3. **Set Environment Variables:** Make sure `RUNPOD_API_KEY` and `RUNPOD_ENDPOINT_ID` are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Make sure environment variables are set (or pass them directly to RunPod)\n",
    "if \"RUNPOD_API_KEY\" not in os.environ:\n",
    "    os.environ[\"RUNPOD_API_KEY\"] = getpass.getpass(\"Enter your RunPod API Key: \")\n",
    "if \"RUNPOD_ENDPOINT_ID\" not in os.environ:\n",
    "    os.environ[\"RUNPOD_ENDPOINT_ID\"] = input(\"Enter your RunPod Endpoint ID: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Initialize the `RunPod` class. You can pass model-specific parameters via `model_kwargs` and configure polling behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_runpod import RunPod\n",
    "\n",
    "llm = RunPod(\n",
    "    # runpod_endpoint_id can be passed here if not set in env\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_k\": 50,\n",
    "        # Add other parameters supported by your endpoint handler\n",
    "    },\n",
    "    # Optional: Adjust polling\n",
    "    # poll_interval=0.3,\n",
    "    # max_polling_attempts=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invocation\n",
    "\n",
    "Use the standard LangChain `.invoke()` and `.ainvoke()` methods to call the model. Streaming is also supported via `.stream()` and `.astream()` (simulated by polling the RunPod `/stream` endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Write a tagline for an ice cream shop on the moon.\"\n",
    "\n",
    "# Invoke (Sync)\n",
    "try:\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"--- Sync Invoke Response ---\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Error invoking LLM: {e}. Ensure endpoint ID/API key are correct and endpoint is active/compatible.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Stream (Sync, simulated via polling /stream)\n",
    "print(\"\\n--- Sync Stream Response ---\")\n",
    "try:\n",
    "    for chunk in llm.stream(prompt):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()  # Newline\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"\\nError streaming LLM: {e}. Ensure endpoint handler supports streaming output format.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# AInvoke (Async)\n",
    "try:\n",
    "    async_response = await llm.ainvoke(prompt)\n",
    "    print(\"--- Async Invoke Response ---\")\n",
    "    print(async_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error invoking LLM asynchronously: {e}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# AStream (Async)\n",
    "print(\"\\n--- Async Stream Response ---\")\n",
    "try:\n",
    "    async for chunk in llm.astream(prompt):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()  # Newline\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"\\nError streaming LLM asynchronously: {e}. Ensure endpoint handler supports streaming output format.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "The LLM integrates seamlessly with LangChain Expression Language (LCEL) chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Assumes 'llm' variable is instantiated from the 'Instantiation' cell\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "try:\n",
    "    chain_response = chain.invoke({\"topic\": \"bears\"})\n",
    "    print(\"--- Chain Response ---\")\n",
    "    print(chain_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error running chain: {e}\")\n",
    "\n",
    "# Async chain\n",
    "try:\n",
    "    async_chain_response = await chain.ainvoke({\"topic\": \"robots\"})\n",
    "    print(\"--- Async Chain Response ---\")\n",
    "    print(async_chain_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error running async chain: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint Considerations\n",
    "\n",
    "- **Input:** The endpoint handler should expect the prompt string within `{\"input\": {\"prompt\": \"...\", ...}}`.\n",
    "- **Output:** The handler should return the generated text within the `\"output\"` key of the final status response (e.g., `{\"output\": \"Generated text...\"}` or `{\"output\": {\"text\": \"...\"}}`).\n",
    "- **Streaming:** For simulated streaming via the `/stream` endpoint, the handler must populate the `\"stream\"` key in the status response with a list of chunk dictionaries, like `[{\"output\": \"token1\"}, {\"output\": \"token2\"}]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of the `RunPod` LLM class, parameters, and methods, refer to the source code or the generated API reference (if available).\n",
    "\n",
    "Link to source code: [https://github.com/runpod/langchain-runpod/blob/main/langchain_runpod/llms.py](https://github.com/runpod/langchain-runpod/blob/main/langchain_runpod/llms.py)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
