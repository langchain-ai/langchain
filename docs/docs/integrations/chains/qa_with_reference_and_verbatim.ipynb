{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4993da90",
   "metadata": {},
   "source": [
    "# QA with reference\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pprados/langchain-qa_with_references/blob/master/qa_with_reference_and_verbatim.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf56f04",
   "metadata": {},
   "source": [
    "We believe that hallucinations pose a major problem in the adoption of LLMs (Language Model Models). It is imperative to provide a simple and quick solution that allows the user to verify the coherence of the answers to the questions they are asked.\n",
    "\n",
    "The conventional approach is to provide a list of URLs of the documents that helped in answering (see qa_with_source). However, this approach is unsatisfactory in several scenarios:\n",
    "\n",
    "1. The question is asked about a PDF of over 100 pages. Each fragment comes from the same document, but from where?\n",
    "2. Some documents do not have URLs (data retrieved from a database or other loaders).\n",
    "\n",
    "It appears essential to have a means of retrieving all references to the actual data sources used by the model to answer the question. \n",
    "\n",
    "This includes:\n",
    "- The precise list of documents used for the answer (the `Documents`, along with their metadata that may contain page numbers, slide numbers, or any other information allowing the retrieval of the fragment in the original document).\n",
    "- The excerpts of text used for the answer in each fragment. Even if a fragment is used, the LLM only utilizes a small portion to generate the answer. Access to these verbatim excerpts helps to quickly ascertain the validity of the answer.\n",
    "\n",
    "We propose a new pipeline: `qa_with_reference` for this purpose. It is a Question/Answer type pipeline that returns the list of documents used, and in the metadata, the list of verbatim excerpts exploited to produce the answer.\n",
    "\n",
    "*At this time, only the `map_reduce` chain type car extract the verbatim excerpts.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730b0fd3-82d3-439a-87b8-3ef77d9d1d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T12:39:22.005634758Z",
     "start_time": "2023-11-06T12:39:19.006453887Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q 'langchain-experimental' --upgrade pip  openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q  python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"XXXXX\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d60381b7ca806d6d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d181ff19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T12:45:28.578231803Z",
     "start_time": "2023-11-06T12:45:28.571603635Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.schema import Document\n",
    "\n",
    "llm = OpenAI(\n",
    "    max_tokens=1500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d935867c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T12:45:31.959915788Z",
     "start_time": "2023-11-06T12:45:29.035033057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer \"he eats apples, he eats pears, he eats carrots.\", the LLM use:\n",
      "Document 0\n",
      "- \"he eats apples\"\n",
      "- \"he eats pears.\"\n",
      "Document 1\n",
      "- \"he eats carrots.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.chains import QAWithReferencesAndVerbatimsChain\n",
    "\n",
    "chain_type = \"map_reduce\"\n",
    "qa_chain = QAWithReferencesAndVerbatimsChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    ")\n",
    "\n",
    "question = \"what does it eat?\"\n",
    "bodies = [\n",
    "    \"he eats apples and plays football.\" \"My name is Philippe.\" \"he eats pears.\",\n",
    "    \"he eats carrots. I like football.\",\n",
    "    \"The Earth is round.\",\n",
    "]\n",
    "docs = [\n",
    "    Document(page_content=body, metadata={\"id\": i}) for i, body in enumerate(bodies)\n",
    "]\n",
    "\n",
    "answer = qa_chain(\n",
    "    inputs={\n",
    "        \"docs\": docs,\n",
    "        \"question\": question,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "print(f'To answer \"{answer[\"answer\"]}\", the LLM use:')\n",
    "for doc in answer[\"source_documents\"]:\n",
    "    print(f\"Document {doc.metadata['id']}\")\n",
    "    for verbatim in doc.metadata.get(\"verbatims\", []):\n",
    "        print(f'- \"{verbatim}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "296c49aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T12:45:40.269265983Z",
     "start_time": "2023-11-06T12:45:31.943845958Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q chromadb wikipedia\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "question = \"what is the Machine learning?\"\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever()\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "docs = wikipedia_retriever.get_relevant_documents(question)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "split_docs = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=10\n",
    ").split_documents(docs)\n",
    "\n",
    "vectorstore.add_documents(split_docs)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e9e8e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T12:45:47.003018015Z",
     "start_time": "2023-11-06T12:45:40.274007200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the question \"what is the Machine learning?\", to answer \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.\", the LLM use:\n",
      "Source \u001B[94mhttps://en.wikipedia.org/wiki/Machine_learning\u001B[0m\n",
      "-  \"\u001B[92mMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.\u001B[0m\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.chains import (\n",
    "    RetrievalQAWithReferencesAndVerbatimsChain,\n",
    ")\n",
    "from typing import Literal, List\n",
    "\n",
    "chain_type: Literal[\"stuff\", \"map_reduce\", \"map_rerank\", \"refine\"] = \"map_reduce\"\n",
    "\n",
    "qa_chain = RetrievalQAWithReferencesAndVerbatimsChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    reduce_k_below_max_tokens=True,\n",
    ")\n",
    "result = qa_chain(\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def merge_result_by_urls(result):\n",
    "    references = {}\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        source = doc.metadata.get(\"source\", [])\n",
    "        verbatims_for_source: List[str] = doc.metadata.get(source, [])\n",
    "        verbatims_for_source.extend(doc.metadata.get(\"verbatims\", []))\n",
    "        references[source] = verbatims_for_source\n",
    "    return references\n",
    "\n",
    "\n",
    "print(f'For the question \"{question}\", to answer \"{result[\"answer\"]}\", the LLM use:')\n",
    "references = merge_result_by_urls(result)\n",
    "# Print the result\n",
    "for source, verbatims in references.items():\n",
    "    print(f\"Source \\033[94m{source}\\033[0m\")\n",
    "    for verbatim in verbatims:\n",
    "        print(f'-  \"\\033[92m{verbatim}\\033[0m\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c65515ad36cf5010"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "langchain-qa_with_references",
   "language": "python",
   "display_name": "langchain-qa_with_references"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
