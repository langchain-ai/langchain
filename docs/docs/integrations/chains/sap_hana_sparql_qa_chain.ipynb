{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48024ef",
   "metadata": {},
   "source": [
    "# Question Answering with `HanaSparqlQAChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b340e95",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "To use this feature, install the `langchain-hana` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d6d15",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain_hana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be79aa5",
   "metadata": {},
   "source": [
    "And then, create a connection to your SAP HANA Cloud instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c22a2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# Load environment variables if needed\n",
    "load_dotenv()\n",
    "\n",
    "# Establish connection to SAP HANA Cloud\n",
    "connection = dbapi.connect(\n",
    "    address=os.environ.get(\"HANA_DB_ADDRESS\"),\n",
    "    port=os.environ.get(\"HANA_DB_PORT\"),\n",
    "    user=os.environ.get(\"HANA_DB_USER\"),\n",
    "    password=os.environ.get(\"HANA_DB_PASSWORD\"),\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe5007",
   "metadata": {},
   "source": [
    "`HanaSparqlQAChain` ties together:\n",
    "\n",
    "1. **Schema-aware SPARQL generation**  \n",
    "2. **Query execution** against SAP HANA  \n",
    "3. **Natural-language answer formatting**\n",
    "\n",
    "\n",
    "## Initialization\n",
    "\n",
    "You need:\n",
    "\n",
    "- An **LLM** to generate and interpret queries  \n",
    "- A **`HanaRdfGraph`** (with connection, `graph_uri`, and ontology)\n",
    "\n",
    "Follow the steps here [HanaRdfGraph](/docs/integrations/graphs/sap_hana_rdf_graph) to know more about creating a `HanaRdfGraph` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c9c99",
   "metadata": {},
   "source": [
    "Import the HanaSparqlQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa69303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_hana import HanaSparqlQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = HanaSparqlQAChain.from_llm(\n",
    "    llm=llm, graph=graph, allow_dangerous_requests=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c5294",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "\n",
    "1. **SPARQL Generation**  \n",
    "   - Uses `SPARQL_GENERATION_SELECT_PROMPT`  \n",
    "   - Inputs:  \n",
    "     - `schema` (Turtle from `graph.get_schema`)  \n",
    "     - `prompt` (user’s question)  \n",
    "2. **Query Post-processing**  \n",
    "   - Extracts the SPARQL code from the llm output.\n",
    "   - Inject `FROM <graph_uri>` if missing  \n",
    "   - Ensure required common prefixes are declared (`rdf:`, `rdfs:`, `owl:`, `xsd:`)  \n",
    "3. **Execution**  \n",
    "   - Calls `graph.query(generated_sparql)`  \n",
    "4. **Answer Formulation**  x\n",
    "   - Uses `SPARQL_QA_PROMPT`  \n",
    "   - Inputs:  \n",
    "     - `context` (raw query results)  \n",
    "     - `prompt` (original question)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33ffd6",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "\n",
    "#### \"SPARQL Generation\" prompt\n",
    "\n",
    "The `sparql_generation_prompt` is used to guide the LLM in generating a SPARQL query from the user question and the provided schema.\n",
    "\n",
    "#### Answering prompt\n",
    "\n",
    "The `qa_prompt` instructs the LLM to create a natural language answer based solely on the database results.\n",
    "\n",
    "The default prompts can be found here: [`prompts.py`](https://github.com/SAP/langchain-integration-for-sap-hana-cloud/blob/main/langchain_hana/chains/graph_qa/prompts.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572ed2e",
   "metadata": {},
   "source": [
    "## Customizing Prompts\n",
    "\n",
    "You can override the defaults at initialization:\n",
    "\n",
    "```python\n",
    "qa_chain = HanaSparqlQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    allow_dangerous_requests=True,\n",
    "    verbose=True,\n",
    "    sparql_generation_prompt=YOUR_SPARQL_PROMPT,\n",
    "    qa_prompt=YOUR_QA_PROMPT\n",
    ")\n",
    "```\n",
    "\n",
    "Or swap them afterward:\n",
    "\n",
    "```python\n",
    "qa_chain.sparql_generation_chain.prompt = YOUR_SPARQL_PROMPT\n",
    "qa_chain.qa_chain.prompt              = YOUR_QA_PROMPT\n",
    "```\n",
    "\n",
    "> - `sparql_generation_prompt` must have the input variables: `[\"schema\", \"prompt\"]`\n",
    "> - `qa_prompt` must have the input variables: `[\"context\", \"prompt\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443cdd7",
   "metadata": {},
   "source": [
    "## Example: Question Answering over a “Movies” Knowledge Graph\n",
    "\n",
    "**Prerequisite**:  \n",
    "You must have an SAP HANA Cloud instance with the **triple store** feature enabled.  \n",
    "For detailed instructions, refer to: [Enable Triple Store](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-knowledge-graph-guide/enable-triple-store/)<br />\n",
    "Load the `kgdocu_movies` example data. See [Knowledge Graph Example](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-knowledge-graph-guide/knowledge-graph-example).\n",
    "\n",
    "Below we’ll:\n",
    "\n",
    "1. Instantiate the `HanaRdfGraph` pointing at our “movies” data graph  \n",
    "2. Wrap it in a `HanaSparqlQAChain` powered by an LLM  \n",
    "3. Ask natural-language questions and print out the chain’s responses  \n",
    "\n",
    "This demonstrates how the LLM generates SPARQL under the hood, executes it against SAP HANA, and returns a human-readable answer.\n",
    "\n",
    "We'll use the `sap-ai-sdk-gen` package. Currently still installed via: \n",
    "\n",
    "`pip install \"generative-ai-hub-sdk[all]\"` \n",
    "\n",
    "Please check [sap-ai-sdk-gen](https://pypi.org/project/sap-ai-sdk-gen/) for future releases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638733f",
   "metadata": {},
   "source": [
    "First, create a connection to your SAP HANA Cloud instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# Load environment variables if needed\n",
    "load_dotenv()\n",
    "\n",
    "# Establish connection to SAP HANA Cloud\n",
    "connection = dbapi.connect(\n",
    "    address=os.environ.get(\"HANA_DB_ADDRESS\"),\n",
    "    port=os.environ.get(\"HANA_DB_PORT\"),\n",
    "    user=os.environ.get(\"HANA_DB_USER\"),\n",
    "    password=os.environ.get(\"HANA_DB_PASSWORD\"),\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66b111",
   "metadata": {},
   "source": [
    "Then, set up the knowledge graph instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from langchain_hana import HanaRdfGraph, HanaSparqlQAChain\n",
    "\n",
    "# from langchain_openai import ChatOpenAI  # or your chosen LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Knowledge Graph\n",
    "graph_uri = \"kgdocu_movies\"\n",
    "\n",
    "graph = HanaRdfGraph(\n",
    "    connection=connection, graph_uri=graph_uri, auto_extract_ontology=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic graph schema is extracted from the data graph. This schema will guide the LLM to generate a proper SPARQL query.\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd22a30",
   "metadata": {},
   "source": [
    "After that, initialise the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(proxy_model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18d596",
   "metadata": {},
   "source": [
    "Then, we create a SPARQL QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SPARQL QA Chain\n",
    "chain = HanaSparqlQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    graph=graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b426da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = chain.invoke(\"Which movies are in the data?\")\n",
    "# output = chain.invoke(\"In which movies did Keanu Reeves and Carrie-Anne Moss play in together\")\n",
    "# output = chain.invoke(\"which movie genres are in the data?\")\n",
    "# output = chain.invoke(\"which are the two most assigned movie genres?\")\n",
    "# output = chain.invoke(\"where were the actors of \"Blade Runner\" born?\")\n",
    "# output = chain.invoke(\"which actors acted together in a movie and were born in the same city?\")\n",
    "output = chain.invoke(\"which actors acted in Blade Runner?\")\n",
    "\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233a4c1",
   "metadata": {},
   "source": [
    "### What’s happening under the hood?\n",
    "\n",
    "1. **SPARQL Generation**  \n",
    "   The chain invokes the LLM with your Turtle-formatted ontology (`graph.get_schema`) and the user’s question using the `SPARQL_GENERATION_SELECT_PROMPT`. The LLM then emits a valid `SELECT` query tailored to your schema.\n",
    "\n",
    "2. **Pre-processing & Execution**  \n",
    "   - **Extract & clean**: Pull the raw SPARQL text out of the LLM’s response.  \n",
    "   - **Inject graph context**: Add `FROM <graph_uri>` if it’s missing and ensure common prefixes (`rdf:`, `rdfs:`, `owl:`, `xsd:`) are declared.  \n",
    "   - **Run on HANA**: Execute the finalized query via `HanaRdfGraph.query()` over your named graph.\n",
    "\n",
    "3. **Answer Formulation**  \n",
    "   The returned CSV (or Turtle) results feed into the LLM again—this time with the `SPARQL_QA_PROMPT`. The LLM produces a concise, human-readable answer strictly based on the retrieved data, without hallucination.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
