{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62727aaa-bcff-4087-891c-e539f824ee1f",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Wikipedia\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a16c1-10de-4f99-b392-c4ad2e6123a1",
   "metadata": {},
   "source": [
    "# WikipediaRetriever\n",
    "\n",
    "## Overview\n",
    ">[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.\n",
    "\n",
    "This notebook shows how to retrieve wiki pages from `wikipedia.org` into the [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) format that is used downstream.\n",
    "\n",
    "### Integration details\n",
    "\n",
    "| Retriever | Source | Package |\n",
    "| :--- | :--- | :---: |\n",
    "[WikipediaRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html) | [Wikipedia](https://www.wikipedia.org/) articles | langchain_community |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d377c-168b-40e8-bd61-af6a4fb1b44f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "If you want to get automated tracing from runs of individual tools, you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc6013-2617-4f7e-9d8b-7453d09315c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51489529-5dcd-4b86-bda6-de0a39d8ffd1",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The integration lives in the `langchain-community` package. We also need to install the `wikipedia` python package itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a737220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -qU langchain_community wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae622ac6-d18a-4754-a4bd-d30a078c19b5",
   "metadata": {},
   "source": [
    "## Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15470b-a16b-4e0d-bc6a-6998bafbb5a4",
   "metadata": {},
   "source": [
    "Now we can instantiate our retriever:\n",
    "\n",
    "`WikipediaRetriever` parameters include:\n",
    "- optional `lang`: default=\"en\". Use it to search in a specific language part of Wikipedia\n",
    "- optional `load_max_docs`: default=100. Use it to limit number of downloaded documents. It takes time to download all 100 documents, so use a small number for experiments. There is a hard limit of 300 for now.\n",
    "- optional `load_all_available_meta`: default=False. By default only the most important fields downloaded: `Published` (date when document was published/last updated), `title`, `Summary`. If True, other fields also downloaded.\n",
    "\n",
    "`get_relevant_documents()` has one argument, `query`: free text which used to find documents in Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78f0cd0-ffea-4fe3-9d1d-54639c4ef1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "retriever = WikipediaRetriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aead36-7b97-4d9c-82e7-ec644a3127f9",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a76605-6b1e-44bf-b8a2-7d48119290c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"TOKYO GHOUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ada2b7-3507-4dcb-9982-5f8f4e97a2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo Ghoul (Japanese: 東京喰種（トーキョーグール）, Hepburn: Tōkyō Gūru) is a Japanese dark fantasy manga series written and illustrated by Sui Ishida. It was serialized in Shueisha's seinen manga magazine Weekly Young Jump from September 2011 to September 2014, with its chapters collected in 14 tankōbon volumes. The story is set in an alternate version of Tokyo where humans coexist with ghouls, beings who loo\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c3d16",
   "metadata": {},
   "source": [
    "## Use within a chain\n",
    "Like other retrievers, `WikipediaRetriever` can be incorporated into LLM applications via [chains](/docs/how_to/sequence/).\n",
    "\n",
    "We will need a LLM or chat model:\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd3d268-eb8c-46e9-930a-18f5e2a50008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b52bc65-1b2e-4c30-ab43-41eaa5bf79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question based only on the context provided.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d268905-3b19-4338-ac10-223c0fe4d5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main character in Tokyo Ghoul is Ken Kaneki, who transforms into a ghoul after receiving an organ transplant from a ghoul named Rize.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"Who is the main character in `Tokyo Ghoul` and does he transform into a ghoul?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236bbafb-ebd4-4165-9b8f-d47605f6eef3",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all `WikipediaRetriever` features and configurations head to the [API reference](https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html#langchain-community-retrievers-wikipedia-wikipediaretriever)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
