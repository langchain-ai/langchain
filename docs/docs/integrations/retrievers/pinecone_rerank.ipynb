{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Pinecone Rerank\n",
    "\n",
    "> This notebook shows how to use **PineconeRerank** for two-stage vector retrieval reranking using Pinecone's hosted reranking API as demonstrated in `langchain_pinecone/libs/pinecone/rerank.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install the `langchain-pinecone` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \"langchain-pinecone\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "Set your Pinecone API key to use the reranking API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") or getpass(\n",
    "    \"Enter your Pinecone API key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "Use `PineconeRerank` to rerank a list of documents by relevance to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9998 | Content: Paris is the capital of France.\n",
      "Score: 0.1950 | Content: The Eiffel Tower is in Paris.\n",
      "Score: 0.0042 | Content: Berlin is the capital of Germany.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_pinecone import PineconeRerank\n",
    "\n",
    "# Initialize reranker\n",
    "reranker = PineconeRerank(model=\"bge-reranker-v2-m3\")\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    Document(page_content=\"Paris is the capital of France.\"),\n",
    "    Document(page_content=\"Berlin is the capital of Germany.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is in Paris.\"),\n",
    "]\n",
    "\n",
    "# Rerank documents\n",
    "query = \"What is the capital of France?\"\n",
    "reranked_docs = reranker.compress_documents(documents, query)\n",
    "\n",
    "# Print results\n",
    "for doc in reranked_docs:\n",
    "    score = doc.metadata.get(\"relevance_score\")\n",
    "    print(f\"Score: {score:.4f} | Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## Usage\n",
    "### Reranking with Top-N\n",
    "Specify `top_n` to limit the number of returned documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Result:\n",
      "Score: 0.9998 | Content: Paris is the capital of France.\n"
     ]
    }
   ],
   "source": [
    "# Return only top-1 result\n",
    "reranker_top1 = PineconeRerank(model=\"bge-reranker-v2-m3\", top_n=1)\n",
    "top1_docs = reranker_top1.compress_documents(documents, query)\n",
    "print(\"Top-1 Result:\")\n",
    "for doc in top1_docs:\n",
    "    print(f\"Score: {doc.metadata['relevance_score']:.4f} | Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": [
    "## Reranking with Custom Rank Fields\n",
    "If your documents are dictionaries or have custom fields, use `rank_fields` to specify the field to rank on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc3 | Score: 0.9892\n",
      "ID: doc1 | Score: 0.0006\n",
      "ID: doc2 | Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Sample dictionary documents with 'text' field\n",
    "docs_dict = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"text\": \"Article about renewable energy.\",\n",
    "        \"title\": \"Renewable Energy\",\n",
    "    },\n",
    "    {\"id\": \"doc2\", \"text\": \"Report on economic growth.\", \"title\": \"Economic Growth\"},\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"text\": \"News on climate policy changes.\",\n",
    "        \"title\": \"Climate Policy\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Initialize reranker with rank_fields\n",
    "reranker_text = PineconeRerank(model=\"bge-reranker-v2-m3\", rank_fields=[\"text\"])\n",
    "climate_docs = reranker_text.rerank(docs_dict, \"Latest news on climate change.\")\n",
    "\n",
    "# Show IDs and scores\n",
    "for res in climate_docs:\n",
    "    print(f\"ID: {res['id']} | Score: {res['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bb6c3",
   "metadata": {},
   "source": [
    "We can rerank based on title field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f2768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc2 | Score: 0.8918 | Title: Economic Growth\n",
      "ID: doc3 | Score: 0.0002 | Title: Climate Policy\n",
      "ID: doc1 | Score: 0.0000 | Title: Renewable Energy\n"
     ]
    }
   ],
   "source": [
    "economic_docs = reranker_text.rerank(docs_dict, \"Economic forecast.\")\n",
    "\n",
    "# Show IDs and scores\n",
    "for res in economic_docs:\n",
    "    print(\n",
    "        f\"ID: {res['id']} | Score: {res['score']:.4f} | Title: {res['document']['title']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## Reranking with Additional Parameters\n",
    "You can pass model-specific parameters (e.g., `truncate`) directly to `.rerank()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c501c",
   "metadata": {},
   "source": [
    "How to handle inputs longer than those supported by the model. Accepted values: END or NONE.\n",
    "END truncates the input sequence at the input token limit. NONE returns an error when the input exceeds the input token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: docA | Score: 0.6950\n",
      "ID: docB | Score: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Rerank with custom truncate parameter\n",
    "docs_simple = [\n",
    "    {\"id\": \"docA\", \"text\": \"Quantum entanglement is a physical phenomenon...\"},\n",
    "    {\"id\": \"docB\", \"text\": \"Classical mechanics describes motion...\"},\n",
    "]\n",
    "\n",
    "reranked = reranker.rerank(\n",
    "    documents=docs_simple,\n",
    "    query=\"Explain the concept of quantum entanglement.\",\n",
    "    truncate=\"END\",\n",
    ")\n",
    "# Print reranked IDs and scores\n",
    "for res in reranked:\n",
    "    print(f\"ID: {res['id']} | Score: {res['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78bcd8",
   "metadata": {},
   "source": [
    "## Use within a chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4d11e",
   "metadata": {},
   "source": [
    "Create a retreiver from a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c7a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First setup a vector store\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "\n",
    "pinecone_client = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "index_name = \"langchain-test-index\"  # change if desired\n",
    "\n",
    "if not pinecone_client.has_index(index_name):\n",
    "    pinecone_client.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "\n",
    "# Pinecone index\n",
    "pinecone_index = pinecone_client.Index(index_name)\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Vector store\n",
    "vector_store = PineconeVectorStore(index=pinecone_index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edd9bf",
   "metadata": {},
   "source": [
    "### Manage vector store\n",
    "\n",
    "Once you have created your vector store, we can interact with it by adding and deleting different items.\n",
    "\n",
    "#### Add items to vector store\n",
    "\n",
    "We can add items to our vector store by using the `add_documents` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce4c65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0ffee2f7-7313-4a5c-a4b4-f1be7bcdfe58',\n",
       " '1e6e5965-d7fd-498d-b9d2-b6e3a30a25bc',\n",
       " 'a11e9373-b6aa-447c-95fd-936574eb6e29',\n",
       " '40115da7-56ca-4394-b3f0-2c5f195cd321',\n",
       " 'cec08688-05cc-423e-a8dc-469126c9fa6b',\n",
       " '73aaf41e-4d9e-4ea3-9c3e-892e1fc100ac',\n",
       " '0a55243c-653c-4325-b972-4a80874608a9',\n",
       " 'a929a129-4ee6-4134-8438-a09aa3336c57',\n",
       " 'caf87d2a-6fb6-47bf-be9e-c11d001ddf92',\n",
       " '0e37f7dd-317d-4a3e-bd29-dc404f380ba1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa20ff",
   "metadata": {},
   "source": [
    "### Turning into retriever\n",
    "You can also transform the vector store into a retriever for easier usage in your chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f213d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 1, \"score_threshold\": 0.4},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152ae20",
   "metadata": {},
   "source": [
    "### Combining into chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9422994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document text: The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\n",
      "Relevance score: 0.9839091\n",
      "Source: news\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_pinecone import PineconeRerank \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "compressor = PineconeRerank(model=\"bge-reranker-v2-m3\")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What is the weather tomorrow?\"\n",
    ")\n",
    "\n",
    "for doc in compressed_docs:\n",
    "    print(f\"Document text: {doc.page_content}\")\n",
    "    print(f\"Relevance score: {doc.metadata.get(\"relevance_score\")}\")\n",
    "    print(f\"Source: {doc.metadata.get(\"source\")}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cfdc4",
   "metadata": {},
   "source": [
    "### Using this retriever within a QA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc8538b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakit/customers/aurelio/pinecone_repos/langchain-pinecone/libs/pinecone/.venv/lib/python3.12/site-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The weather tomorrow will be cloudy and overcast with a high of 62 degrees.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": compression_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "qa_chain.invoke(\"What is the weather tomorrow?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "source": [
    "## API reference\n",
    "- `PineconeRerank(model, top_n, rank_fields, return_documents)`\n",
    "- `.rerank(documents, query, rank_fields=None, model=None, top_n=None, truncate=\"END\")`\n",
    "- `.compress_documents(documents, query)` (returns `Document` objects with `relevance_score` in metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113dda6",
   "metadata": {},
   "source": [
    "## Related\n",
    "- Retriever [conceptual guide](https://python.langchain.com/docs/concepts/retrievers/)\n",
    "- Retriever [how-to guides](https://python.langchain.com/docs/how_to/#retrievers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
