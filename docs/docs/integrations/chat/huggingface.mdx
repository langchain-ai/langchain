---
title: Hugging Face (chat)
sidebar_label: Hugging Face
---

## Overview

This page shows how to use Hugging Face models as chat models in LangChain.

## Setup

Install the required packages:

```bash
pip install langchain-huggingface transformers
```

> For Hugging Face pipelines, prefer `max_new_tokens` (not `max_tokens`). The pipeline will use CPU/GPU automatically depending on availability.

## Instantiation

### Option 1 (works today): pipeline → wrap with `ChatHuggingFace`

```python
from transformers import pipeline
from langchain_huggingface import ChatHuggingFace

# Create a text-generation pipeline (CPU/GPU as available)
pipe = pipeline(
    "text-generation",
    model="microsoft/Phi-3-mini-4k-instruct",
    do_sample=False,        # deterministic
    max_new_tokens=128,     # HF uses max_new_tokens (not max_tokens)
)

# Wrap the pipeline as a LangChain chat model
llm = ChatHuggingFace(llm=pipe)
```

### Option 2 (coming after fix): `init_chat_model(..., model_provider="huggingface")`

Once available in your version, you can initialize via `init_chat_model`:

```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    model="microsoft/Phi-3-mini-4k-instruct",
    model_provider="huggingface",
    task="text-generation",
    do_sample=False,
    max_new_tokens=128,
)
```

> If your version doesn’t support this yet, use **Option 1** above.

## Invocation

```python
msg = llm.invoke("Say hi in one sentence.")
print(msg.content)
```

## Chaining

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are helpful."),
    ("human", "{question}"),
])

chain = prompt | llm
result = chain.invoke({"question": "What is the capital of France?"})
print(result.content)
```

## API reference

- `langchain_huggingface.ChatHuggingFace`
- `transformers.pipeline` (Hugging Face)
- `langchain.chat_models.init_chat_model` (when available for Hugging Face)


