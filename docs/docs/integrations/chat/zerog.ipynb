{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: 0G Compute Network\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatZeroG\n",
    "\n",
    "This page will help you get started with 0G Compute Network [chat models](../../concepts/chat_models.mdx). For detailed documentation of all `ChatZeroG` features and configurations, head to the [API reference](https://python.langchain.com/api_reference/zerog/chat_models/langchain_zerog.chat_models.ChatZeroG.html).\n",
    "\n",
    "The [0G Compute Network](https://0g.ai/) provides decentralized AI inference services with verified computations running in Trusted Execution Environments (TEE). This integration allows you to use 0G's distributed GPU network for AI inference through LangChain.\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | Serializable | JS support | Package downloads | Package latest |\n",
    "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
    "| [ChatZeroG](https://python.langchain.com/api_reference/zerog/chat_models/langchain_zerog.chat_models.ChatZeroG.html) | [langchain-zerog](https://python.langchain.com/api_reference/zerog/index.html) | ❌ | ✅ | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-zerog?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-zerog?style=flat-square&label=%20) |\n",
    "\n",
    "### Model features\n",
    "| [Tool calling](../../how_to/tool_calling.ipynb) | [Structured output](../../how_to/structured_output.ipynb) | JSON mode | [Image input](../../how_to/multimodal_inputs.ipynb) | Audio input | Video input | [Token-level streaming](../../how_to/chat_streaming.ipynb) | Native async | [Token usage](../../how_to/chat_token_usage_tracking.ipynb) | [Logprobs](../../how_to/logprobs.ipynb) |\n",
    "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | ❌ |\n",
    "\n",
    "## Setup\n",
    "\n",
    "To access 0G Compute Network models, you'll need an Ethereum private key for wallet authentication, OG tokens to fund your account, and the `langchain-zerog` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "You'll need an Ethereum private key and OG tokens. Set the `ZEROG_PRIVATE_KEY` environment variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"ZEROG_PRIVATE_KEY\" not in os.environ:\n",
    "    os.environ[\"ZEROG_PRIVATE_KEY\"] = getpass.getpass(\n",
    "        \"Enter your Ethereum private key: \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain 0G integration lives in the `langchain-zerog` package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-zerog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Now we can instantiate our model object. Note that you'll need to fund your account before making requests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_zerog import ChatZeroG\n",
    "\n",
    "llm = ChatZeroG(\n",
    "    model=\"llama-3.3-70b-instruct\",  # or \"deepseek-r1-70b\"\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account Management\n",
    "\n",
    "Before using the model, you need to fund your account with OG tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fund your account (required before first use)\n",
    "await llm.fund_account(\"0.1\")  # Add 0.1 OG tokens\n",
    "\n",
    "# Check your balance\n",
    "balance = await llm.get_balance()\n",
    "print(f\"Available balance: {balance['available']} OG tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "]\n",
    "\n",
    "# Use async invocation for 0G network\n",
    "ai_msg = await llm.ainvoke(messages)\n",
    "print(ai_msg.content)\n",
    "print(f\"Usage: {ai_msg.usage_metadata}\")\n",
    "print(f\"Provider: {ai_msg.response_metadata['provider_address']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "The 0G integration supports token-level streaming for real-time responses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in llm.astream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "The 0G integration supports tool calling for function execution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    unit: str = Field(default=\"celsius\", description=\"Temperature unit\")\n",
    "\n",
    "\n",
    "# Bind tools to the model\n",
    "llm_with_tools = llm.bind_tools([GetWeather])\n",
    "\n",
    "# Invoke with a tool-calling request\n",
    "tool_message = [HumanMessage(content=\"What's the weather like in San Francisco?\")]\n",
    "\n",
    "ai_msg = await llm_with_tools.ainvoke(tool_message)\n",
    "print(f\"Tool calls: {ai_msg.tool_calls}\")\n",
    "print(f\"Content: {ai_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Use structured output to get responses in a specific format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: int = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "# Create structured output model\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "# Get a structured response\n",
    "joke = await structured_llm.ainvoke(\"Tell me a joke about artificial intelligence\")\n",
    "print(f\"Setup: {joke.setup}\")\n",
    "print(f\"Punchline: {joke.punchline}\")\n",
    "print(f\"Rating: {joke.rating}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Models\n",
    "\n",
    "The 0G Compute Network currently supports the following models:\n",
    "\n",
    "- **llama-3.3-70b-instruct**: State-of-the-art 70B parameter model for general AI tasks (TEE verified)\n",
    "- **deepseek-r1-70b**: Advanced reasoning model optimized for complex problem solving (TEE verified)\n",
    "\n",
    "All models run in Trusted Execution Environments (TEE) for verified computation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
