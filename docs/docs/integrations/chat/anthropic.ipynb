{
 "cells": [
  {
   "cell_type": "raw",
   "id": "afaf8039",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Anthropic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f1e0d",
   "metadata": {},
   "source": [
    "# ChatAnthropic\n",
    "\n",
    "This notebook provides a quick overview for getting started with Anthropic [chat models](/docs/concepts/chat_models). For detailed documentation of all ChatAnthropic features and configurations head to the [API reference](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html).\n",
    "\n",
    "Anthropic has several chat models. You can find information about their latest models and their costs, context windows, and supported input types in the [Anthropic docs](https://docs.anthropic.com/en/docs/models-overview).\n",
    "\n",
    "\n",
    ":::info AWS Bedrock and Google VertexAI\n",
    "\n",
    "Note that certain Anthropic models can also be accessed via AWS Bedrock and Google VertexAI. See the [ChatBedrock](/docs/integrations/chat/bedrock/) and [ChatVertexAI](/docs/integrations/chat/google_vertex_ai_palm/) integrations to use Anthropic models via these services.\n",
    "\n",
    ":::\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/anthropic) | Package downloads | Package latest |\n",
    "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
    "| [ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [langchain-anthropic](https://python.langchain.com/api_reference/anthropic/index.html) | ❌ | beta | ✅ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-anthropic?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-anthropic?style=flat-square&label=%20) |\n",
    "\n",
    "### Model features\n",
    "| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n",
    "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | ❌ | \n",
    "\n",
    "## Setup\n",
    "\n",
    "To access Anthropic models you'll need to create an Anthropic account, get an API key, and install the `langchain-anthropic` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "Head to https://console.anthropic.com/ to sign up for Anthropic and generate an API key. Once you've done this set the ANTHROPIC_API_KEY environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433e8d2b-9519-4b49-b2c4-7ab65b046c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee0c4b-9764-423a-9dbf-95129e185210",
   "metadata": {},
   "source": [
    "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15d341e-3e26-4ca3-830b-5aab30ed66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730d6a1-c893-4840-9817-5e5251676d5d",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain Anthropic integration lives in the `langchain-anthropic` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d6238-1f87-422a-b135-f5abbb8652fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cde65-254d-4219-a441-068766c0d4b5",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Now we can instantiate our model object and generate chat completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {},
   "source": [
    "## Invocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e0dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", response_metadata={'id': 'msg_018Nnu76krRPq8HvgKLW4F8T', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 29, 'output_tokens': 11}}, id='run-57e9295f-db8a-48dc-9619-babd2bedd891-0', usage_metadata={'input_tokens': 29, 'output_tokens': 11, 'total_tokens': 40})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2bfc0-7e78-4528-a73f-499ac150dca8",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e197d1d7-a070-4c96-9f8a-a0e86d046e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's the German translation:\\n\\nIch liebe Programmieren.\", response_metadata={'id': 'msg_01GhkRtQZUkA5Ge9hqmD8HGY', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 23, 'output_tokens': 18}}, id='run-da5906b4-b200-4e08-b81a-64d4453643b6-0', usage_metadata={'input_tokens': 23, 'output_tokens': 18, 'total_tokens': 41})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee55bc-ffc8-4cfa-801c-993953a08cfd",
   "metadata": {},
   "source": [
    "## Content blocks\n",
    "\n",
    "One key difference to note between Anthropic models and most others is that the contents of a single Anthropic AI message can either be a single string or a **list of content blocks**. For example when an Anthropic model invokes a tool, the tool invocation is part of the message content (as well as being exposed in the standardized `AIMessage.tool_calls`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a374a24-2534-4e6f-825b-30fab7bbe0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"To answer this question, we'll need to check the current weather in both Los Angeles (LA) and New York (NY). I'll use the GetWeather function to retrieve this information for both cities.\",\n",
       "  'type': 'text'},\n",
       " {'id': 'toolu_01Ddzj5PkuZkrjF4tafzu54A',\n",
       "  'input': {'location': 'Los Angeles, CA'},\n",
       "  'name': 'GetWeather',\n",
       "  'type': 'tool_use'},\n",
       " {'id': 'toolu_012kz4qHZQqD4qg8sFPeKqpP',\n",
       "  'input': {'location': 'New York, NY'},\n",
       "  'name': 'GetWeather',\n",
       "  'type': 'tool_use'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([GetWeather])\n",
    "ai_msg = llm_with_tools.invoke(\"Which city is hotter today: LA or NY?\")\n",
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4a1ead-952c-489f-a8d4-355d3fb55f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'GetWeather',\n",
       "  'args': {'location': 'Los Angeles, CA'},\n",
       "  'id': 'toolu_01Ddzj5PkuZkrjF4tafzu54A'},\n",
       " {'name': 'GetWeather',\n",
       "  'args': {'location': 'New York, NY'},\n",
       "  'id': 'toolu_012kz4qHZQqD4qg8sFPeKqpP'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d372f-4dec-43e6-b58c-eee25633e1a6",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "Anthropic supports a [citations](https://docs.anthropic.com/en/docs/build-with-claude/citations) feature that lets Claude attach context to its answers based on source documents supplied by the user. When [document content blocks](https://docs.anthropic.com/en/docs/build-with-claude/citations#document-types) with `\"citations\": {\"enabled\": True}` are included in a query, Claude may generate citations in its response.\n",
    "\n",
    "### Simple example\n",
    "\n",
    "In this example we pass a [plain text document](https://docs.anthropic.com/en/docs/build-with-claude/citations#plain-text-documents). In the background, Claude [automatically chunks](https://docs.anthropic.com/en/docs/build-with-claude/citations#plain-text-documents) the input text into sentences, which are used when generating citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5370e6e-5a9a-4546-848b-5f5bf313c3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Based on the document, ', 'type': 'text'},\n",
       " {'text': 'the grass is green',\n",
       "  'type': 'text',\n",
       "  'citations': [{'type': 'char_location',\n",
       "    'cited_text': 'The grass is green. ',\n",
       "    'document_index': 0,\n",
       "    'document_title': 'My Document',\n",
       "    'start_char_index': 0,\n",
       "    'end_char_index': 20}]},\n",
       " {'text': ', and ', 'type': 'text'},\n",
       " {'text': 'the sky is blue',\n",
       "  'type': 'text',\n",
       "  'citations': [{'type': 'char_location',\n",
       "    'cited_text': 'The sky is blue.',\n",
       "    'document_index': 0,\n",
       "    'document_title': 'My Document',\n",
       "    'start_char_index': 20,\n",
       "    'end_char_index': 36}]},\n",
       " {'text': '.', 'type': 'text'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"document\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"media_type\": \"text/plain\",\n",
    "                    \"data\": \"The grass is green. The sky is blue.\",\n",
    "                },\n",
    "                \"title\": \"My Document\",\n",
    "                \"context\": \"This is a trustworthy document.\",\n",
    "                \"citations\": {\"enabled\": True},\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69956596-0e6c-492b-934d-c08ed3c9de9a",
   "metadata": {},
   "source": [
    "### Using with text splitters\n",
    "\n",
    "Anthropic also lets you specify your own splits using [custom document](https://docs.anthropic.com/en/docs/build-with-claude/citations#custom-content-documents) types. LangChain [text splitters](/docs/concepts/text_splitters/) can be used to generate meaningful splits for this purpose. See the below example, where we split the LangChain README (a markdown document) and pass it to Claude as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04cc2841-7987-47a5-906c-09ea7fa28323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"You can find LangChain's tutorials at https://python.langchain.com/docs/tutorials/\\n\\nThe tutorials section is recommended for those looking to build something specific or who prefer a hands-on learning approach. It's considered the best place to get started with LangChain.\",\n",
       "  'type': 'text',\n",
       "  'citations': [{'type': 'content_block_location',\n",
       "    'cited_text': \"[Tutorials](https://python.langchain.com/docs/tutorials/):If you're looking to build something specific orare more of a hands-on learner, check out ourtutorials. This is the best place to get started.\",\n",
       "    'document_index': 0,\n",
       "    'document_title': None,\n",
       "    'start_block_index': 243,\n",
       "    'end_block_index': 248}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "\n",
    "\n",
    "def format_to_anthropic_documents(documents: list[str]):\n",
    "    return {\n",
    "        \"type\": \"document\",\n",
    "        \"source\": {\n",
    "            \"type\": \"content\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": document} for document in documents],\n",
    "        },\n",
    "        \"citations\": {\"enabled\": True},\n",
    "    }\n",
    "\n",
    "\n",
    "# Pull readme\n",
    "get_response = requests.get(\n",
    "    \"https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\"\n",
    ")\n",
    "readme = get_response.text\n",
    "\n",
    "# Split into chunks\n",
    "splitter = MarkdownTextSplitter(\n",
    "    chunk_overlap=0,\n",
    "    chunk_size=50,\n",
    ")\n",
    "documents = splitter.split_text(readme)\n",
    "\n",
    "# Construct message\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        format_to_anthropic_documents(documents),\n",
    "        {\"type\": \"text\", \"text\": \"Give me a link to LangChain's tutorials.\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Query LLM\n",
    "llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n",
    "response = llm.invoke([message])\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all ChatAnthropic features and configurations head to the API reference: https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
