{
 "cells": [
  {
   "cell_type": "raw",
   "id": "afaf8039",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "sidebar_label: Opper Chat Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f1e0d",
   "metadata": {},
   "source": [
    "# Opper Chat Models\n",
    "\n",
    "This guide will help you get started with Opper [chat models](/docs/concepts/chat_models) through the `langchain-opperai` integration. Opper provides a unified API for building AI applications with structured input/output, tracing, and model-independent code.\n",
    "\n",
    "For detailed information about Opper's capabilities, visit [docs.opper.ai](https://docs.opper.ai).\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | Serializable | PY support | Package downloads | Package latest |\n",
    "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
    "| [ChatOpperAI](https://github.com/opper-ai/integration-langchain) | [langchain-opperai](https://github.com/opper-ai/integration-langchain) | ✅ | ✅ | ✅ | ![GitHub](https://img.shields.io/github/stars/opper-ai/integration-langchain?style=flat-square) | ![GitHub release](https://img.shields.io/github/v/release/opper-ai/integration-langchain?style=flat-square) |\n",
    "\n",
    "### Model features\n",
    "| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n",
    "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ✅ | ❌ | \n",
    "\n",
    "## Setup\n",
    "\n",
    "To access Opper models you'll need to create an Opper account, get an API key, and install the `langchain-opper` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "Head to [platform.opper.ai](https://platform.opper.ai) to sign up to Opper and generate an API key. Once you've done this, set the `OPPER_API_KEY` environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433e8d2b-9519-4b49-b2c4-7ab65b046c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"OPPER_API_KEY\"):\n",
    "    os.environ[\"OPPER_API_KEY\"] = getpass.getpass(\"Enter your Opper API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee0c4b-9764-423a-9dbf-95129e185210",
   "metadata": {},
   "source": [
    "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15d341e-3e26-4ca3-830b-5aab30ed66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730d6a1-c893-4840-9817-5e5251676d5d",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain Opper integration lives in the `langchain-opperai` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652d6238-1f87-422a-b135-f5abbb8652fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-opperai (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-opperai\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-opperai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cde65-254d-4219-a441-068766c0d4b5",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Now we can instantiate our model object and generate chat completions. Opper provides a `OpperProvider` for easy model management and tracing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s the translation of \"Hello, world!\" to several common languages:\\n\\nSpanish: ¡Hola, mundo!\\nFrench: Bonjour, monde!\\nGerman: Hallo, Welt!\\nItalian: Ciao, mondo!\\nPortuguese: Olá, mundo!\\nJapanese: こんにちは、世界！\\nChinese (Simplified): 你好，世界！\\nRussian: Привет, мир!\\n\\nWould you like the translation in any specific language?' additional_kwargs={'span_id': '52073b05-55e3-44b2-8015-63eb24615fce', 'structured': False} response_metadata={} id='run--08e2aba0-f20c-4de2-a18a-7eb5b63ba228-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_opperai import OpperProvider, ChatOpperAI\n",
    "\n",
    "# Using the provider (recommended for tracing and model management)\n",
    "provider = OpperProvider()\n",
    "llm = provider.create_chat_model(\n",
    "    task_name=\"translation\",\n",
    "    instructions=\"You are a helpful assistant that translates text.\",\n",
    ")\n",
    "\n",
    "# Or directly instantiate the chat model\n",
    "llm = ChatOpperAI(\n",
    "    task_name=\"translation\",\n",
    "    instructions=\"You are a helpful assistant that translates text.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {},
   "source": [
    "## Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62e0dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Translation to French:\n",
      "\"J'aime la programmation.\"\n",
      "Span ID: ccb8b9c2-c18f-4842-9b05-ef1d260caf4e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Translation to French:\\n\"J\\'aime la programmation.\"', additional_kwargs={'span_id': 'ccb8b9c2-c18f-4842-9b05-ef1d260caf4e', 'structured': False}, response_metadata={}, id='run--f238cae4-81df-4e2d-a4f7-b2cc4b7b57dd-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(f\"Response: {ai_msg.content}\")\n",
    "print(f\"Span ID: {ai_msg.additional_kwargs.get('span_id', 'N/A')}\")\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation to French:\n",
      "\"J'aime la programmation.\"\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2bfc0-7e78-4528-a73f-499ac150dca8",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e197d1d7-a070-4c96-9f8a-a0e86d046e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: \"Ich liebe Programmierung.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Ich liebe Programmierung.\"', additional_kwargs={'span_id': 'a0445f35-f428-4774-9e98-e3a16c9aac2d', 'structured': False}, response_metadata={}, id='run--edc047f6-30a1-4da3-beb2-bcec2b67cc95-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")\n",
    "print(f\"Translation: {result.content}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee55bc-ffc8-4cfa-801c-993953a08cfd",
   "metadata": {},
   "source": [
    "## Structured Input and Output\n",
    "\n",
    "One of Opper's key features is structured output using Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6165fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, world!\n",
      "Translation: ¡Hola, mundo!\n",
      "Confidence: 0.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TranslationOutput(thoughts='Processing simple greeting translation from English to Spanish. This is a common phrase with standard translation.', original_text='Hello, world!', translated_text='¡Hola, mundo!', source_language='English', target_language='Spanish', confidence=0.98)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "class TranslationInput(BaseModel):\n",
    "    \"\"\"Structured input for translations.\"\"\"\n",
    "    text: str = Field(description=\"The text to translate\")\n",
    "    target_language: str = Field(description=\"The target language for translation\")\n",
    "    source_language: str = Field(description=\"The source language (optional)\", default=\"auto-detect\")\n",
    "\n",
    "class TranslationOutput(BaseModel):\n",
    "    \"\"\"Structured output for translations.\"\"\"\n",
    "    thoughts: str = Field(description=\"Translation analysis process\")\n",
    "    original_text: str = Field(description=\"The original text\")\n",
    "    translated_text: str = Field(description=\"The translated text\")\n",
    "    source_language: str = Field(description=\"Detected source language\")\n",
    "    target_language: str = Field(description=\"Target language\")\n",
    "    confidence: float = Field(description=\"Translation confidence (0-1)\", ge=0, le=1)\n",
    "\n",
    "# Create a structured model with both input and output schemas\n",
    "structured_llm = provider.create_structured_model(\n",
    "    task_name=\"structured_translation\",\n",
    "    instructions=\"Translate text and provide structured output with metadata.\",\n",
    "    output_schema=TranslationOutput\n",
    ")\n",
    "\n",
    "# Create structured input for translation\n",
    "translation_request = TranslationInput(\n",
    "    text=\"Hello, world!\",\n",
    "    target_language=\"Spanish\",\n",
    "    source_language=\"English\"\n",
    ")\n",
    "\n",
    "# Create message with structured input\n",
    "message = HumanMessage(\n",
    "    content=\"Translate the provided text to the specified language\",\n",
    "    additional_kwargs=translation_request.model_dump(),\n",
    ")\n",
    "\n",
    "# Invoke with structured message\n",
    "result = structured_llm.invoke([message])\n",
    "\n",
    "# Access the parsed structured output\n",
    "parsed_output = result.additional_kwargs.get(\"parsed\")\n",
    "print(f\"Original: {parsed_output.original_text}\")\n",
    "print(f\"Translation: {parsed_output.translated_text}\")\n",
    "print(f\"Confidence: {parsed_output.confidence}\")\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c411b",
   "metadata": {},
   "source": [
    "## Tracing and Observability\n",
    "\n",
    "Opper provides built-in tracing for observability across your AI workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab409c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started trace: 0135cd76-52c6-4629-8cc1-7b4ab9918122\n",
      "Call 1 span: 01d70752-5195-489d-b4e2-2c284ebb9a78\n",
      "Call 2 span: fb6937ee-45f8-4f52-b472-875af4c91e2f\n",
      "Trace completed - view in Opper dashboard\n"
     ]
    }
   ],
   "source": [
    "# Start a trace for a workflow\n",
    "trace_id = provider.start_trace(\"translation_workflow\", \"Translate multiple texts\")\n",
    "print(f\"Started trace: {trace_id}\")\n",
    "\n",
    "# All subsequent calls will be part of this trace\n",
    "result1 = llm.invoke([(\"human\", \"Translate 'Good morning' to French\")])\n",
    "result2 = llm.invoke([(\"human\", \"Translate 'Good evening' to Spanish\")])\n",
    "\n",
    "print(f\"Call 1 span: {result1.additional_kwargs.get('span_id')}\")\n",
    "print(f\"Call 2 span: {result2.additional_kwargs.get('span_id')}\")\n",
    "\n",
    "# End the trace\n",
    "provider.end_trace(\"Translation workflow completed\")\n",
    "print(\"Trace completed - view in Opper dashboard\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f822a",
   "metadata": {},
   "source": [
    "## LangGraph Integration\n",
    "\n",
    "Opper works seamlessly with LangGraph for building complex AI workflows. Here's a simple multi-researcher demo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8437b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Research: I'll analyze the market opportunity for an AI-powered personalized learning platform across key dimensions:\n",
      "\n",
      "Market Overview:\n",
      "- Global edtech market size: $254.8B (2021), projected CAGR of 13.5% throu...\n",
      "Tech Research: Technical Feasibility Analysis: AI-Powered Personalized Learning Platform\n",
      "\n",
      "1. Core Technical Components\n",
      "\n",
      "a) Learning Management System (LMS) Foundation\n",
      "- Cloud-based architecture for scalability\n",
      "- Mic...\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    \"\"\"State for research workflow.\"\"\"\n",
    "    messages: Annotated[List, add_messages]\n",
    "    query: str\n",
    "    provider: OpperProvider\n",
    "    market_research: str\n",
    "    tech_research: str\n",
    "\n",
    "def market_researcher(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Market research specialist.\"\"\"\n",
    "    provider = state[\"provider\"]\n",
    "    \n",
    "    market_model = provider.create_chat_model(\n",
    "        task_name=\"market_research\",\n",
    "        instructions=\"You are a market research specialist. Focus on market opportunities and business viability.\"\n",
    "    )\n",
    "    \n",
    "    result = market_model.invoke([\n",
    "        (\"human\", f\"Conduct market research on: {state['query']}\")\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"market_research\": result.content,\n",
    "        \"messages\": state[\"messages\"] + [result]\n",
    "    }\n",
    "\n",
    "def tech_researcher(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Technical research specialist.\"\"\" \n",
    "    provider = state[\"provider\"]\n",
    "    \n",
    "    tech_model = provider.create_chat_model(\n",
    "        task_name=\"tech_research\", \n",
    "        instructions=\"You are a technical research specialist. Focus on implementation feasibility and architecture.\"\n",
    "    )\n",
    "    \n",
    "    result = tech_model.invoke([\n",
    "        (\"human\", f\"Conduct technical feasibility research on: {state['query']}\")\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"tech_research\": result.content,\n",
    "        \"messages\": state[\"messages\"] + [result]\n",
    "    }\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(ResearchState)\n",
    "workflow.add_node(\"market_researcher\", market_researcher)\n",
    "workflow.add_node(\"tech_researcher\", tech_researcher)\n",
    "\n",
    "workflow.set_entry_point(\"market_researcher\")\n",
    "workflow.add_edge(\"market_researcher\", \"tech_researcher\")\n",
    "workflow.add_edge(\"tech_researcher\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run workflow\n",
    "provider_instance = OpperProvider()\n",
    "trace_id = provider_instance.start_trace(\"research_workflow\", \"AI platform research\")\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"query\": \"AI-powered personalized learning platform\",\n",
    "    \"provider\": provider_instance,\n",
    "    \"market_research\": \"\",\n",
    "    \"tech_research\": \"\"\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "print(\"Market Research:\", final_state[\"market_research\"][:200] + \"...\")\n",
    "print(\"Tech Research:\", final_state[\"tech_research\"][:200] + \"...\")\n",
    "\n",
    "provider_instance.end_trace(\"Research workflow completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70bd6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
