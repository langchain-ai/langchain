# MongoDB Atlas

>[MongoDB Atlas](https://www.mongodb.com/docs/atlas/) is a fully-managed cloud
> database available in AWS, Azure, and GCP.  It now has support for native 
> Vector Search on the MongoDB document data.

## Installation and Setup

See [detail configuration instructions](/docs/integrations/vectorstores/mongodb_atlas).

We need to install `langchain-mongodb` python package.

```bash
pip install langchain-mongodb
```

## Vector Store

See a [usage example](/docs/integrations/vectorstores/mongodb_atlas).

```python
from langchain_mongodb import MongoDBAtlasVectorSearch
```

## Retrievers

### Full Text Search Retriever

>`Hybrid Search Retriever` performs full-text searches using 
> Luceneâ€™s standard (`BM25`) analyzer.

```python
from langchain_mongodb.retrievers import MongoDBAtlasFullTextSearchRetriever
```

### Hybrid Search Retriever

>`Hybrid Search Retriever` combines vector and full-text searches weighting 
> them the via `Reciprocal Rank Fusion` (`RRF`) algorithm.
 
```python
from langchain_mongodb.retrievers import MongoDBAtlasHybridSearchRetriever
```

## Model Caches

### MongoDBCache

An abstraction to store a simple cache in MongoDB. This does not use Semantic Caching, nor does it require an index to be made on the collection before generation.

To import this cache:
```python
from langchain_mongodb.cache import MongoDBCache
```

To use this cache with your LLMs:
```python
from langchain_core.globals import set_llm_cache

# use any embedding provider...
from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings

mongodb_atlas_uri = "<YOUR_CONNECTION_STRING>"
COLLECTION_NAME="<YOUR_CACHE_COLLECTION_NAME>"
DATABASE_NAME="<YOUR_DATABASE_NAME>"

set_llm_cache(MongoDBCache(
    connection_string=mongodb_atlas_uri,
    collection_name=COLLECTION_NAME,
    database_name=DATABASE_NAME,
))
```


### MongoDBAtlasSemanticCache
Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends MongoDBAtlas as both a cache and a vectorstore.
The MongoDBAtlasSemanticCache inherits from `MongoDBAtlasVectorSearch` and needs an Atlas Vector Search Index defined to work. Please look at the [usage example](/docs/integrations/vectorstores/mongodb_atlas) on how to set up the index.

To import this cache:
```python
from langchain_mongodb.cache import MongoDBAtlasSemanticCache
```

To use this cache with your LLMs:
```python
from langchain_core.globals import set_llm_cache

# use any embedding provider...
from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings

mongodb_atlas_uri = "<YOUR_CONNECTION_STRING>"
COLLECTION_NAME="<YOUR_CACHE_COLLECTION_NAME>"
DATABASE_NAME="<YOUR_DATABASE_NAME>"

set_llm_cache(MongoDBAtlasSemanticCache(
    embedding=FakeEmbeddings(),
    connection_string=mongodb_atlas_uri,
    collection_name=COLLECTION_NAME,
    database_name=DATABASE_NAME,
))
```
``