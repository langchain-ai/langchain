{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunPod\n",
    "\n",
    "This notebook demonstrates how to use [RunPod](https://www.runpod.io/) with LangChain to run your own LLMs, embeddings models, and more. RunPod provides GPU cloud computing with both dedicated and serverless options, allowing you to deploy inference endpoints for your AI applications.\n",
    "You'll learn how to:\n",
    "\n",
    "- Set up a RunPod account and obtain an API key\n",
    "- Create a serverless endpoint on RunPod\n",
    "- Connect LangChain to your RunPod endpoints\n",
    "- Use streaming with LLMs\n",
    "- Configure different models and parameters\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, we need to install the `runpod` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain-runpod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also install a few other packages that will be useful for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up RunPod\n",
    "### Getting a RunPod API Key\n",
    "To use RunPod with LangChain, you'll need a RunPod account and an API key. Here's how to get one:\n",
    "1. Create an account on RunPod\n",
    "2. Navigate to your account settings by clicking on your profile in the top-right corner\n",
    "3. Select \"API Keys\" from the left sidebar\n",
    "4. Click \"Create API Key\", give it a name, and copy the key\n",
    "\n",
    "Let's set up the API key as an environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your RunPod API key\n",
    "os.environ[\"RUNPOD_API_KEY\"] = \"your-runpod-api-key\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a RunPod Serverless Endpoint\n",
    "To use LLMs through RunPod, you need to set up a serverless endpoint. Here's a step-by-step guide:\n",
    "1. Log in to your RunPod account\n",
    "2. Navigate to the \"Serverless\" section from the dashboard\n",
    "3. Click \"New Endpoint\"\n",
    "4. Select a template based on the LLM you want to use\n",
    "    - For text-based LLMs, good options include templates for:\n",
    "        - vLLM (recommended for performance)\n",
    "        - Text Generation WebUI\n",
    "        - FastChat\n",
    "5. Choose a GPU type based on your model's requirements\n",
    "    - For Llama 3 70B, consider at least an A100 or H100\n",
    "    - For smaller models (7B, 13B), RTX 4090 or A10 GPUs may be sufficient\n",
    "6. Configure your endpoint settings:\n",
    "    - Set a minimum and maximum number of workers\n",
    "    - Set the idle timeout\n",
    "    - Configure any template-specific settings\n",
    "7. Deploy your endpoint\n",
    "\n",
    "Once deployed, note your endpoint ID - you'll need this to connect LangChain to your endpoint.\n",
    "**Important Tip**: Make sure your endpoint is running a text-based LLM server that accepts standard input/output formats. RunPod has text based templates we recomennd usingfound inthe serverless section of the RunPod dashboard.\n",
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_runpod import ChatRunPod\n",
    "\n",
    "# Initialize the ChatRunPod model\n",
    "# Replace \"endpoint-id\" with your actual RunPod endpoint ID\n",
    "chat = ChatRunPod(\n",
    "    endpoint_id=\"endpoint-id\",  # Your RunPod serverless endpoint ID\n",
    "    model_name=\"llama3-70b-chat\",  # Optional - helps with identification\n",
    "    temperature=0.7,  # Control randomness (0.0 to 1.0)\n",
    "    max_tokens=1024,  # Maximum tokens in the response\n",
    "    # api_key=\"your-runpod-api-key\",  # Alternative to using environment variable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic invocation\n",
    "response = chat.invoke(\"Explain how transformer models work in 3 sentences.\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
