{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runpod\n",
    "\n",
    "[RunPod](https://www.runpod.io/) provides GPU cloud infrastructure, including Serverless endpoints optimized for deploying and scaling AI models.\n",
    "\n",
    "This guide covers how to use the `langchain-runpod` integration package to connect LangChain applications to models hosted on [RunPod Serverless](https://www.runpod.io/serverless-gpu).\n",
    "\n",
    "The integration offers interfaces for both standard Language Models (LLMs) and Chat Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intstallation\n",
    "\n",
    "Install the dedicated partner package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU langchain-runpod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### 1. Deploy an Endpoint on RunPod\n",
    "- Navigate to your [RunPod Serverless Console](https://www.runpod.io/console/serverless/user/endpoints).\n",
    "- Create a \\\"New Endpoint\\\", selecting an appropriate GPU and template (e.g., vLLM, TGI, text-generation-webui) compatible with your model and the expected input/output format (see component guides or the package [README](https://github.com/runpod/langchain-runpod)).\n",
    "- Configure settings and deploy.\n",
    "- **Crucially, copy the Endpoint ID** after deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set API Credentials\n",
    "The integration needs your RunPod API Key and the Endpoint ID. Set them as environment variables for secure access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"RUNPOD_API_KEY\"] = getpass.getpass(\"Enter your RunPod API Key: \")\n",
    "os.environ[\"RUNPOD_ENDPOINT_ID\"] = input(\"Enter your RunPod Endpoint ID: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Optional)* If using different endpoints for LLM and Chat models, you might need to set `RUNPOD_CHAT_ENDPOINT_ID` or pass the ID directly during initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "This package provides two main components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LLM\n",
    "\n",
    "For interacting with standard text completion models.\n",
    "\n",
    "See the [RunPod LLM Integration Guide](/docs/integrations/llms/runpod) for detailed usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_runpod import RunPod\n",
    "\n",
    "# Example initialization (uses environment variables)\n",
    "llm = RunPod(model_kwargs={\"max_new_tokens\": 100})  # Add generation params here\n",
    "\n",
    "# Example Invocation\n",
    "try:\n",
    "    response = llm.invoke(\"Write a short poem about the cloud.\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Error invoking LLM: {e}. Ensure endpoint ID and API key are correct and endpoint is active.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chat Model\n",
    "\n",
    "For interacting with conversational models.\n",
    "\n",
    "See the [RunPod Chat Model Integration Guide](/docs/integrations/chat/runpod) for detailed usage and feature support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_runpod import ChatRunPod\n",
    "\n",
    "# Example initialization (uses environment variables)\n",
    "chat = ChatRunPod(model_kwargs={\"temperature\": 0.8})  # Add generation params here\n",
    "\n",
    "# Example Invocation\n",
    "try:\n",
    "    response = chat.invoke(\n",
    "        [HumanMessage(content=\"Explain RunPod Serverless in one sentence.\")]\n",
    "    )\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Error invoking Chat Model: {e}. Ensure endpoint ID and API key are correct and endpoint is active.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
