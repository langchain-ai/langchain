{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d184f91",
   "metadata": {},
   "source": [
    "# MLflow\n",
    "\n",
    ">[MLflow](https://mlflow.org/) is a versatile, open-source platform for managing workflows and artifacts across the machine learning and generative AI lifecycle. It has built-in integrations with many popular AI and ML libraries, but can be used with any library, algorithm, or deployment tool. It is designed to be extensible, so you can write plugins to support new workflows, libraries, and tools.\n",
    "\n",
    "MLflow's LangChain integration provides the following capabilities:\n",
    "\n",
    "- **Tracing**: MLflow allows you to visually trace data flows through your LangChain chains, agents, retrievers, or other components, all with just one line of code (`mlflow.langchain.autolog()`).\n",
    "- **Experiment Tracking**: MLflow tracks and stores artifacts from your LangChain experiments, including models, code, prompts, metrics, and more.\n",
    "- **Dependency Management**: MLflow automatically records model dependencies, ensuring consistency between development and production environments.\n",
    "- **Model Evaluation** MLflow offers native capabilities for evaluating LangChain applications.\n",
    "\n",
    "**Note**: MLflow tracing is available in MLflow versions 2.14.0 and later.\n",
    "\n",
    "This notebook demonstrates how to track your LangChain experiments using MLflow. For more information about this feature and to explore tutorials and examples of using LangChain with MLflow, please refer to the [MLflow documentation for LangChain integration](https://mlflow.org/docs/latest/llms/langchain/index.html). If you're new to MLflow, check out the [Getting Started with MLflow](https://mlflow.org/docs/latest/getting-started/index.html) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbd74b-1542-45a4-a72b-b2eedeffd2e0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install MLflow Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42406548",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e626bb4",
   "metadata": {},
   "source": [
    "This example will use OpenAI's GPT-4o model. Feel free to skip the command below and proceed with a different LLM if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7bd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-openai -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e87b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set MLflow tracking URI if you have MLflow Tracking Server running\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84616d96",
   "metadata": {},
   "source": [
    "To begin, let's create a dedicated [MLflow experiment](https://mlflow.org/docs/latest/tracking.html#experiments) in order track our model and artifacts. While you can opt to skip this step and use the default experiment, we strongly recommend organizing your runs and artifacts into separate experiments to avoid clutter and maintain a clean, structured workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"LangChain MLflow Integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48accc76",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Integrate MLflow with your LangChain Application using one of the following methods:\n",
    "\n",
    "1. **Autologging**: Enable comprehensive tracing with the `mlflow.langchain.autolog()` command. MLflow tracing will record your LangChain chains, agents, retrievers, or other components, all with just one line of code, make it much easier to understand your chain's behavior and to identify bugs.\n",
    "2. **Manual Logging**: Use MLflow APIs to log LangChain chains and agents, enabling you to record and version your chains and agents, providing fine-grained control over what to track in your experiment.\n",
    "3. **Custom Callbacks**: Pass MLflow callbacks manually when invoking chains, allowing for semi-automated customization of your workload, such as tracking specific invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f10055",
   "metadata": {},
   "source": [
    "## Scenario 1: MLFlow Autologging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71118a27",
   "metadata": {},
   "source": [
    "To get started with MLflow tracing for LangChain, simply call `mlflow.langchain.autolog()`. After running this one line of code, MLflow will record all calls to your LangChain models as [traces](https://mlflow.org/docs/latest/llms/tracing/index.html), which you can view in the MLflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b08145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0570c18",
   "metadata": {},
   "source": [
    "### Define a Chain\n",
    "\n",
    "Now, we will define a chain, run it, and examine the MLflow UI to see the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2627ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b38bae",
   "metadata": {},
   "source": [
    "### Invoke the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = {\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"German\",\n",
    "    \"input\": \"I love programming.\",\n",
    "}\n",
    "\n",
    "chain.invoke(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173cdd4",
   "metadata": {},
   "source": [
    "Now let's inspect the traces in the MLflow UI. You can launch the UI by running `mlflow ui` in your terminal. You can find the traces by navigating to the experiment you created above (`LangChain MLflow Integration`) and clicking on the \"Traces\" tab.\n",
    "\n",
    "![MLflow Trace](/img/mlflow_1_trace.png)\n",
    "\n",
    "The displayed trace visualizes the call stack of your chain invocation, providing you with a deep insight into how each component is executed within the chain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d720a7f",
   "metadata": {},
   "source": [
    "### Tracing LangGraph\n",
    "\n",
    "LangGraph graph invocations are also traced automatically when you enable autologging with `mlflow.langchain.autolog()`. You can install LangGraph with `pip install langgraph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd52bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def count_words(text: str) -> str:\n",
    "    \"\"\"Counts the number of words in a text.\"\"\"\n",
    "    word_count = len(text.split())\n",
    "    return f\"This text contains {word_count} words.\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "tools = [count_words]\n",
    "graph = create_react_agent(llm, tools)\n",
    "\n",
    "result = graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write me a 71-word story about a cat.\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf1d7a",
   "metadata": {},
   "source": [
    "We can, again, find this trace in the MLflow UI.\n",
    "\n",
    "![LangGraph Trace](/img/mlflow_2_langgraph_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23a78c",
   "metadata": {},
   "source": [
    "### More about Autologging\n",
    "\n",
    "By default, `mlflow.langchain.autolog()` only records traces; it does not log any models. You can configure it to log models by setting the `log_models` parameter to `True`. When working with LangChain and LangGraph, however, manual model logging using [MLflow's models-from-code logging](https://mlflow.org/docs/latest/llms/langchain/index.html#logging-models-from-code) approach is recommended. We will cover this in the next section.\n",
    "\n",
    "For a comprehensive list of available configurations, please refer to the latest [MLflow LangChain Autologging Documentation](https://mlflow.org/docs/latest/llms/langchain/autologging.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf6bb02",
   "metadata": {},
   "source": [
    "## Scenario 2: Manually Logging an Agent from Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c914e3",
   "metadata": {},
   "source": [
    "### Define an Agent\n",
    "\n",
    "In this example, we'll log our tip calculator agent to MLflow. Model logging provides several key benefits:\n",
    "\n",
    "1. **Version Control**: Track different versions of your agents and chains as you experiment and improve them\n",
    "2. **Reproducibility**: Ensure your agent can be recreated exactly as it was when you logged it\n",
    "3. **Deployment Ready**: Logged models can be easily loaded and deployed in other environments\n",
    "4. **Documentation**: The logged code serves as clear documentation of how your agent works\n",
    "\n",
    "This example uses MLflow's [models-from-code](https://mlflow.org/docs/latest/llms/langchain/index.html#logging-models-from-code) approach to model logging, which stores the model definition as Python code rather than using serialization. This approach is particularly valuable for LangChain applications since many components (like lambda functions or external connections) cannot be properly serialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190a609",
   "metadata": {},
   "source": [
    "To log your LangChain model, create a separate `.py` file that defines the agent instance.\n",
    "\n",
    "In the interest of time, you can run the following cell to generate a Python file `agent.py`, which contains the agent definition code, using the `%%writefile` magic command. In actual dev scenario, you would define it in another notebook or hand-crafted python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62b20e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import mlflow\n",
    "\n",
    "@tool\n",
    "def calculate_tip(bill_amount: float, tip_percent: float) -> str:\n",
    "    \"\"\"Calculate tip amount and total bill.\n",
    "    Args:\n",
    "        bill_amount: The total amount of the bill in dollars\n",
    "        tip_percent: The tip percentage (e.g., 20 for 20%)\n",
    "    \"\"\"\n",
    "    tip_amount = bill_amount * (tip_percent / 100)\n",
    "    total = bill_amount + tip_amount\n",
    "    return f\"For a bill of ${bill_amount:.2f} with {tip_percent}% tip:\\\\nTip amount: ${tip_amount:.2f}\\\\nTotal bill: ${total:.2f}\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "tools = [calculate_tip]\n",
    "\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# IMPORTANT: call set_model() to register the instance to be logged.\n",
    "mlflow.models.set_model(agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a21f06",
   "metadata": {},
   "source": [
    "### Log the Agent\n",
    "\n",
    "Return to the original notebook and run the following cell to log the agent you've defined in the `agent.py` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the tip for a $216.32 bill with a 22% tip?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"tip-agent\") as run:\n",
    "    info = mlflow.langchain.log_model(\n",
    "        lc_model=\"agent.py\",  # Specify the relative code path to the agent definition\n",
    "        artifact_path=\"model\",\n",
    "        input_example=messages,\n",
    "    )\n",
    "\n",
    "print(\"The agent is successfully logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4687052",
   "metadata": {},
   "source": [
    "Now, open the MLflow UI and navigate to the \"Artifacts\" tab in the Run detail page. You should see that the `agent.py` file has been successfully logged, along with other model artifacts, such as dependencies, input examples, and more.\n",
    "\n",
    "![MLflow Artifacts](/img/mlflow_3_model_from_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9011db62",
   "metadata": {},
   "source": [
    "### Invoke the Logged Agent\n",
    "\n",
    "Now load the agent back and invoke it. There are two ways to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's turn on the autologging with default configuration, so we can see the trace for the agent invocation.\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Load the model back\n",
    "agent = mlflow.pyfunc.load_model(info.model_uri)\n",
    "\n",
    "# Invoke\n",
    "agent.predict(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf6133",
   "metadata": {},
   "source": [
    "As before, we can inspect the trace in the MLflow UI.\n",
    "\n",
    "![Logged Model Trace](/img/mlflow_4_trace_logged.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd10f34",
   "metadata": {},
   "source": [
    "## Scenario 3. Using MLflow Callbacks\n",
    "\n",
    "MLflow provides a callback-based approach to trace your LangChain applications. The `MlflowLangchainTracer` callback allows you to generate traces for specific chain or agent invocations, providing more granular control over what gets traced compared to autologging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013d309",
   "metadata": {},
   "source": [
    "### MlflowLangchainTracer\n",
    "\n",
    "When the chain or agent is invoked with the `MlflowLangchainTracer` callback, MLflow will automatically generate a trace for the call stack and log it to the MLflow tracking server.  The outcome is exactly same as `mlflow.langchain.autolog()`, but this is particularly useful when you want to only trace specific invocation. Autologging is applied to all invocation in the same notebook/script, on the other hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d48044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.langchain.langchain_tracer import MlflowLangchainTracer\n",
    "\n",
    "mlflow_tracer = MlflowLangchainTracer()\n",
    "\n",
    "# This call generates a trace\n",
    "chain.invoke(test_input, config={\"callbacks\": [mlflow_tracer]})\n",
    "\n",
    "# This call does not generate a trace\n",
    "chain.invoke(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6692c",
   "metadata": {},
   "source": [
    "#### Where to Pass the Callback\n",
    "LangChain supports two ways of passing callback instances: (1) Request time callbacks: pass them to the `invoke` method or bind with `with_config()` (2) Constructor callbacks: set them in the chain constructor. When using the `MlflowLangchainTracer` as a callback, you **must use request time callbacks**. Setting it in the constructor instead will only apply the callback to the top-level object, preventing it from being propagated to child components, resulting in incomplete traces. For more information on this behavior, please refer to [Callbacks Documentation](https://python.langchain.com/docs/concepts/callbacks) for more details.\n",
    "\n",
    "```python\n",
    "# Recommended: these methods will work correctly\n",
    "chain.invoke(test_input, config={\"callbacks\": [mlflow_tracer]})\n",
    "chain.with_config(callbacks=[mlflow_tracer])\n",
    "# Not recommended: this will result in incomplete traces\n",
    "chain = TheNameOfSomeChain(callbacks=[mlflow_tracer])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a60ba7",
   "metadata": {},
   "source": [
    "#### Supported Methods\n",
    "\n",
    "`MlflowLangchainTracer` supports the following invocation methods from the [Runnable Interfaces](https://python.langchain.com/v0.1/docs/expression_language/interface/).\n",
    "-  Standard interfaces: `invoke`, `stream`, `batch`\n",
    "-  Async interfaces: `astream`, `ainvoke`, `abatch`, `astream_log`, `astream_events`\n",
    "\n",
    "Other methods are not guaranteed to be fully compatible.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this guide, we have seen three main ways to track your LangChain applications using MLflow:\n",
    "\n",
    "1. **Autologging**: Enable comprehensive tracing with the `mlflow.langchain.autolog()` command. MLflow tracing will record your LangChain chains, agents, retrievers, or other components, all with just one line of code, make it much easier to understand your chain's behavior and to identify bugs.\n",
    "2. **Manual Logging**: Use MLflow APIs to log LangChain and LangGraph models, enabling you to record and version your chains and agents.\n",
    "3. **Custom Callbacks**: Use MLflow callbacks to trace specific chain or agent invocations when you want more granular control. Instead of tracing everything with autologging, you can add the MLflow tracer only to the specific calls you want to monitor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84924e35",
   "metadata": {},
   "source": [
    "## References\n",
    "To learn more about the feature and visit tutorials and examples of using LangChain with MLflow, please refer to the [MLflow documentation for LangChain integration](https://mlflow.org/docs/latest/llms/langchain/index.html).\n",
    "\n",
    "`MLflow` also provides several [tutorials](https://mlflow.org/docs/latest/llms/langchain/index.html#getting-started-with-the-mlflow-langchain-flavor-tutorials-and-guides) and [examples](https://github.com/mlflow/mlflow/tree/master/examples/langchain) for the `LangChain` integration:\n",
    "- [Quick Start](https://mlflow.org/docs/latest/llms/langchain/notebooks/langchain-quickstart.html)\n",
    "- [RAG Tutorial](https://mlflow.org/docs/latest/llms/langchain/notebooks/langchain-retriever.html)\n",
    "- [Agent Example](https://github.com/mlflow/mlflow/blob/master/examples/langchain/simple_agent.py)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
