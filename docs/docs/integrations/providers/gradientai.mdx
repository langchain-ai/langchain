# ChatGradientAI

This will help you getting started with DigitalOcean GradientAI [chat models](/docs/concepts/chat_models).

## Overview
### Integration details

| Class | Package | Package downloads | Package latest |
| :--- | :--- | :---: | :---: |
| [ChatGradientAI](https://python.langchain.com/api_reference/digitalocean/chat_models/langchain_digitalocean.chat_models.ChatDigitalocean.html) | [langchain-gradientai](https://python.langchain.com/api_reference/digitalocean/) | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-gradientai?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-gradientai?style=flat-square&label=%20) |


## Setup

To use DigitalOcean GradientAI you'll need to create an account in DigitalOcean, get DIGITALOCEAN_INFERENCE_KEY API key from GradientAI Platform, and install the `langchain-gradientai` integration package.

### Credentials

Head to [DigitalOcean GradientAI](https://www.digitalocean.com/products/gradientai) 

1. Sign up/Login to DigitalOcean Cloud Console
2. Go to the GradienAI Platform and navigate to Serverless Inference.
3. Click on Create model access key, enter a name, and create the key.

Once you've done this set the DIGITALOCEAN_INFERENCE_KEY environment variable:

```python
import os
os.environ["DIGITALOCEAN_INFERENCE_KEY"] = "your-api-key"
```

### Installation

The LangChain GradientAI integration is in the `langchain-gradientai` package:

```bash
pip install -qU langchain-gradientai
```

## Instantiation

```python
from langchain_gradientai import ChatGradientAI

llm = ChatGradientAI(
    model="llama3.3-70b-instruct",
    api_key=os.environ.get("DIGITALOCEAN_INFERENCE_KEY")
)
```

## Invocation

```python
messages = [
    (
        "system",
        "You are a creative storyteller. Continue any story prompt you receive in an engaging and imaginative way.",
    ),
    ("human", "Once upon a time, in a village at the edge of a mysterious forest, a young girl named Mira found a glowing stone..."),
]
ai_msg = llm.invoke(messages)
ai_msg
print(ai_msg.content)
```

## Chaining

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate(
    [
        (
            "system",
            "You are a knowledgeable assistant. Carefully read the provided context and answer the user's question. If the answer is present in the context, cite the relevant sentence. If not, reply with \"Not found in context.\"",
        ),
        ("human", "Context: {context}\nQuestion: {question}"),
    ]
)

chain = prompt | llm
chain.invoke(
    {
        "context": (
            "The Eiffel Tower is located in Paris and was completed in 1889. "
            "It was designed by Gustave Eiffel's engineering company. "
            "The tower is one of the most recognizable structures in the world. "
            "The Statue of Liberty was a gift from France to the United States."
        ),
        "question": "Who designed the Eiffel Tower and when was it completed?"
    }
)
```