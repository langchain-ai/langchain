# Baseten

>[Baseten](https://baseten.co) is a provider of all the infrastructure you need to deploy and serve
> ML models performantly, scalably, and cost-efficiently.

>As a model inference platform, `Baseten` is a `Provider` in the LangChain ecosystem.
The `Baseten` integration implements chat models and embeddings for comprehensive AI workflows.

>`Baseten` lets you run both open source models like Kimi-K2 or DeepSeekV3.1 through Model APIs.
All models can be accessed through a slug via OpenAI-compatible endpoints. Simply choose one of the following
[model slugs](https://docs.baseten.co/development/model-apis/overview#supported-models) when instantiating a ChatBaseten object.

>`Baseten` also offers performant embeddings through dedicated models.
Click deploy after choosing the model from the [Baseten model library](https://www.baseten.co/library/tag/embedding/),
then supply the `model_url` when instantiating a BasetenEmbeddings object.

>[Learn more](https://docs.baseten.co/deploy/lifecycle) about model IDs and deployments.

>Learn more about Baseten in [the Baseten docs](https://docs.baseten.co/).



## Installation and setup

- Install the Baseten integration package.

  ```bash
  pip install langchain-baseten
  ```

- Get a Baseten API key by signing up at [baseten.co](https://baseten.co).
- Authenticate by setting the BASETEN_API_KEY environment variable.

### Authentication

There are two ways to authenticate using your Baseten API key:

1. Setting the `BASETEN_API_KEY` environment variable.

   ```python
   import os
   os.environ["BASETEN_API_KEY"] = "<KEY>"
   ```

2. Setting `api_key` field in the Baseten modules.

   ```python
   from langchain_baseten import ChatBaseten
   chat = ChatBaseten(api_key="<KEY>")
   ```

## Chat Models

See a [usage example](/docs/integrations/chat/baseten).

```python
from langchain_baseten import ChatBaseten
```

## Embedding models

See a [usage example](/docs/integrations/text_embedding/baseten).

```python
from langchain_baseten import BasetenEmbeddings
```
