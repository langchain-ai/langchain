{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual AI\n",
    "\n",
    "Contextual AI is a platform that offers state-of-the-art Retrieval-Augmented Generation (RAG) technology for enterprise applications. Our platformant models helps innovative teams build production-ready AI applications that can process millions of pages of documents with exceptional accuracy.\n",
    "\n",
    "## Grounded Language Model (GLM)\n",
    "\n",
    "The Grounded Language Model (GLM) is specifically engineered to minimize hallucinations in RAG and agentic applications. The GLM achieves:\n",
    "\n",
    "- State-of-the-art performance on the FACTS benchmark\n",
    "- Responses strictly grounded in provided knowledge sources\n",
    "\n",
    "## Using Contextual AI with LangChain\n",
    "\n",
    "See details [here](/docs/integrations/chat/contextual).\n",
    "\n",
    "This integration allows you to easily incorporate Contextual AI's GLM into your LangChain workflows. Whether you're building applications for regulated industries or security-conscious environments, Contextual AI provides the grounded and reliable responses your use cases demand.\n",
    "\n",
    "Get started with a free trial today and experience the most grounded language model for enterprise AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8ku6X96sebl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the information available, there are two types of cats in the world:\n",
      "\n",
      "1. Good cats\n",
      "2. Best cats\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain_contextual import ChatContextual\n",
    "\n",
    "# Set credentials\n",
    "if not os.getenv(\"CONTEXTUAL_AI_API_KEY\"):\n",
    "    os.environ[\"CONTEXTUAL_AI_API_KEY\"] = getpass.getpass(\n",
    "        \"Enter your Contextual API key: \"\n",
    "    )\n",
    "\n",
    "# initialize Contextual llm\n",
    "llm = ChatContextual(\n",
    "    model=\"v1\",\n",
    "    api_key=\"\",\n",
    ")\n",
    "# include a system prompt (optional)\n",
    "system_prompt = \"You are a helpful assistant that uses all of the provided knowledge to answer the user's query to the best of your ability.\"\n",
    "\n",
    "# provide your own knowledge from your knowledge-base here in an array of string\n",
    "knowledge = [\n",
    "    \"There are 2 types of dogs in the world: good dogs and best dogs.\",\n",
    "    \"There are 2 types of cats in the world: good cats and best cats.\",\n",
    "]\n",
    "\n",
    "# create your message\n",
    "messages = [\n",
    "    (\"human\", \"What type of cats are there in the world and what are the types?\"),\n",
    "]\n",
    "\n",
    "# invoke the GLM by providing the knowledge strings, optional system prompt\n",
    "# if you want to turn off the GLM's commentary, pass True to the `avoid_commentary` argument\n",
    "ai_msg = llm.invoke(\n",
    "    messages, knowledge=knowledge, system_prompt=system_prompt, avoid_commentary=True\n",
    ")\n",
    "\n",
    "print(ai_msg.content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
