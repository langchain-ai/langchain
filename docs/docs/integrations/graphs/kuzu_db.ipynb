{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kuzu\n",
    "\n",
    "> [Kùzu](https://kuzudb.com/) is an embeddable, scalable, extremely fast graph database.\n",
    "> It is permissively licensed with an MIT license, and you can see its source code [here](https://github.com/kuzudb/kuzu).\n",
    "\n",
    "> Key characteristics of Kùzu:\n",
    ">- Performance and scalability: Implements modern, state-of-the-art join algorithms for graphs.\n",
    ">- Usability: Very easy to set up and get started with, as there are no servers (embedded architecture).\n",
    ">- Interoperability: Can conveniently scan and copy data from external columnar formats, CSV, JSON and relational databases.\n",
    ">- Structured property graph model: Implements the property graph model, with added structure.\n",
    ">- Cypher support: Allows convenient querying of the graph in Cypher, a declarative query language.\n",
    "\n",
    "> Get started with Kùzu by visiting their [documentation](https://docs.kuzudb.com/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "Kùzu is an embedded database (it runs in-process), so there are no servers to manage. Install the\n",
    "following dependencies to get started:\n",
    "\n",
    "```bash\n",
    "pip install -U langchain-kuzu langchain-openai langchain-experimental\n",
    "```\n",
    "\n",
    "This installs Kùzu along with the LangChain integration for it, as well as the OpenAI Python package\n",
    "so that we can use OpenAI's LLMs. If you want to use other LLM providers, you can install their\n",
    "respective Python packages that come with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you would first create a Kùzu database on your local machine and connect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kuzu\n",
    "\n",
    "db = kuzu.Database(\"test_db\")\n",
    "conn = kuzu.Connection(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `KuzuGraph`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kùzu's integration with LangChain makes it convenient to create and update graphs from unstructured text, and also to query graphs via a Text2Cypher pipeline that utilizes the\n",
    "power of LangChain's LLM chains. To begin, we create a `KuzuGraph` object that uses the database object we created above in combination with the `KuzuGraph` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_kuzu.graphs.kuzu_graph import KuzuGraph\n",
    "\n",
    "graph = KuzuGraph(db, allow_dangerous_requests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to transform the following text into a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tim Cook is the CEO of Apple. Apple has its headquarters in California.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of `LLMGraphTransformer` to use an LLM to extract nodes and relationships from the text.\n",
    "To make the graph more useful, we will define the following schema, such that the LLM will only\n",
    "extract nodes and relationships that match the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema\n",
    "allowed_nodes = [\"Person\", \"Company\", \"Location\"]\n",
    "allowed_relationships = [\n",
    "    (\"Person\", \"IS_CEO_OF\", \"Company\"),\n",
    "    (\"Company\", \"HAS_HEADQUARTERS_IN\", \"Location\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LLMGraphTransformer` class provides a convenient way to convert the text into a list of graph documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the LLMGraphTransformer\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=OPENAI_API_KEY),  # noqa: F821\n",
    "    allowed_nodes=allowed_nodes,\n",
    "    allowed_relationships=allowed_relationships,\n",
    ")\n",
    "\n",
    "documents = [Document(page_content=text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GraphDocument(nodes=[Node(id='Tim Cook', type='Person', properties={}), Node(id='Apple', type='Company', properties={}), Node(id='California', type='Location', properties={})], relationships=[Relationship(source=Node(id='Tim Cook', type='Person', properties={}), target=Node(id='Apple', type='Company', properties={}), type='IS_CEO_OF', properties={}), Relationship(source=Node(id='Apple', type='Company', properties={}), target=Node(id='California', type='Location', properties={}), type='HAS_HEADQUARTERS_IN', properties={})], source=Document(metadata={}, page_content='Tim Cook is the CEO of Apple. Apple has its headquarters in California.'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_documents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then call the above defined `KuzuGraph` object's `add_graph_documents` method to ingest the graph documents into the Kùzu database.\n",
    "The `include_source` argument is set to `True` so that we also create relationships between each entity node and the source document that it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the graph document to the graph\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    include_source=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `KuzuQAChain`\n",
    "\n",
    "To query the graph via a Text2Cypher pipeline, we can define a `KuzuQAChain` object. Then, we can invoke the chain with a query by connecting to the existing database that's stored in the `test_db` directory defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_kuzu.chains.graph_qa.kuzu import KuzuQAChain\n",
    "\n",
    "# Create the KuzuQAChain with verbosity enabled to see the generated Cypher queries\n",
    "chain = KuzuQAChain.from_llm(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, api_key=OPENAI_API_KEY),  # noqa: F821\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we set a temperature that's slightly higher than zero to avoid the LLM being overly concise in its response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask some questions using the QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:IS_CEO_OF]->(c:Company {id: 'Apple'}) RETURN p\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'p': {'_id': {'offset': 0, 'table': 1}, '_label': 'Person', 'id': 'Tim Cook', 'type': 'entity'}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who is the CEO of Apple?',\n",
       " 'result': 'Tim Cook is the CEO of Apple.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is the CEO of Apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (c:Company {id: 'Apple'})-[:HAS_HEADQUARTERS_IN]->(l:Location) RETURN l\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'l': {'_id': {'offset': 0, 'table': 2}, '_label': 'Location', 'id': 'California', 'type': 'entity'}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Where is Apple headquartered?',\n",
       " 'result': 'Apple is headquartered in California.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Where is Apple headquartered?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresh graph schema\n",
    "\n",
    "If you mutate or update the graph, you can inspect the refreshed schema information that's used by the Text2Cypher chain to generate Cypher statements.\n",
    "You don't need to manually call `refresh_schema()` each time as it's called automatically when you invoke the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties: [{'properties': [('id', 'STRING'), ('type', 'STRING')], 'label': 'Person'}, {'properties': [('id', 'STRING'), ('type', 'STRING')], 'label': 'Location'}, {'properties': [('id', 'STRING'), ('text', 'STRING'), ('type', 'STRING')], 'label': 'Chunk'}, {'properties': [('id', 'STRING'), ('type', 'STRING')], 'label': 'Company'}]\n",
      "Relationships properties: [{'properties': [], 'label': 'HAS_HEADQUARTERS_IN'}, {'properties': [('label', 'STRING'), ('triplet_source_id', 'STRING')], 'label': 'MENTIONS_Chunk_Person'}, {'properties': [('label', 'STRING'), ('triplet_source_id', 'STRING')], 'label': 'MENTIONS_Chunk_Location'}, {'properties': [], 'label': 'IS_CEO_OF'}, {'properties': [('label', 'STRING'), ('triplet_source_id', 'STRING')], 'label': 'MENTIONS_Chunk_Company'}]\n",
      "Relationships: ['(:Company)-[:HAS_HEADQUARTERS_IN]->(:Location)', '(:Chunk)-[:MENTIONS_Chunk_Person]->(:Person)', '(:Chunk)-[:MENTIONS_Chunk_Location]->(:Location)', '(:Person)-[:IS_CEO_OF]->(:Company)', '(:Chunk)-[:MENTIONS_Chunk_Company]->(:Company)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph.refresh_schema()\n",
    "\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use separate LLMs for Cypher and answer generation\n",
    "\n",
    "You can specify `cypher_llm` and `qa_llm` separately to use different LLMs for Cypher generation and answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = KuzuQAChain.from_llm(\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4o-mini\"),\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:IS_CEO_OF]->(c:Company {id: 'Apple'}) RETURN p.id, p.type\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'p.id': 'Tim Cook', 'p.type': 'entity'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who is the CEO of Apple?',\n",
       " 'result': 'Tim Cook is the CEO of Apple.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is the CEO of Apple?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
