{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memgraph\n",
    "\n",
    "Memgraph is an open-source graph database, tuned for dynamic analytics environments and compatible with Neo4j. To query the database, Memgraph uses Cypher - the most widely adopted, fully-specified, and open query language for property graph databases.\n",
    "\n",
    "This notebook will show you how to [query Memgraph with natural language](#natural-language-querying) and how to [construct a knowledge graph](#constructing-knowledge-graph) from your unstructured data. \n",
    "\n",
    "But first, make sure to [set everything up](#setting-up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "To go over this guide, you will need [Docker](https://www.docker.com/get-started/) and [Python 3.x](https://www.python.org/) installed.\n",
    "\n",
    "To quickly run **Memgraph Platform** (Memgraph database + MAGE library + Memgraph Lab) for the first time, do the following:\n",
    "\n",
    "On Linux/MacOS:\n",
    "```\n",
    "curl https://install.memgraph.com | sh\n",
    "```\n",
    "\n",
    "On Windows:\n",
    "```\n",
    "iwr https://windows.memgraph.com | iex\n",
    "```\n",
    "\n",
    "Both commands run a script that downloads a Docker Compose file to your system, builds and starts `memgraph-mage` and `memgraph-lab` Docker services in two separate containers. Now you have Memgraph up and running! Read more about the installation process on [Memgraph documentation](https://memgraph.com/docs/getting-started/install-memgraph).\n",
    "\n",
    "To use LangChain, install and import all the necessary packages. We'll use the package manager [pip](https://pip.pypa.io/en/stable/installation/), along with the `--user` flag, to ensure proper permissions. If you've installed Python 3.4 or a later version, `pip` is included by default. You can install all the required packages using the following command:\n",
    "\n",
    "```\n",
    "pip install langchain langchain-openai neo4j --user\n",
    "```\n",
    "\n",
    "You can either run the provided code blocks in this notebook or use a separate Python file to experiment with Memgraph and LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural language querying\n",
    "\n",
    "Memgraph's integration with LangChain includes natural language querying. To utilized it, first do all the necessary imports. We will discuss them as they appear in the code.\n",
    "\n",
    "First, instantiate `MemgraphGraph`. This object holds the connection to the running Memgraph instance. Make sure to set up all the environment variables properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.chains.graph_qa.memgraph import MemgraphQAChain\n",
    "from langchain_community.graphs import MemgraphGraph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "url = os.environ.get(\"MEMGRAPH_URI\", \"bolt://localhost:7687\")\n",
    "username = os.environ.get(\"MEMGRAPH_USERNAME\", \"\")\n",
    "password = os.environ.get(\"MEMGRAPH_PASSWORD\", \"\")\n",
    "\n",
    "graph = MemgraphGraph(\n",
    "    url=url, username=username, password=password, refresh_schema=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `refresh_schema` is initially set to `False` because there is still no data in the database and we want to avoid unnecessary database calls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating the database\n",
    "\n",
    "To populate the database, first make sure it's empty. The most efficient way to do that is to switch to the in-memory analytical storage mode, drop the graph and go back to the in-memory transactional mode. Learn more about Memgraph's [storage modes](https://memgraph.com/docs/fundamentals/storage-memory-usage#storage-modes).\n",
    "\n",
    "The data we'll add to the database is about video games of different genres available on various platforms and related to publishers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop graph\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\n",
    "graph.query(\"DROP GRAPH\")\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n",
    "\n",
    "# Creating and executing the seeding query\n",
    "query = \"\"\"\n",
    "    MERGE (g:Game {name: \"Baldur's Gate 3\"})\n",
    "    WITH g, [\"PlayStation 5\", \"Mac OS\", \"Windows\", \"Xbox Series X/S\"] AS platforms,\n",
    "            [\"Adventure\", \"Role-Playing Game\", \"Strategy\"] AS genres\n",
    "    FOREACH (platform IN platforms |\n",
    "        MERGE (p:Platform {name: platform})\n",
    "        MERGE (g)-[:AVAILABLE_ON]->(p)\n",
    "    )\n",
    "    FOREACH (genre IN genres |\n",
    "        MERGE (gn:Genre {name: genre})\n",
    "        MERGE (g)-[:HAS_GENRE]->(gn)\n",
    "    )\n",
    "    MERGE (p:Publisher {name: \"Larian Studios\"})\n",
    "    MERGE (g)-[:PUBLISHED_BY]->(p);\n",
    "\"\"\"\n",
    "\n",
    "graph.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the `graph` object holds the `query` method. That method executes query in Memgraph and it is also used by the `MemgraphQAChain` to query the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresh graph schema\n",
    "\n",
    "Since the new data is created in Memgraph, it is necessary to refresh the schema. The generated schema will be used by the `MemgraphQAChain` to instruct LLM to better generate Cypher queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To familiarize yourself with the data and verify the updated graph schema, you can print it using the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node labels and properties (name and type) are:\n",
      "- labels: (:Platform)\n",
      "  properties:\n",
      "    - name: string\n",
      "- labels: (:Genre)\n",
      "  properties:\n",
      "    - name: string\n",
      "- labels: (:Game)\n",
      "  properties:\n",
      "    - name: string\n",
      "- labels: (:Publisher)\n",
      "  properties:\n",
      "    - name: string\n",
      "\n",
      "Nodes are connected with the following relationships:\n",
      "(:Game)-[:HAS_GENRE]->(:Genre)\n",
      "(:Game)-[:PUBLISHED_BY]->(:Publisher)\n",
      "(:Game)-[:AVAILABLE_ON]->(:Platform)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the database\n",
    "\n",
    "To interact with the OpenAI API, you must configure your API key as an environment variable. This ensures proper authorization for your requests. You can find more information on obtaining your API key [here](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key). To configure the API key, you can use Python [os](https://docs.python.org/3/library/os.html) package:\n",
    "\n",
    "```\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
    "```\n",
    "\n",
    "Run the above code snippet if you're running the code within the Jupyter notebook. \n",
    "\n",
    "Next, create `MemgraphQAChain`, which will be utilized in the question-answering process based on your graph data. The `temperature parameter` is set to zero to ensure predictable and consistent answers. You can set `verbose` parameter to `True` to receive more detailed messages regarding query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MemgraphQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    model_name=\"gpt-4-turbo\",\n",
    "    allow_dangerous_requests=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start asking questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (:Game{name: \"Baldur's Gate 3\"})-[:AVAILABLE_ON]->(platform:Platform)\n",
      "RETURN platform.name\n",
      "Baldur's Gate 3 is available on PlayStation 5, Mac OS, Windows, and Xbox Series X/S.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"Which platforms is Baldur's Gate 3 available on?\")\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (:Game{name: \"Baldur's Gate 3\"})-[:AVAILABLE_ON]->(:Platform{name: \"Windows\"})\n",
      "RETURN \"Yes\"\n",
      "Yes, Baldur's Gate 3 is available on Windows.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"Is Baldur's Gate 3 available on Windows?\")\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain modifiers\n",
    "\n",
    "To modify the behavior of your chain and obtain more context or additional information, you can modify the chain's parameters.\n",
    "\n",
    "#### Return direct query results\n",
    "The `return_direct` modifier specifies whether to return the direct results of the executed Cypher query or the processed natural language response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (g:Game {name: \"Baldur's Gate 3\"})-[:PUBLISHED_BY]->(p:Publisher)\n",
      "RETURN p.name\n",
      "[{'p.name': 'Larian Studios'}]\n"
     ]
    }
   ],
   "source": [
    "# Return the result of querying the graph directly\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    return_direct=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    model_name=\"gpt-4-turbo\",\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"Which studio published Baldur's Gate 3?\")\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return query intermediate steps\n",
    "The `return_intermediate_steps` chain modifier enhances the returned response by including the intermediate steps of the query in addition to the initial query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (:Game {name: \"Baldur's Gate 3\"})-[:HAS_GENRE]->(:Genre {name: \"Adventure\"})\n",
      "RETURN \"Yes\"\n",
      "Intermediate steps: [{'query': 'MATCH (:Game {name: \"Baldur\\'s Gate 3\"})-[:HAS_GENRE]->(:Genre {name: \"Adventure\"})\\nRETURN \"Yes\"'}, {'context': [{'\"Yes\"': 'Yes'}]}]\n",
      "Final response: Yes.\n"
     ]
    }
   ],
   "source": [
    "# Return all the intermediate steps of query execution\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    allow_dangerous_requests=True,\n",
    "    return_intermediate_steps=True,\n",
    "    model_name=\"gpt-4-turbo\",\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"Is Baldur's Gate 3 an Adventure game?\")\n",
    "print(f\"Intermediate steps: {response['intermediate_steps']}\")\n",
    "print(f\"Final response: {response['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit the number of query results\n",
    "\n",
    "The `top_k` modifier can be used when you want to restrict the maximum number of query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (:Game {name: \"Baldur's Gate 3\"})-[:HAS_GENRE]->(g:Genre)\n",
      "RETURN g.name;\n",
      "Adventure, Role-Playing Game\n"
     ]
    }
   ],
   "source": [
    "# Limit the maximum number of results returned by query\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    top_k=2,\n",
    "    allow_dangerous_requests=True,\n",
    "    model_name=\"gpt-4-turbo\",\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"What genres are associated with Baldur's Gate 3?\")\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced querying\n",
    "\n",
    "As the complexity of your solution grows, you might encounter different use-cases that require careful handling. Ensuring your application's scalability is essential to maintain a smooth user flow without any hitches.\n",
    "\n",
    "Let's instantiate our chain once again and attempt to ask some questions that users might potentially ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (:Game{name: \"Baldur's Gate 3\"})-[:AVAILABLE_ON]->(:Platform{name: \"PS5\"})\n",
      "RETURN \"Yes\"\n",
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "chain = MemgraphQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    model_name=\"gpt-4-turbo\",\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"Is Baldur's Gate 3 available on PS5?\")\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated Cypher query looks fine, but we didn't receive any information in response. This illustrates a common challenge when working with LLMs - the misalignment between how users phrase queries and how data is stored. In this case, the difference between user perception and the actual data storage can cause mismatches. Prompt refinement, the process of honing the model's prompts to better grasp these distinctions, is an efficient solution that tackles this issue. Through prompt refinement, the model gains increased proficiency in generating precise and pertinent queries, leading to the successful retrieval of the desired data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt refinement\n",
    "\n",
    "To address this, we can adjust the initial Cypher prompt of the QA chain. This involves adding guidance to the LLM on how users can refer to specific platforms, such as PS5 in our case. We achieve this using the LangChain [PromptTemplate](/docs/how_to#prompt-templates), creating a modified initial prompt. This modified prompt is then supplied as an argument to our refined `MemgraphQAChain` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (:Game{name: \"Baldur's Gate 3\"})-[:AVAILABLE_ON]->(:Platform{name: \"PlayStation 5\"})\n",
      "RETURN \"Yes\"\n",
      "Yes, Baldur's Gate 3 is available on PS5.\n"
     ]
    }
   ],
   "source": [
    "MEMGRAPH_GENERATION_TEMPLATE = \"\"\"Your task is to directly translate natural language inquiry into precise and executable Cypher query for Memgraph database. \n",
    "You will utilize a provided database schema to understand the structure, nodes and relationships within the Memgraph database.\n",
    "Instructions: \n",
    "- Use provided node and relationship labels and property names from the\n",
    "schema which describes the database's structure. Upon receiving a user\n",
    "question, synthesize the schema to craft a precise Cypher query that\n",
    "directly corresponds to the user's intent. \n",
    "- Generate valid executable Cypher queries on top of Memgraph database. \n",
    "Any explanation, context, or additional information that is not a part \n",
    "of the Cypher query syntax should be omitted entirely. \n",
    "- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n",
    "- Do not include any explanations or apologies in your responses. \n",
    "- Do not include any text except the generated Cypher statement.\n",
    "- For queries that ask for information or functionalities outside the direct\n",
    "generation of Cypher queries, use the Cypher query format to communicate\n",
    "limitations or capabilities. For example: RETURN \"I am designed to generate\n",
    "Cypher queries based on the provided schema only.\"\n",
    "Schema: \n",
    "{schema}\n",
    "\n",
    "With all the above information and instructions, generate Cypher query for the\n",
    "user question. \n",
    "If the user asks about PS5, Play Station 5 or PS 5, that is the platform called PlayStation 5.\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "MEMGRAPH_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=MEMGRAPH_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    cypher_prompt=MEMGRAPH_GENERATION_PROMPT,\n",
    "    graph=graph,\n",
    "    model_name=\"gpt-4-turbo\",\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"Is Baldur's Gate 3 available on PS5?\")\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the revised initial Cypher prompt that includes guidance on platform naming, we are obtaining accurate and relevant results that align more closely with user queries. \n",
    "\n",
    "This approach allows for further improvement of your QA chain. You can effortlessly integrate extra prompt refinement data into your chain, thereby enhancing the overall user experience of your app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing knowledge graph\n",
    "\n",
    "Transforming unstructured data to structured is not an easy or straightforward task. This guide will show how LLMs can be utilized to help us there and how to construct a knowledge graph in Memgraph. After knowledge graph is created, you can use it for your GraphRAG application.\n",
    "\n",
    "The steps of constructing a knowledge graph from the text are:\n",
    "\n",
    "- [Extracting structured information from text](#extracting-structured-information-from-text): LLM is used to extract structured graph information from text in a form of nodes and relationships.\n",
    "- [Storing into Memgraph](#storing-into-memgraph): Storing the extracted structured graph information into Memgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting structured information from text\n",
    "\n",
    "Besides all the imports in the [setup section](#setting-up), import `LLMGraphTransformer` and `Document` which will be used to extract structured information from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example text about Charles Darwin ([source](https://en.wikipedia.org/wiki/Charles_Darwin)) from which knowledge graph will be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    Charles Robert Darwin was an English naturalist, geologist, and biologist,\n",
    "    widely known for his contributions to evolutionary biology. His proposition that\n",
    "    all species of life have descended from a common ancestor is now generally\n",
    "    accepted and considered a fundamental scientific concept. In a joint\n",
    "    publication with Alfred Russel Wallace, he introduced his scientific theory that\n",
    "    this branching pattern of evolution resulted from a process he called natural\n",
    "    selection, in which the struggle for existence has a similar effect to the\n",
    "    artificial selection involved in selective breeding. Darwin has been\n",
    "    described as one of the most influential figures in human history and was\n",
    "    honoured by burial in Westminster Abbey.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to initialize `LLMGraphTransformer` from the desired LLM and convert the document to the graph structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "documents = [Document(page_content=text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, LLM extracts important entities from the text and returns them as a list of nodes and relationships. Here's how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Charles Robert Darwin', type='Person', properties={}), Node(id='English', type='Nationality', properties={}), Node(id='Naturalist', type='Profession', properties={}), Node(id='Geologist', type='Profession', properties={}), Node(id='Biologist', type='Profession', properties={}), Node(id='Evolutionary Biology', type='Field', properties={}), Node(id='Common Ancestor', type='Concept', properties={}), Node(id='Scientific Concept', type='Concept', properties={}), Node(id='Alfred Russel Wallace', type='Person', properties={}), Node(id='Natural Selection', type='Concept', properties={}), Node(id='Selective Breeding', type='Concept', properties={}), Node(id='Westminster Abbey', type='Location', properties={})], relationships=[Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='English', type='Nationality', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Naturalist', type='Profession', properties={}), type='PROFESSION', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Geologist', type='Profession', properties={}), type='PROFESSION', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Biologist', type='Profession', properties={}), type='PROFESSION', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Evolutionary Biology', type='Field', properties={}), type='CONTRIBUTION', properties={}), Relationship(source=Node(id='Common Ancestor', type='Concept', properties={}), target=Node(id='Scientific Concept', type='Concept', properties={}), type='BASIS', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Alfred Russel Wallace', type='Person', properties={}), type='COLLABORATION', properties={}), Relationship(source=Node(id='Natural Selection', type='Concept', properties={}), target=Node(id='Selective Breeding', type='Concept', properties={}), type='COMPARISON', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Westminster Abbey', type='Location', properties={}), type='BURIAL', properties={})], source=Document(metadata={}, page_content='\\n    Charles Robert Darwin was an English naturalist, geologist, and biologist,\\n    widely known for his contributions to evolutionary biology. His proposition that\\n    all species of life have descended from a common ancestor is now generally\\n    accepted and considered a fundamental scientific concept. In a joint\\n    publication with Alfred Russel Wallace, he introduced his scientific theory that\\n    this branching pattern of evolution resulted from a process he called natural\\n    selection, in which the struggle for existence has a similar effect to the\\n    artificial selection involved in selective breeding. Darwin has been\\n    described as one of the most influential figures in human history and was\\n    honoured by burial in Westminster Abbey.\\n'))]\n"
     ]
    }
   ],
   "source": [
    "print(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing into Memgraph\n",
    "\n",
    "Once you have the data ready in a format of `GraphDocument`, that is, nodes and relationships, you can use `add_graph_documents` method to import it into Memgraph. That method transforms the list of `graph_documents` into appropriate Cypher queries that need to be executed in Memgraph. Once that's done, a knowledge graph is stored in Memgraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the database\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\n",
    "graph.query(\"DROP GRAPH\")\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n",
    "\n",
    "# Create KG\n",
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the graph looks like in Memgraph Lab (check on `localhost:3000`):\n",
    "\n",
    "![memgraph-kg](../../../static/img/memgraph_kg.png)\n",
    "\n",
    "In case you tried this out and got a different graph, that is expected behavior. The graph construction process is non-deterministic, since LLM which is used to generate nodes and relationships from unstructured data in non-deterministic.\n",
    "\n",
    "### Additional options\n",
    "\n",
    "Additionally, you have the flexibility to define specific types of nodes and relationships for extraction according to your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[Node(id='Charles Robert Darwin', type='Person', properties={}), Node(id='English', type='Nationality', properties={}), Node(id='Evolutionary Biology', type='Concept', properties={}), Node(id='Natural Selection', type='Concept', properties={}), Node(id='Alfred Russel Wallace', type='Person', properties={})]\n",
      "Relationships:[Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='English', type='Nationality', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Evolutionary Biology', type='Concept', properties={}), type='INVOLVED_IN', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Natural Selection', type='Concept', properties={}), type='INVOLVED_IN', properties={}), Relationship(source=Node(id='Charles Robert Darwin', type='Person', properties={}), target=Node(id='Alfred Russel Wallace', type='Person', properties={}), type='COLLABORATES_WITH', properties={})]\n"
     ]
    }
   ],
   "source": [
    "llm_transformer_filtered = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    allowed_nodes=[\"Person\", \"Nationality\", \"Concept\"],\n",
    "    allowed_relationships=[\"NATIONALITY\", \"INVOLVED_IN\", \"COLLABORATES_WITH\"],\n",
    ")\n",
    "graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(\n",
    "    documents\n",
    ")\n",
    "\n",
    "print(f\"Nodes:{graph_documents_filtered[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents_filtered[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the graph would like in such case:\n",
    "\n",
    "![memgraph-kg-2](../../../static/img/memgraph_kg_2.png)\n",
    "\n",
    "Your graph can also have `__Entity__` labels on all nodes which will be indexed for faster retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop graph\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\n",
    "graph.query(\"DROP GRAPH\")\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n",
    "\n",
    "# Store to Memgraph with Entity label\n",
    "graph.add_graph_documents(graph_documents, baseEntityLabel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the graph would look like:\n",
    "\n",
    "![memgraph-kg-3](../../../static/img/memgraph_kg_3.png)\n",
    "\n",
    "There is also an option to include the source of the information that's obtained in the graph. To do that, set `include_source` to `True` and then the source document is stored and it is linked to the nodes in the graph using the `MENTIONS` relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop graph\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\n",
    "graph.query(\"DROP GRAPH\")\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n",
    "\n",
    "# Store to Memgraph with source included\n",
    "graph.add_graph_documents(graph_documents, include_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructed graph would look like this:\n",
    "\n",
    "![memgraph-kg-4](../../../static/img/memgraph_kg_4.png)\n",
    "\n",
    "Notice how the content of the source is stored and `id` property is generated since the document didn't have any `id`.\n",
    "You can combine having both `__Entity__` label and document source. Still, be aware that both take up memory, especially source included due to long strings for content.\n",
    "\n",
    "In the end, you can query the knowledge graph, as explained in the section before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (:Person {id: \"Charles Robert Darwin\"})-[:COLLABORATION]->(collaborator)\n",
      "RETURN collaborator;\n",
      "Alfred Russel Wallace\n"
     ]
    }
   ],
   "source": [
    "chain = MemgraphQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    model_name=\"gpt-4-turbo\",\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "print(chain.invoke(\"Who Charles Robert Darwin collaborated with?\")[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
