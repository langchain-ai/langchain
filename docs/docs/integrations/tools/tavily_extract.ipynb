{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f91f20",
   "metadata": {},
   "source": [
    "# Tavily Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24a889",
   "metadata": {},
   "source": [
    "[Tavily](https://tavily.com) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed. Tavily offers an [Extract](https://docs.tavily.com/api-reference/endpoint/extract) endpoint that can be used to extract content from a URLs.\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Integration details\n",
    "| Class | Package | Serializable | [JS support](https://js.langchain.com/docs/integrations/tools/tavily_search) |  Package latest |\n",
    "| :--- | :--- | :---: | :---: | :---: |\n",
    "| TavilyExtract | [langchain-community](https://python.langchain.com/api_reference/community/index.html) | ❌ | ❌ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-community?style=flat-square&label=%20) |\n",
    "\n",
    "### Tool features\n",
    "| [Returns artifact](/docs/how_to/tool_artifacts/) | Native async | Return data | Pricing |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| ❌ | ✅ | content | 1,000 free searches / month | \n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "The integration lives in the `langchain-community` package. We also need to install the `tavily-python` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85b4089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%pip install -qU \"langchain-community>=0.3.18\" tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e9266",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "We also need to set our Tavily API key. You can get an API key by visiting [this site](https://app.tavily.com/sign-in) and creating an account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b178a2-8816-40ca-b57c-ccdd86dde9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ab717-fd27-4c59-b912-bdd099541478",
   "metadata": {},
   "source": [
    "It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c2f136-6367-4f1f-825d-ae741e1bf281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97218f-f366-479d-8bf7-fe9f2f6df73f",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Here we show how to instantiate an instance of the Tavily extract tool. The tool accepts various parameters, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3ddfe9-ca79-494c-a7ab-1f56d9407a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilyExtract\n",
    "\n",
    "tool = TavilyExtract(\n",
    "    extract_depth=\"basic\",\n",
    "    include_images=True,\n",
    "    # name=\"...\",            # overwrite default tool name\n",
    "    # description=\"...\",     # overwrite default tool description\n",
    "    # args_schema=...,       # overwrite default args_schema: BaseModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74147a1a",
   "metadata": {},
   "source": [
    "## Invocation\n",
    "\n",
    "### [Invoke directly with args](/docs/concepts/tools)\n",
    "\n",
    "The `TavilyExtract` tool accepts the following arguments:\n",
    "\n",
    "- `urls` (required): A list of URLs to extract content from.\n",
    "- `extract_depth` (optional): The depth of the extraction.\n",
    "- `include_images` (optional): Whether to include images in the extraction.\n",
    "\n",
    "NOTE: The optional arguments are available for ReAct agents to dynamically set, if you set a argument during instantiation and then Invoke the tool with a different value, the tool will use the value you passed during invokation. To read more about ReAct agents, check out the [ReAct agent](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/react/) documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65310a8b-eb0c-4d9e-a618-4f4abe2414fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'url': 'https://en.wikipedia.org/wiki/Lionel_Messi',\n",
       "   'raw_content': 'Lionel Messi\\n\\n\\n\\nLionel Andrés \"Leo\" Messi[note 1] (Spanish pronunciation: [ljoˈnel anˈdɾes ˈmesi] ⓘ; born 24 June 1987) is an Argentine professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team. Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughou',\n",
       "   'images': []}],\n",
       " 'failed_results': [],\n",
       " 'response_time': 0.79}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_msg = tool.invoke({\"urls\": [\"https://en.wikipedia.org/wiki/Lionel_Messi\"]})\n",
    "\n",
    "# abbreviate for demo\n",
    "tool_msg[\"results\"][0][\"raw_content\"] = tool_msg[\"results\"][0][\"raw_content\"][:400]\n",
    "tool_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e73897",
   "metadata": {},
   "source": [
    "### [Invoke with ToolCall](/docs/concepts/tools)\n",
    "\n",
    "We can also invoke the tool with a model-generated ToolCall, in which case a ToolMessage will be returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90e33a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [{\"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\", \"raw_content\": \"Contents\\n\\nLionel Messi\\n\\n\\n\\nMessi withArgentinaat the2022 FIFA World Cup\\nPersonal information\\nFull name | Lionel Andrés Messi[1]\\nDate of birth | (1987-06-24)24 June 1987(age 37)[1]\\nPlace of birth | Rosario, Argentina\\nHeight | 1.70 m (5 ft 7 in)[1]\\nPosition(s) | Forward\\nTeam information\\nCurrent team | Int\n"
     ]
    }
   ],
   "source": [
    "# This is usually generated by a model, but we'll create a tool call directly for demo purposes.\n",
    "model_generated_tool_call = {\n",
    "    \"args\": {\"urls\": [\"https://en.wikipedia.org/wiki/Lionel_Messi\"]},\n",
    "    \"id\": \"1\",\n",
    "    \"name\": \"tavily\",\n",
    "    \"type\": \"tool_call\",\n",
    "}\n",
    "tool_msg = tool.invoke(model_generated_tool_call)\n",
    "\n",
    "# The content is a JSON string of results\n",
    "print(tool_msg.content[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab176a3",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "We can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n",
    "\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22930279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "# !pip install -qU langchain langchain-openai\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(model=\"gpt-4o\", model_provider=\"openai\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27ea9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='It seems there was an issue extracting information from the Wikipedia page for Lionel Messi. You can try visiting the page directly [here](https://en.wikipedia.org/wiki/Lionel_Messi) for detailed information. If you have specific questions about Lionel Messi, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 339, 'total_tokens': 397, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'stop', 'logprobs': None}, id='run-30f558bb-83af-4b34-8540-9096c1cc8b58-0', usage_metadata={'input_tokens': 339, 'output_tokens': 58, 'total_tokens': 397, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig, chain\n",
    "\n",
    "today = datetime.datetime.today().strftime(\"%D\")\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", f\"You are a helpful assistant. The date today is {today}.\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# specifying tool_choice will force the model to call this tool.\n",
    "llm_with_tools = llm.bind_tools([tool])\n",
    "\n",
    "llm_chain = prompt | llm_with_tools\n",
    "\n",
    "\n",
    "@chain\n",
    "def tool_chain(user_input: str, config: RunnableConfig):\n",
    "    input_ = {\"user_input\": user_input}\n",
    "    ai_msg = llm_chain.invoke(input_, config=config)\n",
    "    tool_msgs = tool.batch(ai_msg.tool_calls, config=config)\n",
    "    return llm_chain.invoke({**input_, \"messages\": [ai_msg, *tool_msgs]}, config=config)\n",
    "\n",
    "\n",
    "tool_chain.invoke(\n",
    "    \"extract information from the following url: https://en.wikipedia.org/wiki/Lionel_Messi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71e755",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all TavilySearchResults features and configurations head to the API reference:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
