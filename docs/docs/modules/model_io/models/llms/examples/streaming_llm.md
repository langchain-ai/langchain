# Streaming

LangChain provides streaming support for LLMs. Currently, we support streaming for the `OpenAI`, `ChatOpenAI`, and `ChatAnthropic` implementations, but streaming support for other LLM implementations is on the roadmap. To utilize streaming, use a [`CallbackHandler`](https://github.com/hwchase17/langchain/blob/master/langchain/callbacks/base.py) that implements `on_llm_new_token`. In this example, we are using `StreamingStdOutCallbackHandler`.

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->


```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.schema import HumanMessage
```


```python
llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
resp = llm("Write me a song about sparkling water.")
```

<CodeOutputBlock lang="python">

```
    
    
    Verse 1
    I'm sippin' on sparkling water,
    It's so refreshing and light,
    It's the perfect way to quench my thirst
    On a hot summer night.
    
    Chorus
    Sparkling water, sparkling water,
    It's the best way to stay hydrated,
    It's so crisp and so clean,
    It's the perfect way to stay refreshed.
    
    Verse 2
    I'm sippin' on sparkling water,
    It's so bubbly and bright,
    It's the perfect way to cool me down
    On a hot summer night.
    
    Chorus
    Sparkling water, sparkling water,
    It's the best way to stay hydrated,
    It's so crisp and so clean,
    It's the perfect way to stay refreshed.
    
    Verse 3
    I'm sippin' on sparkling water,
    It's so light and so clear,
    It's the perfect way to keep me cool
    On a hot summer night.
    
    Chorus
    Sparkling water, sparkling water,
    It's the best way to stay hydrated,
    It's so crisp and so clean,
    It's the perfect way to stay refreshed.
```

</CodeOutputBlock>

We still have access to the end `LLMResult` if using `generate`. However, `token_usage` is not currently supported for streaming.


```python
llm.generate(["Tell me a joke."])
```

<CodeOutputBlock lang="python">

```
    
    
    Q: What did the fish say when it hit the wall?
    A: Dam!




    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when it hit the wall?\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {}, 'model_name': 'text-davinci-003'})
```

</CodeOutputBlock>

Here's an example with the `ChatOpenAI` chat model implementation:


```python
chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
resp = chat([HumanMessage(content="Write me a song about sparkling water.")])
```

<CodeOutputBlock lang="python">

```
    Verse 1:
    Bubbles rising to the top
    A refreshing drink that never stops
    Clear and crisp, it's oh so pure
    Sparkling water, I can't ignore
    
    Chorus:
    Sparkling water, oh how you shine
    A taste so clean, it's simply divine
    You quench my thirst, you make me feel alive
    Sparkling water, you're my favorite vibe
    
    Verse 2:
    No sugar, no calories, just H2O
    A drink that's good for me, don't you know
    With lemon or lime, you're even better
    Sparkling water, you're my forever
    
    Chorus:
    Sparkling water, oh how you shine
    A taste so clean, it's simply divine
    You quench my thirst, you make me feel alive
    Sparkling water, you're my favorite vibe
    
    Bridge:
    You're my go-to drink, day or night
    You make me feel so light
    I'll never give you up, you're my true love
    Sparkling water, you're sent from above
    
    Chorus:
    Sparkling water, oh how you shine
    A taste so clean, it's simply divine
    You quench my thirst, you make me feel alive
    Sparkling water, you're my favorite vibe
    
    Outro:
    Sparkling water, you're the one for me
    I'll never let you go, can't you see
    You're my drink of choice, forevermore
    Sparkling water, I adore.
```

</CodeOutputBlock>

Here is an example with the `ChatAnthropic` chat model implementation, which uses their `claude` model.


```python
chat = ChatAnthropic(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
resp = chat([HumanMessage(content="Write me a song about sparkling water.")])
```

<CodeOutputBlock lang="python">

```
     Here is my attempt at a song about sparkling water:
    
    Sparkling water, bubbles so bright, 
    Dancing in the glass with delight.
    Refreshing and crisp, a fizzy delight,
    Quenching my thirst with each sip I take.
    The carbonation tickles my tongue,
    As the refreshing water song is sung.
    Lime or lemon, a citrus twist,
    Makes sparkling water such a bliss.
    Healthy and hydrating, a drink so pure,
    Sparkling water, always alluring.
    Bubbles ascending in a stream, 
    Sparkling water, you're my dream!
```

</CodeOutputBlock>
