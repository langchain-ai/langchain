---
sidebar_position: 2
---

# LangChain v1.0 message types

*Last updated: 08.08.25*

LangChain v0.4 allows developers to opt-in to new message types that will become default
in LangChain v1.0. LangChain v1.0 will be released this fall.

These messages should be considered a beta feature and are subject to change in
LangChain v1.0, although we do not anticipate any significant changes.

## Benefits

The new message types offer improvements in performance, type-safety, and consistency
across OpenAI, Anthropic, Gemini, and other providers.

### Performance

Importantly, the new messages are Python dataclasses, saving some runtime from
instantiating (layers of) Pydantic BaseModels.

LangChain v0.4 introduces a new `BaseChatModel` class in `langchain_core.v1.chat_models`
that is faster and leaner than the existing `BaseChatModel` class, offering significant
reductions in overhead above provider SDKs.

### Type-safety

Message content is typed as
```python
import langchain_core.messages.content_blocks as types

content: list[types.ContentBlock]
```

where we have introduced standard types for text, reasoning, citations, server-side
tool executions (e.g., web search and code interpreters). These include
[tool calls](https://python.langchain.com/docs/concepts/tool_calling/) and the
[multi-modal types](/docs/how_to/multimodal_inputs/) introduced in earlier versions
of LangChain. There are no breaking changes associated with the existing content types.

**This is the most significant change from the existing message classes**, which permit
strings, lists of strings, or lists of untyped dicts as content. We have added a
`.text` getter so that developers can easily recover string content. Consequently, we
have deprecated `.text()` (as a method) in favor of the new property.

`.tool_calls`, instead of an attribute, is now also a getter with an associated setter,
so that usage is largely the same. See [usage comparison](#usage-comparison), below,
for details.

### Consistency

Many chat models can generate a variety of content in a single conversational turn,
including reasoning, tool calls and responses, images, text with citations, and other
structured objects. We have standardized these types, resulting in improved
inter-operability of messages across models.

## Usage comparison

| Task                    | Previous                               | New                                                              |
|-------------------------|----------------------------------------|------------------------------------------------------------------|
| Get text content (str)  | `message.content` or `message.text()`  | `message.text`                                                   |
| Get content blocks      | `message.content`                      | `message.content`                                                |
| Get `additional_kwargs` | `message.additional_kwargs`            | `[block for block in message.content if block["type"] == "..."]` |

Getting `response_metadata` and `tool_calls` has not changed.

### Changes in content blocks

For providers that generate `list[dict]` content, the dict elements have changed to
conform to the new content block types. Refer to the
[API reference](https://python.langchain.com/api_reference/core/messages.html) for
details. Below we show some examples.

Importantly:
- Where provider-specific fields map to fields on standard types, LangChain manages
the translation.
- Where provider-specific fields do not map to fields on standard types, LangChain
stores them in an `"extras"` key (see below for examples).

<details>
  <summary>Citations and web search</summary>

<div className="row">
  <div className="col col--6" style={{minWidth: 0}}>
    **Old content**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("openai:gpt-5-mini", output_version="responses/v1")
llm_with_tools = llm.bind_tools([{"type": "web_search_preview"}])

response = llm_with_tools.invoke("What was a positive news story from today?")
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123",
    "summary": []
  },
  {
    "type": "web_search_call",
    "id": "ws_abc123",
    "action": {
      "query": "positive news today August 8 2025 'good news' 'Aug 8 2025' 'today' ",
      "type": "search"
    },
    "status": "completed"
  },
  {
    "type": "text",
    "text": "Here are two positive news items from today...",
    "annotations": [
      {
        "type": "url_citation",
        "end_index": 455,
        "start_index": 196,
        "title": "Document title",
        "url": "<document url>"
      },
      {
        "type": "url_citation",
        "end_index": 1022,
        "start_index": 707,
        "title": "Another Document",
        "url": "<another document url>"
      },
    ],
    "id": "msg_abc123"
  }
]
```
</div>

  <div className="col col--6" style={{minWidth: 0}}>
    **New content**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("openai:gpt-5-mini", message_version="v1")
llm_with_tools = llm.bind_tools([{"type": "web_search_preview"}])

response = llm_with_tools.invoke("What was a positive news story from today?")
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123"
  },
  {
    "type": "web_search_call",
    "id": "ws_abc123",
    "query": "positive news August 8 2025 'good news' 'today' ",
    "extras": {
      "action": {"type": "search"},
      "status": "completed",
    }
  },
  {
    "type": "web_search_result",
    "id": "ws_abc123"
  },
  {
    "type": "text",
    "text": "Here are two positive news items from today...",
    "annotations": [
      {
        "type": "citation",
        "end_index": 455,
        "start_index": 196,
        "title": "Document title",
        "url": "<document url>"
      },
      {
        "type": "citation",
        "end_index": 1022,
        "start_index": 707,
        "title": "Another Document",
        "url": "<another document url>"
      }
    ],
    "id": "msg_abc123"
  }
]
```
  </div>
</div>
</details>


<details>
  <summary>Reasoning</summary>

<div className="row">
  <div className="col col--6" style={{minWidth: 0}}>
    **Old content**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "openai:gpt-5",
    reasoning={"effort": "medium", "summary": "auto"},
    output_version="responses/v1",
)
response = llm.invoke(
    "What was the third tallest building in the world in the year 2000?"
)
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123",
    "summary": [
      {
        "text": "The user is asking about...",
        "type": "summary_text"
      },
      {
        "text": "We should consider...",
        "type": "summary_text"
      }
    ]
  },
  {
    "type": "text",
    "text": "In the year 2000 the third-tallest building in the world was...",
    "id": "msg_abc123"
  }
]
```
  </div>

  <div className="col col--6" style={{minWidth: 0}}>
    **New content**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "openai:gpt-5",
    reasoning={"effort": "medium", "summary": "auto"},
    message_version="v1",
)
response = llm.invoke(
    "What was the third tallest building in the world in the year 2000?"
)
response.content
```
```
[
  {
    "type": "reasoning",
    "reasoning": "The user is asking about...",
    "id": "rs_abc123"
  },
  {
    "type": "reasoning",
    "reasoning": "We should consider...",
    "id": "rs_abc123"
  },
  {
    "type": "text",
    "text": "In the year 2000 the third-tallest building in the world was...",
    "id": "msg_abc123"
  }
]
```
  </div>
</div>
</details>


<details>
  <summary>Non-standard blocks</summary>

Where content blocks from specific providers do not map to a standard type, they are
structured into a `"non_standard"` block:
```python
{
    "type": "non_standard",
    "value": original_block,
}
```
<div className="row">
  <div className="col col--6" style={{minWidth: 0}}>
    **Old content**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("openai:gpt-5-mini", output_version="responses/v1")
llm_with_tools = llm.bind_tools(
    [
        {
            "type": "file_search",
            "vector_store_ids": ["vs_67d0baa0544c8191be194a85e19cbf92"],
        }
    ]
)

response = llm_with_tools.invoke("What is deep research by OpenAI?")
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123",
    "summary": []
  },
  {
    "type": "file_search_call",
    "id": "fs_abc123",
    "queries": [
      "What is deep research by OpenAI?",
      "deep research OpenAI definition"
    ],
    "status": "completed"
  },
  {
    "type": "reasoning",
    "id": "rs_def456",
    "summary": []
  },
  {
    "type": "text",
    "text": "Deep research is...",
    "annotations": [
      {
        "type": "file_citation",
        "file_id": "file-abc123",
        "filename": "sample_file.pdf",
        "index": 305
      },
      {
        "type": "file_citation",
        "file_id": "file-abc123",
        "filename": "sample_file.pdf",
        "index": 675
      },
    ],
    "id": "msg_abc123"
  }
]
```
</div>

  <div className="col col--6" style={{minWidth: 0}}>
    **New content**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("openai:gpt-5-mini", message_version="v1")
llm_with_tools = llm.bind_tools(
    [
        {
            "type": "file_search",
            "vector_store_ids": ["vs_67d0baa0544c8191be194a85e19cbf92"],
        }
    ]
)

response = llm_with_tools.invoke("What is deep research by OpenAI?")
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123",
    "summary": []
  },
  {
    "type": "non_standard",
    "value": {
      "type": "file_search_call",
      "id": "fs_abc123",
      "queries": [
        "What is deep research by OpenAI?",
        "deep research OpenAI definition"
      ],
      "status": "completed"
    }
  },
  {
    "type": "reasoning",
    "id": "rs_def456",
    "summary": []
  },
  {
    "type": "text",
    "text": "Deep research is...",
    "annotations": [
      {
        "type": "citation",
        "title": "sample_file.pdf",
        "extras": {
          "file_id": "file-abc123",
          "index": 305
        }
      },
      {
        "type": "citation",
        "title": "sample_file.pdf",
        "extras": {
          "file_id": "file-abc123",
          "index": 675
        }
      },
    ],
    "id": "msg_abc123"
  }
]
```
  </div>
</div>
</details>


## Feature gaps

The new message types do not yet support LangChain's caching layer. Support will be
added in the coming weeks.
