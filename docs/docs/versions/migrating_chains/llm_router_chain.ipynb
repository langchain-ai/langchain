{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14625d35-efca-41cf-b203-be9f4c375700",
   "metadata": {},
   "source": [
    "# Migrating from LLMRouterChain\n",
    "\n",
    "The [`LLMRouterChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.router.llm_router.LLMRouterChain.html) routed an input query to one of multiple destinations-- that is, given an input query, it used a LLM to select from a list of destination chains, and passed its inputs to the selected chain.\n",
    "\n",
    "`LLMRouterChain` does not support common [chat model](/docs/concepts/chat_models) features, such as message roles and [tool calling](/docs/concepts/tool_calling). Under the hood, `LLMRouterChain` routes a query by instructing the LLM to generate JSON-formatted text, and parsing out the intended destination.\n",
    "\n",
    "Consider an example from a [MultiPromptChain](/docs/versions/migrating_chains/multi_prompt_chain), which uses `LLMRouterChain`. Below is an (example) default prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364814a5-d15c-41bb-bf3f-581df51a4721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "'''json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "'''\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "\n",
      "animals: prompt for animal expert\n",
      "vegetables: prompt for a vegetable expert\n",
      "\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include '''json at the start of the response) >>\n",
      "<< OUTPUT (must end with ''') >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router.multi_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "destinations = \"\"\"\n",
    "animals: prompt for animal expert\n",
    "vegetables: prompt for a vegetable expert\n",
    "\"\"\"\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)\n",
    "\n",
    "print(router_template.replace(\"`\", \"'\"))  # for rendering purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934937d1-fc0a-4d3f-b297-29f96e6a8f5e",
   "metadata": {},
   "source": [
    "Most of the behavior is determined via a single natural language prompt. Chat models that support [tool calling](/docs/how_to/tool_calling/) features confer a number of advantages for this task:\n",
    "\n",
    "- Supports chat prompt templates, including messages with `system` and other roles;\n",
    "- Tool-calling models are fine-tuned to generate structured output;\n",
    "- Support for runnable methods like streaming and async operations.\n",
    "\n",
    "Now let's look at `LLMRouterChain` side-by-side with an LCEL implementation that uses tool-calling. Note that for this guide we will `langchain-openai >= 0.1.20`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12b22b-5452-4776-aee3-b67d9f965082",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0edbba1-a497-49ef-ade7-4fe7967360eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4dc41c-3fdc-4093-ba5e-31a9ebb54e13",
   "metadata": {},
   "source": [
    "## Legacy\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58c9269-5a1d-4234-88b5-7168944618bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    # Note: here we use the prompt template from above. Generally this would need\n",
    "    # to be customized.\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22ebdca-5f53-459e-9cff-a97b2354ffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegetables\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"What color are carrots?\"})\n",
    "\n",
    "print(result[\"destination\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd48120-056f-4c58-a04f-da5198c23068",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## LCEL\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bbebac2-df19-4f59-8a69-f61cd7286e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "route_system = \"Route the user's query to either the animal or vegetable expert.\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", route_system),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define schema for output:\n",
    "class RouteQuery(TypedDict):\n",
    "    \"\"\"Route query to destination expert.\"\"\"\n",
    "\n",
    "    destination: Literal[\"animal\", \"vegetable\"]\n",
    "\n",
    "\n",
    "# Instead of writing formatting instructions into the prompt, we\n",
    "# leverage .with_structured_output to coerce the output into a simple\n",
    "# schema.\n",
    "chain = route_prompt | llm.with_structured_output(RouteQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88012e10-8def-44fa-833f-989935824182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegetable\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"What color are carrots?\"})\n",
    "\n",
    "print(result[\"destination\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7ba9e-65b4-48af-8a39-453c01a7b7cb",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Next steps\n",
    "\n",
    "See [this tutorial](/docs/tutorials/llm_chain) for more detail on building with prompt templates, LLMs, and output parsers.\n",
    "\n",
    "Check out the [LCEL conceptual docs](/docs/concepts/lcel) for more background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e4bab-3b8a-4e89-89e2-200a8d8eb8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
