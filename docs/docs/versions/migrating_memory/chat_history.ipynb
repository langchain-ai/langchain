{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c298a5c9-b9af-481d-9eba-cbd65f987a8a",
   "metadata": {},
   "source": [
    "# How to use BaseChatMessageHistory with LangGraph\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "* [Chat History](/docs/concepts/chat_history)\n",
    "* [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)\n",
    "* [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/)\n",
    "* [Memory](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#memory)\n",
    ":::\n",
    "\n",
    "We recommend that new LangChain applications take advantage of the [built-in LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to implement memory.\n",
    "\n",
    "In some situations, users may need to keep using an existing persistence solution for chat message history.\n",
    "\n",
    "Here, we will show how to use [LangChain chat message histories](https://python.langchain.com/docs/integrations/memory/) (implementations of [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html)) with LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bc988-167b-43f1-860a-d247e28b2b42",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbfd2ab-7537-4269-8249-646fa89bf016",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet langchain-anthropic langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694febf-dfa8-46ef-babc-f8b16b5a2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e08659-b68c-48f2-8b33-e79b0c6999e1",
   "metadata": {},
   "source": [
    "## ChatMessageHistory\n",
    "\n",
    "A message history needs to be parameterized by a conversation ID or maybe by the 2-tuple of (user ID, conversation ID).\n",
    "\n",
    "Many of the [LangChain chat message histories](https://python.langchain.com/docs/integrations/memory/) will have either a `session_id` or some `namespace` to allow keeping track of different conversations. Please refer to the specific implementations to check how it is parameterized.\n",
    "\n",
    "The built-in `InMemoryChatMessageHistory` does not contains such a parameterization, so we'll create a dictionary to keep track of the message histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28049308-2543-48e6-90d0-37a88951a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chats_by_session_id = {}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    chat_history = chats_by_session_id.get(session_id)\n",
    "    if chat_history is None:\n",
    "        chat_history = InMemoryChatMessageHistory()\n",
    "        chats_by_session_id[session_id] = chat_history\n",
    "    return chat_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94c53ce3-4212-41e6-8ad3-f0ab5df6130f",
   "metadata": {},
   "source": [
    "## Use with LangGraph\n",
    "\n",
    "Next, we'll set up a basic chat bot using LangGraph. If you're not familiar with LangGraph, you should look at the following [Quick Start Tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/).\n",
    "\n",
    "We'll create a [LangGraph node](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) for the chat model, and manually manage the conversation history, taking into account the conversation ID passed as part of the RunnableConfig.\n",
    "\n",
    "The conversation ID can be passed as either part of the RunnableConfig (as we'll do here), or as part of the [graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6633dd2-2d6a-4121-b087-4907c9f588ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! It's nice to meet you. I'm Claude, an AI assistant created by Anthropic. How are you doing today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what was my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You introduced yourself as Bob when you said \"hi! I'm bob\".\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "builder = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define a chat model\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:\n",
    "    # Make sure that config is populated with the session id\n",
    "    if \"configurable\" not in config or \"session_id\" not in config[\"configurable\"]:\n",
    "        raise ValueError(\n",
    "            \"Make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}\"\n",
    "        )\n",
    "    # Fetch the history of messages and append to it any new messages.\n",
    "    # highlight-start\n",
    "    chat_history = get_chat_history(config[\"configurable\"][\"session_id\"])\n",
    "    messages = list(chat_history.messages) + state[\"messages\"]\n",
    "    # highlight-end\n",
    "    ai_message = model.invoke(messages)\n",
    "    # Finally, update the chat message history to include\n",
    "    # the new input message from the user together with the\n",
    "    # response from the model.\n",
    "    # highlight-next-line\n",
    "    chat_history.add_messages(state[\"messages\"] + [ai_message])\n",
    "    return {\"messages\": ai_message}\n",
    "\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "builder.add_edge(START, \"model\")\n",
    "builder.add_node(\"model\", call_model)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# Here, we'll create a unique session ID to identify the conversation\n",
    "session_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"session_id\": session_id}}\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Here, let's confirm that the AI remembers our name!\n",
    "input_message = HumanMessage(content=\"what was my name?\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0766af-a3b3-4293-b253-3a10f365ab5d",
   "metadata": {},
   "source": [
    ":::tip\n",
    "\n",
    "This also supports streaming LLM content token by token if using langgraph >= 0.2.28.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "044b63dd-fb15-4a03-89c5-aaaf7346ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You| sai|d your| name was Bob.|"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "first = True\n",
    "\n",
    "for msg, metadata in graph.stream(\n",
    "    {\"messages\": input_message}, config, stream_mode=\"messages\"\n",
    "):\n",
    "    if msg.content and not isinstance(msg, HumanMessage):\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da0536dd-9a0b-49e3-b0b6-e8c7abf3b1f9",
   "metadata": {},
   "source": [
    "## Using With RunnableWithMessageHistory\n",
    "\n",
    "This how-to guide used the `messages` and `add_messages` interface of `BaseChatMessageHistory` directly. \n",
    "\n",
    "Alternatively, you can use [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html), as [LCEL](/docs/concepts/lcel/) can be used inside any [LangGraph node](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes).\n",
    "\n",
    "To do that replace the following code:\n",
    "\n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:\n",
    "    # highlight-start\n",
    "    # Make sure that config is populated with the session id\n",
    "    if \"configurable\" not in config or \"session_id\" not in config[\"configurable\"]:\n",
    "        raise ValueError(\n",
    "            \"You make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}\"\n",
    "        )\n",
    "    # Fetch the history of messages and append to it any new messages.\n",
    "    chat_history = get_chat_history(config[\"configurable\"][\"session_id\"])\n",
    "    messages = list(chat_history.messages) + state[\"messages\"]\n",
    "    ai_message = model.invoke(messages)\n",
    "    # Finally, update the chat message history to include\n",
    "    # the new input message from the user together with the\n",
    "    # response from the model.\n",
    "    chat_history.add_messages(state[\"messages\"] + [ai_message])\n",
    "    # hilight-end\n",
    "    return {\"messages\": ai_message}\n",
    "```\n",
    "\n",
    "With the corresponding instance of `RunnableWithMessageHistory` defined in your current application.\n",
    "\n",
    "```python\n",
    "runnable = RunnableWithMessageHistory(...) # From existing code\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:\n",
    "    # RunnableWithMessageHistory takes care of reading the message history\n",
    "    # and updating it with the new human message and ai response.\n",
    "    ai_message = runnable.invoke(state['messages'], config)\n",
    "    return {\n",
    "        \"messages\": ai_message\n",
    "    }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
