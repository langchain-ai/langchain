{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# Query analysis\n",
    "\n",
    "## Problem\n",
    "In any question answering application we need to search for, or retrieve, information based on a user question. In the simplest case, we can search on the user input directly. This approach has a few common failure modes:\n",
    "\n",
    "* The data has multiple attributes that a user input could be referring to,\n",
    "* The user input contains multiple distinct questions in it,\n",
    "* Search quality is sensitive to phrasing.\n",
    "\n",
    "To handle these, we can do **query analysis** to translate the raw user question into a query or queries optimized for our index. \n",
    "\n",
    "**NOTE**: This guide assumes familiarity with the basic building blocks of a simple RAG application outlined in the [Quickstart](/docs/use_cases/question_answering/quickstart).\n",
    "\n",
    "## Solution\n",
    "\n",
    "Query analysis is the process of transforming a user input into a query optimized for your index(es). This can involve any of the folowing steps:\n",
    "\n",
    "* **Query decomposition**: If a user input contains multiple distinct questions, we can decompose the input into separate queries that will each be executed independently.\n",
    "* **Query expansion**: If an index is sensitive to query phrasing, we can multiple paraphrased versions of the user question to increase our chances of retrieving a relevant result.\n",
    "* **Query structuring**: If our documents have multiple searchable/filterable attributes, we can infer from any raw user question which specific attributes should be searched/filtered over. For example, when a user input specific something about video publication date, that should become a filter on the `publish_date` attribute of each document.\n",
    "* **Routing**: If we have multiple indexes and only a subset are useful for any given user input, we can route the input to only retrieve results from the relevant ones.\n",
    "\n",
    "Which of these steps are needed depends on the indexes we've created, which ultimately depends on the use case. The types of indexes we create and the query analysis steps we'll need might look very different when retrieving from a blog versus retrieving from a codebase. We'll cover all of the techniques at a high level in this guide, including when they're most useful and how you can optimize them.\n",
    "\n",
    "To illustrate, let's build a Q&A bot over the LangChain YouTube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168ef5c-e54e-49a6-8552-5502854a6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain langchain-community langchain-openai youtube-transcript-api pytube elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d66a45-a05c-4d22-b011-b1cdbdfc8f9c",
   "metadata": {},
   "source": [
    "#### Set environment variables\n",
    "\n",
    "We'll use OpenAI in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e2979e-a818-4b96-ac25-039336f94319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e12035-57bf-4395-aaa0-69cfcff1d0e9",
   "metadata": {},
   "source": [
    "### Set up integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35227397-62d4-4593-b001-7625c167e7e6",
   "metadata": {},
   "source": [
    "We'll use Elasticsearch for our vectorstore. We can run an Elasticsearch instance locally with Docker:\n",
    "\n",
    "```bash\n",
    "docker run -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.9.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b48b8-16d7-4089-bc17-f2d240b3935a",
   "metadata": {},
   "source": [
    "### Load documents\n",
    "\n",
    "We can use the `YouTubeLoader` to load transcripts of a few LangChain videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6921e1-3d5a-431c-9999-29a5f33201e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\",\n",
    "    \"https://www.youtube.com/watch?v=ylrew7qb8sQ\",\n",
    "    \"https://www.youtube.com/watch?v=uRya4zRrRx4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=3wAON0Lqviw\",\n",
    "    \"https://www.youtube.com/watch?v=jx7xuHlfsEQ\",\n",
    "    \"https://www.youtube.com/watch?v=xn1jEjRyJ2U\",\n",
    "    \"https://www.youtube.com/watch?v=SaDzIVkYqyY\",\n",
    "    \"https://www.youtube.com/watch?v=gqhlqdawHT4\",\n",
    "    \"https://www.youtube.com/watch?v=Ce03oEotdPs\",\n",
    "    \"https://www.youtube.com/watch?v=rZus0JtRqXE\",\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
    "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
    "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
    "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
    "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
    "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
    "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
    "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
    "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7da456-3023-4f04-bba1-f7e2c468c7fe",
   "metadata": {},
   "source": [
    "Here are the titles of the videos we've loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1a99ee-1078-4373-b80a-630af48bf94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'WebVoyager',\n",
       " 'LangGraph: Planning Agents',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangSmith: In-Depth Platform Overview',\n",
       " 'LangSmith in 10 Minutes',\n",
       " 'RAG from scratch: Part 8 (Query Translation -- Step Back)',\n",
       " 'RAG from scratch: Part 9 (Query Translation -- HyDE)',\n",
       " 'RAG from scratch: Part 7 (Query Translation -- Decomposition - v1)',\n",
       " 'LangChain Agents with Open Source Models!',\n",
       " 'Gemini + Google Retrieval Agent from a LangChain Template',\n",
       " 'OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Auto-Prompt Builder (with Hosted LangServe)',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'SQL Research Assistant',\n",
       " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a71032-14c3-4517-aa9a-3a5e88eaeb92",
   "metadata": {},
   "source": [
    "Here's the metadata associated with each video. We can see that each document also has a title, view count, publication date, and length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7748415-ddbf-4c55-a242-c28833c03caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pbAd8O1Lvm4',\n",
       " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 7946,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
       " 'publish_date': '2024-02-07 00:00:00',\n",
       " 'length': 1058,\n",
       " 'author': 'LangChain'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db72331-1e79-4910-8faa-473a0e370277",
   "metadata": {},
   "source": [
    "And here's a sample from a document's contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845149b7-130e-4228-ac80-d0a9286ef1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561697c8-b848-4b12-847c-ab6a8e2d1ae6",
   "metadata": {},
   "source": [
    "### Indexing documents\n",
    "\n",
    "Whenever we perform retrieval we need to create an index of documents that we can query. We'll use a vector store to index our documents, and we'll chunk them first to make our retrievals more concise and precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "83734980-ee66-4d03-acbf-20e81492008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma, ElasticsearchStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# clean up metadata\n",
    "for doc in docs:\n",
    "    doc.metadata[\"publish_date\"] = datetime.datetime.strptime(\n",
    "        doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
    "    ).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, chunk_overlap=500, add_start_index=True\n",
    ")\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = ElasticsearchStore.from_documents(\n",
    "    chunked_docs,\n",
    "    embeddings,\n",
    "    index_name=\"langchain_youtube_2\",\n",
    "    es_url=\"http://localhost:9200\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d8d0a-5c1b-46b0-862c-a4eccfd5ae3c",
   "metadata": {},
   "source": [
    "## Retrieval without query analysis\n",
    "\n",
    "We can perform similarity search on a user question directly to find chunks relevant to the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09435e9b-57b4-41b1-b34a-449815bdfae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is u\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79ef1b-7edd-4b68-98e5-c0e4c0dd02e6",
   "metadata": {},
   "source": [
    "This works pretty well! Our first result is quite relevant to the question.\n",
    "\n",
    "Now what if we remembered that there was a video series titled \"RAG from scratch\" and wanted to find that specifically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54de6ad-6f9f-49ca-86fd-5fcf3d318161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-reflective RAG with LangGraph: Self-RAG and CRAG\n",
      "hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\n",
    "    \"rag from scratch\",\n",
    ")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58067b4c-847d-4ad6-be65-2ac8afb1cd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'OpenGPTs',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res.metadata[\"title\"] for res in search_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891e8f5-ef0c-4ec0-b25f-eda7a5350a85",
   "metadata": {},
   "source": [
    "Since we're only searching over the transcriptions, and not over titles, our search misses all of the relevant documents. \n",
    "\n",
    "What if we wanted to search for results from a specific time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7adbfc11-ca01-4883-8978-e4f6e4a1d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "2024-01-31 00:00:00\n",
      "it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is u\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].metadata[\"publish_date\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790e2db-3c6e-440b-b6e8-ebdd6600fda5",
   "metadata": {},
   "source": [
    "Our first result is from 2024, and not very relevant to the input. Since we're just searching against document contents, there's no way for the results to be filtered on any document attributes.\n",
    "\n",
    "What if we wanted to know about deploying a LangChain chain as a REST API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52288ab-b048-4aa2-bf91-8183e14a9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve\n",
      " further and see um that the inputs to each of those kind of Lambda steps is going to be one of those documents and then we're outputting um like the highlights DL segment and then formatting that with our prompt template uh if you recall from when we were constructing it um so now we have kind of our uh fully constructed chain uh for our uh search enable chatbot with XF um and now let's convert that to Lang serve um so to do that we'll go back to vs code um and here we're going to um start with\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"chain as rest api\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[1500:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3996c-35d8-42d9-a92b-b8c546d35346",
   "metadata": {},
   "source": [
    "This brings up LangServe, the package for deploying chains as REST API's, as desired.\n",
    "\n",
    "But what if we added that we wanted a chain that made use of multi-modal models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5183436a-9236-4b09-8774-08186dadc4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Events: Introducing a new `stream_events` method\n",
      "streaming is uh an incredibly important ux consideration for building L Ms in a few ways first of all even if you're just working with a single llm call it can often take a while and you might want to stream individual tokens to the user so they can see what's happening as the llm responds second of all a lot of the things that we build in the laying chain are more complicated chains or agents and so being able to stream the intermediate steps what tool are being called what the input to those t\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    ")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c362d2-ec2d-4e0f-a122-130e809229cc",
   "metadata": {},
   "source": [
    "Our first result ends up not being about LangServe or multi-modal models. In reality \"chains as rest API\" and \"using multi-modal models\" are two fairly distinct questions that should be queried for separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396e23-c192-4d97-846b-5eacea4d6b8d",
   "metadata": {},
   "source": [
    "## Query analysis\n",
    "\n",
    "To handle these failure modes we can perform **query analysis**. Specifically, we can perform:\n",
    "\n",
    "* **Query structuring**: If our documents have multiple searchable/filterable attributes, we can infer from any raw user question which specific attributes should be searched/filtered over. For example, when a user input specific something about video publication date, that should become a filter on the `publish_date` attribute of each document.\n",
    "* **Query decomposition**: If a user input contains multiple distinct questions, we can decompose the input into separate queries.\n",
    "* **Query expansion**: If an index is sensitive to query phrasing, we can multiple paraphrased versions of the user question to increase our chances of retrieving a relevant result.\n",
    "\n",
    "To do this we'll define a **query schema** and use a function-calling model to convert a user question into a structured query or queries. The structured nature of the query schema allows us to do query structuring and routing, and the fact that we can extract multiple of these \n",
    "allows us to do decomposition and expansion.\n",
    "\n",
    "### Query schema\n",
    "In this case we'll have explicit min and max attributes for view count, publication date, and video length so that those can be filtered on. And we'll add separate attributes for searches against the transcript contents versus the video title. We'll also add some sorting attributes that we'll touch on later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b51dd76-820d-41a4-98c8-893f6fe0d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None, description=\"Minimum view count filter, inclusive.\"\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None, description=\"Maximum view count filter, exclusive.\"\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None, description=\"Earliest publish date filter, inclusive.\"\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None, description=\"Latest publish date filter, exclusive.\"\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None, description=\"Minimum video length in seconds, inclusive.\"\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None, description=\"Maximum video length in seconds, exclusive.\"\n",
    "    )\n",
    "    sort_by: Literal[\n",
    "        \"relevance\",\n",
    "        \"view_count\",\n",
    "        \"publish_date\",\n",
    "        \"length\",\n",
    "    ] = Field(\"relevance\", description=\"Attribute to sort by.\")\n",
    "    sort_order: Literal[\"ascending\", \"descending\"] = Field(\n",
    "        \"descending\", description=\"Whether to sort in ascending or descending order.\"\n",
    "    )\n",
    "    relevance_rank: int = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The index of this search query when all generated queries are sorted by \"\n",
    "            \"the expected relevance of their results to the original user question. \"\n",
    "            \"Each query must have a distinct index. A lower rank indicates higher \"\n",
    "            \"relevance to the user question.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08c52-1ce9-4d8b-a779-cbe8efde51d1",
   "metadata": {},
   "source": [
    "### Query generation\n",
    "\n",
    "To convert user questions to structured queries we'll make use of OpenAI's function-calling API. Since the latest OpenAI models can return multiple function invocations each turn, this approach automatically supports query expansion and decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question \\\n",
    "or common synonyms for key words in the question, make sure to return multiple versions \\\n",
    "of the query with the different phrasings.\n",
    "\n",
    "Perform query decomposition. If the user input contains a multi-part question, make \\\n",
    "sure to return a separate query for each distinct sub-question.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"examples\", optional=True),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([TutorialSearch])\n",
    "query_analyzer = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | PydanticToolsParser(tools=[TutorialSearch])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403517a-b8e3-44ac-b0a6-02f8305635a2",
   "metadata": {},
   "source": [
    "Let's see what queries our analyzer generates for the questions we searched earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92bc7bac-700d-4666-b523-f0f8c3644ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: RAG from scratch\n",
      "title_search: RAG\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: Reactive Agile Governance from scratch\n",
      "title_search: Reactive Agile Governance\n",
      "relevance_rank: 2\n",
      "\n",
      "content_search: How to build RAG applications from the beginning\n",
      "title_search: RAG applications\n",
      "relevance_rank: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"rag from scratch\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af62af17-4f90-4dbd-a8b4-dfff51f1db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: RAG\n",
      "title_search: RAG\n",
      "earliest_publish_date: 2023-01-01\n",
      "latest_publish_date: 2024-01-01\n",
      "relevance_rank: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"videos on RAG published in 2023\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87590c6d-edd7-4805-bf68-c906907f9291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models chain\n",
      "title_search: multi-modal models chain\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: chain into REST API\n",
      "title_search: chain REST API\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba81f0-f00b-4656-b840-037bf4306c60",
   "metadata": {},
   "source": [
    "### Improvements: Adding examples to the prompt\n",
    "\n",
    "To tune our results we can add some examples of inputs questions and gold standard output queries to our prompt. We'll focus on examples that show how to route and expand queries, to either be against titles or content, how to structure them with filters, and how to decompose them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d00d74b-7bc7-4224-ad09-fff8e7aeeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "171f3c37-36da-4a80-911e-d0447168b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Web Voyager? How about Gemini?\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        content_search=\"what is Web Voyager\",\n",
    "        title_search=\"Web Voyager\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"What is Gemini\", title_search=\"Gemini\", relevance_rank=2\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92523941-5aa0-4d1e-a795-1d14529b48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Have they released any chat langchain updates since 2024?\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        title_search=\"chat langchain\",\n",
    "        content_search=\"chat langchain\",\n",
    "        earliest_publish_date=datetime.date(2024, 1, 1),\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "844df58a-abd3-4c06-9a59-b7eccbbefc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to build multi-agent system and stream intermediate steps from it\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        content_search=\"How to build multi-agent system\",\n",
    "        title_search=\"multi-agent system\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"how to stream intermediate steps from multi-agent system\",\n",
    "        title_search=\"stream intermediate steps multi-agent system\",\n",
    "        relevance_rank=2,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"how to stream intermediate steps\",\n",
    "        title_search=\"stream intermediate steps\",\n",
    "        relevance_rank=3,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ee464-9f72-4a76-96fb-a87aeb29daa3",
   "metadata": {},
   "source": [
    "Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a `tool_example_to_messages` helper function to handle this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80a33517-afa5-4152-a041-55e01eadf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Dict) -> List[BaseMessage]:\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"This is an example of a correct usage of this tool. Well done. Make sure to continue using the tool this way.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages\n",
    "\n",
    "\n",
    "example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]\n",
    "query_analyzer_with_examples = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt.partial(examples=example_msgs)\n",
    "    | llm_with_tools\n",
    "    | PydanticToolsParser(tools=[TutorialSearch])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82824a2b-8985-430c-817a-6c8466bddf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch\n",
      "title_search: rag from scratch\n",
      "relevance_rank: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\"rag from scratch\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc2a270d-c239-4333-8167-5d9d1b0ce7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: how to use multi-modal models in a chain\n",
      "title_search: multi-modal models chain\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: how to turn chain into a REST API\n",
      "title_search: chain REST API\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a13f6aac-90f2-4495-84c2-6ce9de53a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: How to do extraction with agent\n",
      "title_search: extraction with agent\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: How to build agent with anthropic\n",
      "title_search: build agent anthropic\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\n",
    "    \"How to do extraction with agent? How to build agent with anthropic\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c65b2f-7881-45fc-a47b-a4eaaf48245f",
   "metadata": {},
   "source": [
    "## Retrieval with query analysis\n",
    "\n",
    "Our query analysis looks pretty good; now let's try using our generated queries to actually perform retrieval. We'll define a custom retrieval lambda that takes our output queries and correctly applies them to our indexes. \n",
    "\n",
    "Before we do that, we'll also need to create a separate index since we created an additional search field, `title_search`, for searching against video titles specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b87dd377-b2f8-4732-84f5-fb441f8fe3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "docstore.mset([(doc.metadata[\"source\"], doc) for doc in docs])\n",
    "\n",
    "title_docs = []\n",
    "for doc in docs:\n",
    "    metadata = deepcopy(doc.metadata)\n",
    "    title_docs.append(Document(metadata.pop(\"title\"), metadata=metadata))\n",
    "title_vectorstore = ElasticsearchStore.from_documents(\n",
    "    title_docs,\n",
    "    embeddings,\n",
    "    index_name=\"langchain_youtube_titles_3\",\n",
    "    es_url=\"http://localhost:9200\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6184f53b-139f-4dc8-95d5-affd65b0e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chains.query_constructor.ir import (\n",
    "    Comparator,\n",
    "    Comparison,\n",
    "    Operation,\n",
    "    Operator,\n",
    "    StructuredQuery,\n",
    ")\n",
    "from langchain.retrievers.self_query.elasticsearch import ElasticsearchTranslator\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def query_to_filter(query: TutorialSearch) -> dict:\n",
    "    comparisons = []\n",
    "    if query.min_view_count is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.GTE,\n",
    "                attribute=\"view_count\",\n",
    "                value=query.min_view_count,\n",
    "            )\n",
    "        )\n",
    "    if query.max_view_count is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.LT,\n",
    "                attribute=\"view_count\",\n",
    "                value=query.max_view_count,\n",
    "            )\n",
    "        )\n",
    "    if query.min_length_sec is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.GTE,\n",
    "                attribute=\"length\",\n",
    "                value=query.min_length_sec,\n",
    "            )\n",
    "        )\n",
    "    if query.max_length_sec is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.LT, attribute=\"length\", value=query.max_length_sec\n",
    "            )\n",
    "        )\n",
    "    if query.earliest_publish_date is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.GTE,\n",
    "                attribute=\"publish_date\",\n",
    "                value={\"type\": \"date\", \"date\": query.earliest_publish_date},\n",
    "            )\n",
    "        )\n",
    "    if query.latest_publish_date is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.LT,\n",
    "                attribute=\"publish_date\",\n",
    "                value={\"type\": \"date\", \"date\": query.latest_publish_date},\n",
    "            )\n",
    "        )\n",
    "    if comparisons:\n",
    "        filter = Operation(operator=Operator.AND, arguments=comparisons)\n",
    "        return ElasticsearchTranslator().visit_operation(filter)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def content_search(input: dict) -> List[Document]:\n",
    "    return vectorstore.similarity_search_with_score(\n",
    "        input[\"query\"].content_search, filter=input[\"filter\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def title_search(input: dict) -> List[Document]:\n",
    "    title_docs_scores = title_vectorstore.similarity_search_with_score(\n",
    "        input[\"query\"].title_search, filter=input[\"filter\"]\n",
    "    )\n",
    "    docs = docstore.mget(\n",
    "        [title_doc.metadata[\"source\"] for title_doc, _ in title_docs_scores]\n",
    "    )\n",
    "    scores = [scores for _, scores in title_docs_scores]\n",
    "    return list(zip(docs, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "232ad8a7-7990-4066-9228-d35a555f7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_and_filters(queries: List[TutorialSearch]) -> List[Dict]:\n",
    "    return [{\"query\": q, \"filter\": query_to_filter(q)} for q in queries]\n",
    "\n",
    "\n",
    "search = RunnablePassthrough.assign(\n",
    "    content_docs=content_search, title_docs=title_search\n",
    ")\n",
    "retrieval = query_analyzer_with_examples | queries_and_filters | search.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "028b0203-ac01-42b9-a630-492729609e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieval.invoke(\"rag from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b7ebbd3e-08b5-4f7d-85a5-4a005f9e9dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'OpenGPTs',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for result in results for doc, _ in result[\"content_docs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b46bb84d-57f9-49fb-86eb-52459d953c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'RAG from scratch: Part 8 (Query Translation -- Step Back)',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'RAG from scratch: Part 7 (Query Translation -- Decomposition - v1)']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for result in results for doc, _ in result[\"title_docs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "35040f41-ba34-4079-ab96-f23568a09515",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieval.invoke(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "38f10523-7811-4596-b78e-02acbbe3e5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Streaming Events: Introducing a new `stream_events` method',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'LangChain Agents with Open Source Models!',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for result in results for doc, _ in result[\"content_docs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5c8d8ec8-42c8-40aa-90db-fc7ed851dcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain Agents with Open Source Models!',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for result in results for doc, _ in result[\"title_docs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e7f683b5-b1c5-4dec-b163-2efc162a2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieval.invoke(\"RAG tutorial published in 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1ad52512-b3e8-42a3-8701-d9e87fb8b46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Getting Started with Multi-Modal LLMs', '2023-12-20'),\n",
       " ('LangServe and LangChain Templates Webinar', '2023-11-02'),\n",
       " ('Getting Started with Multi-Modal LLMs', '2023-12-20'),\n",
       " ('SQL Research Assistant', '2023-12-19')]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (doc.metadata[\"title\"], doc.metadata[\"publish_date\"])\n",
    "    for result in results\n",
    "    for doc, _ in result[\"content_docs\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8c770730-363a-4b1a-948c-6b2c79c34ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieval.invoke(\"short tutorial (less than 15 min) on agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "07681898-0a1b-4633-be58-a7493b8a894c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gemini + Google Retrieval Agent from a LangChain Template', 776),\n",
       " ('Gemini + Google Retrieval Agent from a LangChain Template', 776),\n",
       " ('Gemini + Google Retrieval Agent from a LangChain Template', 776),\n",
       " ('LangSmith in 10 Minutes', 561)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (doc.metadata[\"title\"], doc.metadata[\"length\"])\n",
    "    for result in results\n",
    "    for doc, _ in result[\"content_docs\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a90a6-b61b-45d5-96d4-920a3e455594",
   "metadata": {},
   "source": [
    "### Generation\n",
    "\n",
    "Most of the time we're doing retrieval so that we can generate an informed response. To do this we'll need to consolidate our current retrieval results. Currently we return a search result for each constructed query, and that result contains both document chunks that match the query and full documents whose titles match the query. There's a few possible ways we could combine these results:\n",
    "\n",
    "* Dump all docs in the prompt. This might work fine for models with large context windows, but could cause issues with smaller windows since the results from the title search can be quite long.\n",
    "* Compress title docs. If our title docs are too large to pass to the model, we can do some post-processing to chunk them and discard all but the most relevant chunks.\n",
    "* Use title docs to rerank chunked docs. We could use the returned titles to rerank the chunked docs by taking into account how relevant their titles are. We might fetch 20 chunks and 20 titles, rerank the chunks based on their content + title relevance, and then include the top 10 reranked chunks in the prompt.\n",
    "\n",
    "To keep things simple here we'll go with the first approach, which is including all results in the model prompt. To account for the fact that title docs are much longer, we'll update our retrieval to return fewer title docs than chunked docs. We'll also update our retrieval to drop any chunks which are a part of one of the title docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f3ab293a-c5ef-4be5-b0ae-0d5830512417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "def content_search(input: dict, config: RunnableConfig) -> List[Document]:\n",
    "    return vectorstore.similarity_search_with_score(\n",
    "        input[\"query\"].content_search,\n",
    "        filter=input[\"filter\"],\n",
    "        k=config[\"configurable\"][\"content_k\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def title_search(input: dict, config: RunnableConfig) -> List[Document]:\n",
    "    title_docs_scores = title_vectorstore.similarity_search_with_score(\n",
    "        input[\"query\"].title_search,\n",
    "        filter=input[\"filter\"],\n",
    "        k=config[\"configurable\"][\"title_k\"],\n",
    "    )\n",
    "    docs = docstore.mget(\n",
    "        [title_doc.metadata[\"source\"] for title_doc, _ in title_docs_scores]\n",
    "    )\n",
    "    scores = [scores for _, scores in title_docs_scores]\n",
    "    return list(zip(docs, scores))\n",
    "\n",
    "\n",
    "def dedup(input: dict):\n",
    "    titles = [doc.metadata[\"source\"] for doc, _ in input[\"title_docs\"]]\n",
    "    content_docs = []\n",
    "    for doc, score in input[\"content_docs\"]:\n",
    "        if doc.metadata[\"source\"] not in titles:\n",
    "            content_docs.append((doc, score))\n",
    "    input[\"content_docs\"] = content_docs\n",
    "    return input\n",
    "\n",
    "\n",
    "search = (\n",
    "    RunnablePassthrough.assign(content_docs=content_search, title_docs=title_search)\n",
    "    | dedup\n",
    ")\n",
    "\n",
    "retrieval = query_analyzer_with_examples | queries_and_filters | search.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0fafd316-2d57-4c2c-8b73-d410bcf157b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = retrieval.invoke(\n",
    "    \"what's RAG\", config={\"configurable\": {\"content_k\": 10, \"title_k\": 3}}\n",
    ")\n",
    "len(results[0][\"content_docs\"]), len(results[0][\"title_docs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63070dde-384a-457e-b606-139a6a5db79e",
   "metadata": {},
   "source": [
    "Now we can set up the actual question-answering portion of our Q&A application. Again, for simplicity we'll just dump all the retrieved documents into a single prompt and let the model answer based off that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2508ecf1-cfe3-4fa8-8ce9-78e2ed1b9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_docs(input: list):\n",
    "    docs = []\n",
    "    for query_results in input:\n",
    "        docs.extend(query_results[\"content_docs\"])\n",
    "        docs.extend(query_results[\"title_docs\"])\n",
    "    return \"\\n\\n\".join(doc.page_content for doc, _ in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user question ONLY using the following up-to-date context. Do not worry about your last update date:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)\n",
    "parser = StrOutputParser()\n",
    "qa_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retrieval | format_docs}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5f38f004-4954-4a9e-b435-0371017ccd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2024, the LangGraph team has been actively working on enhancing the capabilities and applications of LangGraph, focusing on multi-agent workflows, collaboration, and hierarchical agent teams. Here are some key updates and developments:\n",
      "\n",
      "1. **Multi-Agent Collaboration**: The team introduced concepts for multi-agent collaboration, where multiple agents work on the same state of messages, sharing information and tasks to achieve complex goals. This approach allows for a more collaborative and efficient problem-solving process, where agents can leverage each other's strengths.\n",
      "\n",
      "2. **Agent Supervisor Model**: A new model called the \"Agent Supervisor\" was introduced, where a supervisor agent oversees the operation of multiple independent agents. Each agent works on its task and returns the final answer to the supervisor, which then decides the next course of action. This model allows for a clear separation of concerns and can enable more complex workflows by combining the outputs of various agents.\n",
      "\n",
      "3. **Hierarchical Agent Teams**: The team explored hierarchical agent teams, where agents are organized in a hierarchical structure, with each agent or team of agents handling specific parts of a larger task. This structure allows for the decomposition of complex tasks into manageable sub-tasks, which can be tackled by specialized agents or teams.\n",
      "\n",
      "4. **Enhanced Debugging and Observability with LangSmith**: Integration with LangSmith has been improved to provide better debugging and observability for multi-agent workflows created with LangGraph. This allows developers to track the execution of agents, understand the flow of information between agents, and debug issues more effectively.\n",
      "\n",
      "5. **Expanded Toolset for Agents**: The toolset available for agents to use within LangGraph workflows has been expanded, including new tools for data retrieval, processing, and analysis. This expansion enables agents to perform a wider range of tasks and handle more complex workflows.\n",
      "\n",
      "6. **Improved Performance and Scalability**: Efforts have been made to improve the performance and scalability of LangGraph, ensuring that it can handle larger and more complex multi-agent workflows efficiently. This includes optimizations in the execution of agent workflows and enhancements in the underlying infrastructure.\n",
      "\n",
      "7. **Community Contributions and Templates**: The LangGraph team has encouraged community contributions, leading to a richer set of templates and examples for building multi-agent workflows. These community-contributed resources provide valuable starting points and inspiration for developers creating their own workflows.\n",
      "\n",
      "8. **Educational Resources and Documentation**: The team has released new educational resources, including tutorials, videos, and documentation, to help developers get started with LangGraph and build effective multi-agent workflows. These resources cover best practices, design patterns, and case studies to illustrate the capabilities of LangGraph.\n",
      "\n",
      "These updates reflect the LangGraph team's commitment to advancing the state of multi-agent systems and providing developers with powerful tools to build complex, collaborative workflows."
     ]
    }
   ],
   "source": [
    "for chunk in qa_chain.stream(\n",
    "    \"what are some updates the team has posted about LangGraph in 2024\",\n",
    "    config={\"configurable\": {\"content_k\": 10, \"title_k\": 3}},\n",
    "):\n",
    "    print(chunk, flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b2abc-6a71-4be4-ac63-d2d772515825",
   "metadata": {},
   "source": [
    "And here's what the trace would look like for this call: https://smith.langchain.com/public/9a70236f-5510-443c-a9da-084d9d07345d/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc228c39-01f0-4475-b9bf-15d33033dbb7",
   "metadata": {},
   "source": [
    "##  Under construction   Sorting: Going beyond search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "608f11cc-a24f-499a-941f-28f68cc16cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What has LangChain released lately?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(sort_by=\"publish_date\", relevance_rank=1),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9695291a-7f14-4eea-8a79-693bf3e0b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the most popular videos about RAG?\"\n",
    "queries = [\n",
    "    TutorialSearch(title_search=\"RAG\", sort_by=\"view_count\", relevance_rank=1),\n",
    "    TutorialSearch(content_search=\"RAG\", sort_by=\"view_count\", relevance_rank=2),\n",
    "    TutorialSearch(\n",
    "        content_search=\"retrieval augmented generation\",\n",
    "        sort_by=\"view_count\",\n",
    "        relevance_rank=3,\n",
    "    ),\n",
    "    TutorialSearch(content_search=\"retrieval\", sort_by=\"view_count\", relevance_rank=4),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa52db91-622d-439b-bc2d-ed4b17c350eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some short videos about agentic systems with local models?\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        content_search=\"how to build an agent using a local model\",\n",
    "        sort_by=\"length\",\n",
    "        sort_order=\"ascending\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        title_search=\"agent local model\",\n",
    "        sort_by=\"length\",\n",
    "        sort_order=\"ascending\",\n",
    "        relevance_rank=2,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5965eb3a-9dc1-4a01-9453-87f248de046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainYouTubeSearch(content='local LLMs', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='publish_date', sort_order='descending', relevance_rank=1)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"Recent talks about local llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a5b80f99-5733-407a-a593-8f37868c4a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainYouTubeSearch(content='new videos about streaming state machines', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='relevance', sort_order='descending', relevance_rank=1)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"new videos about streaming state machines\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv-2",
   "language": "python",
   "name": "poetry-venv-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
