interactions:
- request:
    body: null
    headers:
      Accept:
      - application/json
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - langsmith-py/0.1.128
    method: GET
    uri: https://api.smith.langchain.com/info
  response:
    body:
      string: '{"version":"0.7.36","license_expiration_time":null,"batch_ingest_config":{"scale_up_qsize_trigger":1000,"scale_up_nthreads_limit":16,"scale_down_nempty_trigger":4,"size_limit":100,"size_limit_bytes":20971520},"instance_flags":{"generate_ai_query_enabled":true,"org_creation_disabled":false,"show_ttl_ui":true,"trace_tier_duration_days":{"longlived":400,"shortlived":14},"search_enabled":true,"workspace_scope_org_invites":false,"s3_storage_enabled":true}}'
    headers:
      Access-Control-Allow-Credentials:
      - 'true'
      Access-Control-Allow-Headers:
      - '*'
      Access-Control-Allow-Methods:
      - '*'
      Access-Control-Allow-Origin:
      - ''
      Access-Control-Expose-Headers:
      - '*'
      Access-Control-Max-Age:
      - '600'
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      Content-Length:
      - '455'
      Via:
      - 1.1 google
      cache-control:
      - no-cache
      content-type:
      - application/json
      date:
      - Wed, 25 Sep 2024 22:31:13 GMT
      server:
      - uvicorn
    status:
      code: 200
      message: OK
- request:
    body: "{\"post\":[{\"id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"start_time\":\"2024-09-25T22:31:14.336966+00:00\",\"end_time\":null,\"extra\":{\"runtime\":{\"sdk\":\"langsmith-py\",\"sdk_version\":\"0.1.128\",\"library\":\"langchain-core\",\"platform\":\"macOS-14.6-arm64-arm-64bit\",\"runtime\":\"python\",\"py_implementation\":\"CPython\",\"runtime_version\":\"3.11.7\",\"langchain_version\":\"0.3.0\",\"langchain_core_version\":\"0.3.5\",\"library_version\":\"0.3.5\"},\"metadata\":{\"revision_id\":\"langchain-experimental==0.3.1-32-g184428cfd-dirty\"}},\"error\":null,\"events\":[{\"name\":\"start\",\"time\":\"2024-09-25T22:31:14.336966+00:00\"}],\"reference_example_id\":null,\"parent_run_id\":null,\"tags\":[],\"session_name\":\"default\",\"session_id\":null,\"dotted_order\":\"20240925T223114336966Za6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"trace_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"outputs\":{},\"name\":\"stuff_documents_chain\",\"inputs\":{\"context\":[{\"metadata\":{\"source\":\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\"title\":\"LLM
      Powered Autonomous Agents | Lil'Log\",\"description\":\"Building agents with
      LLM (large language model) as its core controller is a cool concept. Several
      proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as
      inspiring examples. The potentiality of LLM extends beyond generating well-written
      copies, stories, essays and programs; it can be framed as a powerful general
      problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system,
      LLM functions as the agent\u2019s brain, complemented by several key components:\",\"language\":\"en\"},\"page_content\":\"\\n\\n\\n\\n\\n\\nLLM
      Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"type\":\"Document\"}]},\"run_type\":\"chain\"},{\"id\":\"ff6a7999-da46-4bb8-a3a0-ef26103d91ac\",\"start_time\":\"2024-09-25T22:31:14.337312+00:00\",\"end_time\":\"2024-09-25T22:31:14.338578+00:00\",\"extra\":{\"runtime\":{\"sdk\":\"langsmith-py\",\"sdk_version\":\"0.1.128\",\"library\":\"langsmith\",\"platform\":\"macOS-14.6-arm64-arm-64bit\",\"runtime\":\"python\",\"py_implementation\":\"CPython\",\"runtime_version\":\"3.11.7\",\"langchain_version\":\"0.3.0\",\"langchain_core_version\":\"0.3.5\"},\"metadata\":{\"revision_id\":\"langchain-experimental==0.3.1-32-g184428cfd-dirty\"}},\"error\":null,\"events\":[{\"name\":\"start\",\"time\":\"2024-09-25T22:31:14.337312+00:00\"},{\"name\":\"end\",\"time\":\"2024-09-25T22:31:14.338578+00:00\"}],\"reference_example_id\":null,\"parent_run_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"tags\":[\"seq:step:1\"],\"session_name\":\"default\",\"session_id\":null,\"dotted_order\":\"20240925T223114336966Za6bac5cf-713e-4d9d-84cc-d3687edb3479.20240925T223114337312Zff6a7999-da46-4bb8-a3a0-ef26103d91ac\",\"trace_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"outputs\":{\"context\":\"\\n\\n\\n\\n\\n\\nLLM
      Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"name\":\"format_inputs\",\"inputs\":{\"context\":[{\"metadata\":{\"source\":\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\"title\":\"LLM
      Powered Autonomous Agents | Lil'Log\",\"description\":\"Building agents with
      LLM (large language model) as its core controller is a cool concept. Several
      proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as
      inspiring examples. The potentiality of LLM extends beyond generating well-written
      copies, stories, essays and programs; it can be framed as a powerful general
      problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system,
      LLM functions as the agent\u2019s brain, complemented by several key components:\",\"language\":\"en\"},\"page_content\":\"\\n\\n\\n\\n\\n\\nLLM
      Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"type\":\"Document\"}]},\"run_type\":\"chain\"},{\"id\":\"e3bea8c4-9fad-4d16-89cc-dac2e4e0a33b\",\"start_time\":\"2024-09-25T22:31:14.337937+00:00\",\"end_time\":\"2024-09-25T22:31:14.338503+00:00\",\"extra\":{\"runtime\":{\"sdk\":\"langsmith-py\",\"sdk_version\":\"0.1.128\",\"library\":\"langsmith\",\"platform\":\"macOS-14.6-arm64-arm-64bit\",\"runtime\":\"python\",\"py_implementation\":\"CPython\",\"runtime_version\":\"3.11.7\",\"langchain_version\":\"0.3.0\",\"langchain_core_version\":\"0.3.5\"},\"metadata\":{\"revision_id\":\"langchain-experimental==0.3.1-32-g184428cfd-dirty\"}},\"error\":null,\"events\":[{\"name\":\"start\",\"time\":\"2024-09-25T22:31:14.337937+00:00\"},{\"name\":\"end\",\"time\":\"2024-09-25T22:31:14.338503+00:00\"}],\"reference_example_id\":null,\"parent_run_id\":\"ff6a7999-da46-4bb8-a3a0-ef26103d91ac\",\"tags\":[],\"session_name\":\"default\",\"session_id\":null,\"dotted_order\":\"20240925T223114336966Za6bac5cf-713e-4d9d-84cc-d3687edb3479.20240925T223114337312Zff6a7999-da46-4bb8-a3a0-ef26103d91ac.20240925T223114337937Ze3bea8c4-9fad-4d16-89cc-dac2e4e0a33b\",\"trace_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"outputs\":{\"context\":\"\\n\\n\\n\\n\\n\\nLLM
      Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"name\":\"RunnableParallel<context>\",\"inputs\":{\"context\":[{\"metadata\":{\"source\":\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\"title\":\"LLM
      Powered Autonomous Agents | Lil'Log\",\"description\":\"Building agents with
      LLM (large language model) as its core controller is a cool concept. Several
      proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as
      inspiring examples. The potentiality of LLM extends beyond generating well-written
      copies, stories, essays and programs; it can be framed as a powerful general
      problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system,
      LLM functions as the agent\u2019s brain, complemented by several key components:\",\"language\":\"en\"},\"page_content\":\"\\n\\n\\n\\n\\n\\nLLM
      Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"type\":\"Document\"}]},\"run_type\":\"chain\"},{\"id\":\"444a8245-13b7-420a-98e1-bbefd3f11d90\",\"start_time\":\"2024-09-25T22:31:14.338202+00:00\",\"end_time\":\"2024-09-25T22:31:14.338388+00:00\",\"extra\":{\"runtime\":{\"sdk\":\"langsmith-py\",\"sdk_version\":\"0.1.128\",\"library\":\"langsmith\",\"platform\":\"macOS-14.6-arm64-arm-64bit\",\"runtime\":\"python\",\"py_implementation\":\"CPython\",\"runtime_version\":\"3.11.7\",\"langchain_version\":\"0.3.0\",\"langchain_core_version\":\"0.3.5\"},\"metadata\":{\"revision_id\":\"langchain-experimental==0.3.1-32-g184428cfd-dirty\"}},\"error\":null,\"events\":[{\"name\":\"start\",\"time\":\"2024-09-25T22:31:14.338202+00:00\"},{\"name\":\"end\",\"time\":\"2024-09-25T22:31:14.338388+00:00\"}],\"reference_example_id\":null,\"parent_run_id\":\"e3bea8c4-9fad-4d16-89cc-dac2e4e0a33b\",\"tags\":[\"map:key:context\"],\"session_name\":\"default\",\"session_id\":null,\"dotted_order\":\"20240925T223114336966Za6bac5cf-713e-4d9d-84cc-d3687edb3479.20240925T223114337312Zff6a7999-da46-4bb8-a3a0-ef26103d91ac.20240925T223114337937Ze3bea8c4-9fad-4d16-89cc-dac2e4e0a33b.20240925T223114338202Z444a8245-13b7-420a-98e1-bbefd3f11d90\",\"trace_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"outputs\":{\"output\":\"\\n\\n\\n\\n\\n\\nLLM
      Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"name\":\"format_docs\",\"inputs\":{\"context\":[{\"metadata\":{\"source\":\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\"title\":\"LLM
      Powered Autonomous Agents | Lil'Log\",\"description\":\"Building agents with
      LLM (large language model) as its core controller is a cool concept. Several
      proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as
      inspiring examples. The potentiality of LLM extends beyond generating well-written
      copies, stories, essays and programs; it can be framed as a powerful general
      problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system,
      LLM functions as the agent\u2019s brain, complemented by several key components:\",\"language\":\"en\"},\"page_content\":\"\\n\\n\\n\\n\\n\\nLLM
      Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"type\":\"Document\"}]},\"run_type\":\"chain\"},{\"id\":\"7995a2ff-0c2f-46de-b9fe-dc7fbcd62342\",\"start_time\":\"2024-09-25T22:31:14.338796+00:00\",\"end_time\":\"2024-09-25T22:31:14.339045+00:00\",\"extra\":{\"runtime\":{\"sdk\":\"langsmith-py\",\"sdk_version\":\"0.1.128\",\"library\":\"langsmith\",\"platform\":\"macOS-14.6-arm64-arm-64bit\",\"runtime\":\"python\",\"py_implementation\":\"CPython\",\"runtime_version\":\"3.11.7\",\"langchain_version\":\"0.3.0\",\"langchain_core_version\":\"0.3.5\"},\"metadata\":{\"revision_id\":\"langchain-experimental==0.3.1-32-g184428cfd-dirty\"}},\"error\":null,\"serialized\":{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain\",\"prompts\",\"chat\",\"ChatPromptTemplate\"],\"kwargs\":{\"input_variables\":[\"context\"],\"messages\":[{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain\",\"prompts\",\"chat\",\"SystemMessagePromptTemplate\"],\"kwargs\":{\"prompt\":{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain\",\"prompts\",\"prompt\",\"PromptTemplate\"],\"kwargs\":{\"input_variables\":[\"context\"],\"template\":\"Write
      a concise summary of the following:\\\\n\\\\n{context}\",\"template_format\":\"f-string\"},\"name\":\"PromptTemplate\"}}}]},\"name\":\"ChatPromptTemplate\"},\"events\":[{\"name\":\"start\",\"time\":\"2024-09-25T22:31:14.338796+00:00\"},{\"name\":\"end\",\"time\":\"2024-09-25T22:31:14.339045+00:00\"}],\"reference_example_id\":null,\"parent_run_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"tags\":[\"seq:step:2\"],\"session_name\":\"default\",\"session_id\":null,\"dotted_order\":\"20240925T223114336966Za6bac5cf-713e-4d9d-84cc-d3687edb3479.20240925T223114338796Z7995a2ff-0c2f-46de-b9fe-dc7fbcd62342\",\"trace_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"outputs\":{\"output\":{\"messages\":[{\"content\":\"Write
      a concise summary of the following:\\\\n\\\\n\\n\\n\\n\\n\\n\\nLLM Powered Autonomous
      Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"additional_kwargs\":{},\"response_metadata\":{},\"type\":\"system\"}]}},\"name\":\"ChatPromptTemplate\",\"inputs\":{\"context\":\"\\n\\n\\n\\n\\n\\nLLM
      Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\"run_type\":\"prompt\"},{\"id\":\"8a926a1e-b52d-458d-9501-3a5a733a9813\",\"start_time\":\"2024-09-25T22:31:14.339276+00:00\",\"end_time\":null,\"extra\":{\"invocation_params\":{\"model\":\"gpt-4o-mini\",\"model_name\":\"gpt-4o-mini\",\"stream\":false,\"n\":1,\"temperature\":0.0,\"_type\":\"openai-chat\",\"stop\":null},\"options\":{\"stop\":null},\"batch_size\":1,\"metadata\":{\"ls_provider\":\"openai\",\"ls_model_name\":\"gpt-4o-mini\",\"ls_model_type\":\"chat\",\"ls_temperature\":0.0,\"revision_id\":\"langchain-experimental==0.3.1-32-g184428cfd-dirty\"},\"runtime\":{\"sdk\":\"langsmith-py\",\"sdk_version\":\"0.1.128\",\"library\":\"langchain-core\",\"platform\":\"macOS-14.6-arm64-arm-64bit\",\"runtime\":\"python\",\"py_implementation\":\"CPython\",\"runtime_version\":\"3.11.7\",\"langchain_version\":\"0.3.0\",\"langchain_core_version\":\"0.3.5\",\"library_version\":\"0.3.5\"}},\"error\":null,\"serialized\":{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain\",\"chat_models\",\"openai\",\"ChatOpenAI\"],\"kwargs\":{\"model_name\":\"gpt-4o-mini\",\"temperature\":0.0,\"openai_api_key\":{\"lc\":1,\"type\":\"secret\",\"id\":[\"OPENAI_API_KEY\"]},\"max_retries\":2,\"n\":1},\"name\":\"ChatOpenAI\"},\"events\":[{\"name\":\"start\",\"time\":\"2024-09-25T22:31:14.339276+00:00\"}],\"reference_example_id\":null,\"parent_run_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"tags\":[\"seq:step:3\"],\"session_name\":\"default\",\"session_id\":null,\"dotted_order\":\"20240925T223114336966Za6bac5cf-713e-4d9d-84cc-d3687edb3479.20240925T223114339276Z8a926a1e-b52d-458d-9501-3a5a733a9813\",\"trace_id\":\"a6bac5cf-713e-4d9d-84cc-d3687edb3479\",\"outputs\":{},\"name\":\"ChatOpenAI\",\"inputs\":{\"messages\":[[{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain\",\"schema\",\"messages\",\"SystemMessage\"],\"kwargs\":{\"content\":\"Write
      a concise summary of the following:\\\\n\\\\n\\n\\n\\n\\n\\n\\nLLM Powered Autonomous
      Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n
      \     LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated
      Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent
      System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent
      Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent
      Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative
      Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks
      down large tasks into smaller, manageable subgoals, enabling efficient handling
      of complex tasks.\\nReflection and refinement: The agent can do self-criticism
      and self-reflection over past actions, learn from mistakes and refine them for
      future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for
      extra information that is missing from the model weights (often hard to change
      after pre-training), including current information, code execution capability,
      access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview
      of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201Cthink step by step\u201D to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM
      with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals
      for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write
      a story outline.\\\" for writing a novel, or (3) with human inputs.\\nAnother
      quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201CProblem
      PDDL\u201D, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201CDomain PDDL\u201D, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows
      autonomous agents to improve iteratively by refining past action decisions and
      correcting previous mistakes. It plays a crucial role in real-world tasks where
      trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning
      and acting within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\\nThought:
      ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.
      \ Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA,
      FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:
      Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt,
      each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$
      is the corresponding human-provided hindsight feedback. Assume the feedback
      tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$
      The process is supervised fine-tuning where the data is a sequence in the form
      of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq
      i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned
      on the sequence prefix, such that the model can self-reflect to produce better
      output based on the feedback sequence. The model can optionally receive multiple
      rounds of instructions with human annotators at test time.\\nTo avoid overfitting,
      CoH adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\\n\\nFig. 5.
      After fine-tuning with CoH, the model can follow instructions to produce outputs
      with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD)
      works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any
      algorithm that generates a set of learning histories can be distilled into a
      neural network by performing behavioral cloning over actions. The history data
      is generated by a set of source policies, each trained for a specific task.
      At the training stage, during each RL run, a random task is sampled and a subsequence
      of multi-episode history is used for training, such that the learned policy
      is task-agnostic.\\nIn reality, the model has limited context window length,
      so episodes should be short enough to construct multi-episode history. Multi-episodic
      contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL
      algorithm. The emergence of in-context RL requires long enough context.\\nIn
      comparison with three baselines, including ED (expert distillation, behavior
      cloning with expert trajectories instead of learning history), source policy
      (used for generating trajectories for distillation by UCB), RL^2 (Duan et al.
      2017; used as upper bound since it needs online RL), AD demonstrates in-context
      RL with performance getting close to RL^2 despite only using offline RL and
      learns much faster than other baselines. When conditioned on partial training
      history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig.
      7. Comparison of AD, ED, source policy and RL^2 on environments that require
      memory and exploration. Only binary reward is assigned. The source policies
      are trained with A3C for \\\"dark\\\" environments and DQN for watermaze.(Image
      source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT
      for helping me draft this section. I\u2019ve learned a lot about the human brain
      and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes
      of Memory#\\nMemory can be defined as the processes used to acquire, store,
      retain, and later retrieve information. There are several types of memory in
      human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory,
      providing the ability to retain impressions of sensory information (visual,
      auditory, etc) after the original stimuli have ended. Sensory memory typically
      only lasts for up to a few seconds. Subcategories include iconic memory (visual),
      echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory
      (STM) or Working Memory: It stores information that we are currently aware of
      and needed to carry out complex cognitive tasks such as learning and reasoning.
      Short-term memory is believed to have the capacity of about 7 items (Miller
      1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts
      and events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can
      roughly consider the following mappings:\\n\\nSensory memory as learning embedding
      representations for raw inputs, including text, image or other modalities;\\nShort-term
      memory as in-context learning. It is short and finite, as it is restricted by
      the finite context window length of Transformer.\\nLong-term memory as the external
      vector store that the agent can attend to at query time, accessible via fast
      retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can
      alleviate the restriction of finite attention span.  A standard practice is
      to save the embedding representation of information into a vector store database
      that can support fast maximum inner-product search (MIPS). To optimize the retrieval
      speed, the common choice is the approximate nearest neighbors (ANN)\u200B algorithm
      to return approximately top k nearest neighbors to trade off a little accuracy
      lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast
      MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function
      such that similar input items are mapped to the same buckets with high probability,
      where the number of buckets is much smaller than the number of inputs.\\nANNOY
      (Approximate Nearest Neighbors Oh Yeah): The core data structure are random
      projection trees, a set of binary trees where each non-leaf node represents
      a hyperplane splitting the input space into half and each leaf stores one data
      point. Trees are built independently and at random, so to some extent, it mimics
      a hashing function. ANNOY search happens in all the trees to iteratively search
      through the half that is closest to the query and then aggregates the results.
      The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical
      Navigable Small World): It is inspired by the idea of small world networks where
      most nodes can be reached by any other nodes within a small number of steps;
      e.g. \u201Csix degrees of separation\u201D feature of social networks. HNSW
      builds hierarchical layers of these small-world graphs, where the bottom layers
      contain the actual data points. The layers in the middle create shortcuts to
      speed up search. When performing a search, HNSW starts from a random node in
      the top layer and navigates towards the target. When it can\u2019t get any closer,
      it moves down to the next layer, until it reaches the bottom layer. Each move
      in the upper layers can potentially cover a large distance in the data space,
      and each move in the lower layers refines the search quality.\\nFAISS (Facebook
      AI Similarity Search): It operates on the assumption that in high dimensional
      space, distances between nodes follow a Gaussian distribution and thus there
      should exist clustering of data points. FAISS applies vector quantization by
      partitioning the vector space into clusters and then refining the quantization
      within clusters. Search first looks for cluster candidates with coarse quantization
      and then further looks into each cluster with finer quantization.\\nScaNN (Scalable
      Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization.
      It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product
      $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle
      q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid
      points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10.
      (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance
      comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is
      a remarkable and distinguishing characteristic of human beings. We create, modify
      and utilize external objects to do things that go beyond our physical and cognitive
      limits. Equipping LLMs with external tools can significantly extend the model
      capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open
      a seashell, while floating in the water. While some other animals can use tools,
      the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL
      (Karpas et al. 2022), short for \u201CModular Reasoning, Knowledge and Language\u201D,
      is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed
      to contain a collection of \u201Cexpert\u201D modules and the general-purpose
      LLM works as a router to route inquiries to the best suitable expert module.
      These modules can be neural (e.g. deep learning models) or symbolic (e.g. math
      calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning
      LLM to call a calculator, using arithmetic as a test case. Their experiments
      showed that it was harder to solve verbal math problems than explicitly stated
      math problems because LLMs (7B Jurassic1-large model) failed to extract the
      right arguments for the basic arithmetic reliably. The results highlight when
      the external symbolic tools can work reliably, knowing when to and how to use
      the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented
      Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune
      a LM to learn to use external tool APIs. The dataset is expanded based on whether
      a newly added API call annotation can improve the quality of model outputs.
      See more details in the \u201CExternal APIs\u201D section of Prompt Engineering.\\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\\n\\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe
      system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe
      AI assistant can parse user input to several tasks: [{\\\"task\\\": task, \\\"id\\\",
      task_id, \\\"dep\\\": dependency_task_ids, \\\"args\\\": {\\\"text\\\": text,
      \\\"image\\\": URL, \\\"audio\\\": URL, \\\"video\\\": URL}}]. The \\\"dep\\\"
      field denotes the id of the previous task which generates a new resource that
      the current task relies on. A special tag \\\"-task_id\\\" refers to the generated
      text image, audio and video in the dependency task with id as task_id. The task
      MUST be selected from the following options: {{ Available Task List }}. There
      is a logical relationship between tasks, please note their order. If the user
      input can't be parsed, you need to reply empty JSON. Here are several cases
      for your reference: {{ Demonstrations }}. The chat history is recorded as {{
      Chat History }}. From this chat history, you can find the path of the user-mentioned
      resources for your task planning.\\n\\n(2) Model selection: LLM distributes
      the tasks to expert models, where the request is framed as a multiple-choice
      question. LLM is presented with a list of models to choose from. Due to the
      limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven
      the user request and the call command, the AI assistant helps the user to select
      a suitable model from a list of models to process the user request. The AI assistant
      merely outputs the model id of the most appropriate model. The output must be
      in a strict JSON format: \\\"id\\\": \\\"id\\\", \\\"reason\\\": \\\"your detail
      reason for the choice\\\". We have a list of models for you to choose from {{
      Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution:
      Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith
      the input and the inference results, the AI assistant needs to describe the
      process and results. The previous stages can be formed as - User Input: {{ User
      Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }},
      Task Execution: {{ Predictions }}. You must first answer the user's request
      in a straightforward manner. Then describe the task process and show your analysis
      and model inference results to the user in the first person. If inference results
      contain a file path, must tell the user the complete file path.\\n\\n(4) Response
      generation: LLM receives the execution results and provides summarized results
      to users.\\nTo put HuggingGPT into real world usage, a couple challenges need
      to solve: (1) Efficiency improvement is needed as both LLM inference rounds
      and interactions with other models slow down the process; (2) It relies on a
      long context window to communicate over complicated task content; (3) Stability
      improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al.
      2023) is a benchmark for evaluating the performance of tool-augmented LLMs.
      It contains 53 commonly used API tools, a complete tool-augmented LLM workflow,
      and 264 annotated dialogues that involve 568 API calls. The selection of APIs
      is quite diverse, including search engines, calculator, calendar queries, smart
      home control, schedule management, health data management, account authentication
      workflow and more. Because there are a large number of APIs, LLM first has access
      to API search engine to find the right API to call and then uses the corresponding
      documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an
      API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow,
      LLMs need to make a couple of decisions and at each step we can evaluate how
      accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify
      the right API to call: if not good enough, LLMs need to iteratively modify the
      API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse
      based on the API results: the model can choose to refine and call again if results
      are not satisfied.\\n\\nThis benchmark evaluates the agent\u2019s tool use capabilities
      at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an
      API\u2019s description, the model needs to determine whether to call a given
      API, call it correctly, and respond properly to API returns.\\nLevel-2 examines
      the ability to retrieve the API. The model needs to search for possible APIs
      that may solve the user\u2019s requirement and learn how to use them by reading
      documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and
      call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant
      for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase
      Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific
      example in which LLM is augmented with 13 expert-designed tools to accomplish
      tasks across organic synthesis, drug discovery, and materials design. The workflow,
      implemented in LangChain, reflects what was previously described in the ReAct
      and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe
      LLM is provided with a list of tool names, descriptions of their utility, and
      details about the expected input/output.\\nIt is then instructed to answer a
      user-given prompt using the tools provided when necessary. The instruction suggests
      the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested
      to \\\"develop a novel anticancer drug\\\", the model came up with the following
      reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected
      a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound
      was identified, the model attempted its synthesis.\\n\\nThey also discussed
      the risks, especially with illicit drugs and bioweapons. They developed a test
      set containing a list of known chemical weapon agents and asked the agent to
      synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis
      solution and the agent attempted to consult documentation to execute the procedure.
      7 out of 11 were rejected and among these 7 rejected cases, 5 happened after
      a Web search while 2 were rejected based on prompt only.\\nGenerative Agents
      Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment
      where 25 virtual characters, each controlled by a LLM-powered agent, are living
      and interacting in a sandbox environment, inspired by The Sims. Generative agents
      create believable simulacra of human behavior for interactive applications.\\nThe
      design of generative agents combines LLM with memory, planning and reflection
      mechanisms to enable agents to behave conditioned on past experience, as well
      as to interact with other agents.\\n\\nMemory stream: is a long-term memory
      module (external database) that records a comprehensive list of agents\u2019
      experience in natural language.\\n\\nEach element is an observation, an event
      directly provided by the agent.\\n- Inter-agent communication can trigger new
      natural language statements.\\n\\n\\nRetrieval model: surfaces the context to
      inform the agent\u2019s behavior, according to relevance, recency and importance.\\n\\nRecency:
      recent events have higher scores\\nImportance: distinguish mundane from core
      memories. Ask LM directly.\\nRelevance: based on how related it is to the current
      situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher
      level inferences over time and guides the agent\u2019s future behavior. They
      are higher-level summaries of past events (<- note that this is a bit different
      from self-reflection above)\\n\\nPrompt LM with 100 most recent observations
      and to generate 3 most salient high-level questions given a set of observations/statements.
      Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate
      the reflections and the environment information into actions\\n\\nPlanning is
      essentially in order to optimize believability at the moment vs in time.\\nPrompt
      template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept
      Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou
      are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must
      always be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1.
      {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5.
      ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short
      term memory is short, so immediately save important information to files.\\n2.
      If you are unsure how you previously did something or want to recall past events,
      thinking about similar events will help you remember.\\n3. No user assistance\\n4.
      Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\\n5.
      Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1.
      Google Search: \\\"google\\\", args: \\\"input\\\": \\\"<search>\\\"\\n2. Browse
      Website: \\\"browse_website\\\", args: \\\"url\\\": \\\"<url>\\\", \\\"question\\\":
      \\\"<what_you_want_to_find_on_website>\\\"\\n3. Start GPT Agent: \\\"start_agent\\\",
      args: \\\"name\\\": \\\"<name>\\\", \\\"task\\\": \\\"<short_task_desc>\\\",
      \\\"prompt\\\": \\\"<prompt>\\\"\\n4. Message GPT Agent: \\\"message_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\", \\\"message\\\": \\\"<message>\\\"\\n5. List
      GPT Agents: \\\"list_agents\\\", args:\\n6. Delete GPT Agent: \\\"delete_agent\\\",
      args: \\\"key\\\": \\\"<key>\\\"\\n7. Clone Repository: \\\"clone_repository\\\",
      args: \\\"repository_url\\\": \\\"<url>\\\", \\\"clone_path\\\": \\\"<directory>\\\"\\n8.
      Write to file: \\\"write_to_file\\\", args: \\\"file\\\": \\\"<file>\\\", \\\"text\\\":
      \\\"<text>\\\"\\n9. Read file: \\\"read_file\\\", args: \\\"file\\\": \\\"<file>\\\"\\n10.
      Append to file: \\\"append_to_file\\\", args: \\\"file\\\": \\\"<file>\\\",
      \\\"text\\\": \\\"<text>\\\"\\n11. Delete file: \\\"delete_file\\\", args: \\\"file\\\":
      \\\"<file>\\\"\\n12. Search Files: \\\"search_files\\\", args: \\\"directory\\\":
      \\\"<directory>\\\"\\n13. Analyze Code: \\\"analyze_code\\\", args: \\\"code\\\":
      \\\"<full_code_string>\\\"\\n14. Get Improved Code: \\\"improve_code\\\", args:
      \\\"suggestions\\\": \\\"<list_of_suggestions>\\\", \\\"code\\\": \\\"<full_code_string>\\\"\\n15.
      Write Tests: \\\"write_tests\\\", args: \\\"code\\\": \\\"<full_code_string>\\\",
      \\\"focus\\\": \\\"<list_of_focus_areas>\\\"\\n16. Execute Python File: \\\"execute_python_file\\\",
      args: \\\"file\\\": \\\"<file>\\\"\\n17. Generate Image: \\\"generate_image\\\",
      args: \\\"prompt\\\": \\\"<prompt>\\\"\\n18. Send Tweet: \\\"send_tweet\\\",
      args: \\\"text\\\": \\\"<text>\\\"\\n19. Do Nothing: \\\"do_nothing\\\", args:\\n20.
      Task Complete (Shutdown): \\\"task_complete\\\", args: \\\"reason\\\": \\\"<reason>\\\"\\n\\nResources:\\n1.
      Internet access for searches and information gathering.\\n2. Long Term memory
      management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4.
      File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze
      your actions to ensure you are performing to the best of your abilities.\\n2.
      Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect
      on past decisions and strategies to refine your approach.\\n4. Every command
      has a cost, so be smart and efficient. Aim to complete tasks in the least number
      of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse
      Format:\\n{\\n    \\\"thoughts\\\": {\\n        \\\"text\\\": \\\"thought\\\",\\n
      \       \\\"reasoning\\\": \\\"reasoning\\\",\\n        \\\"plan\\\": \\\"-
      short bulleted\\\\n- list that conveys\\\\n- long-term plan\\\",\\n        \\\"criticism\\\":
      \\\"constructive self-criticism\\\",\\n        \\\"speak\\\": \\\"thoughts summary
      to say to user\\\"\\n    },\\n    \\\"command\\\": {\\n        \\\"name\\\":
      \\\"command name\\\",\\n        \\\"args\\\": {\\n            \\\"arg name\\\":
      \\\"value\\\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by
      Python json.loads\\nGPT-Engineer is another project to create a whole repository
      of code given a task specified in natural language. The GPT-Engineer is instructed
      to think over a list of smaller components to build and ask for user input to
      clarify questions as needed.\\nHere are a sample conversation for task clarification
      sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs
      are wrapped in {{user input text}}.\\n[\\n  {\\n    \\\"role\\\": \\\"system\\\",\\n
      \   \\\"content\\\": \\\"You will read instructions and not carry them out,
      only seek to clarify them.\\\\nSpecifically you will first summarise a list
      of super short bullets of areas that need clarification.\\\\nThen you will pick
      one clarifying question, and wait for an answer from the user.\\\\n\\\"\\n  },\\n
      \ {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\": \\\"We are writing
      {{a Super Mario game in python. MVC components split in separate files. Keyboard
      control.}}\\\\n\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n    \\\"content\\\":
      \\\"Summary of areas that need clarification:\\\\n1. Specifics of the Super
      Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details
      about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard
      control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying
      question:\\\\nCan you provide more details about the Super Mario game, such
      as level design, characters, and gameplay mechanics?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"{{There are 10 levels in total. The
      main character is a plumber named Mario, who can walk and jump. It is a classical
      platform game just like Super Mario. The main character moves from left to right,
      trying to get to the destination, where there are many obstacles and attacks
      from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only
      answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next
      question}\\\\nIf everything is sufficiently clear, only answer \\\\\\\"Nothing
      more to clarify.\\\\\\\".\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Remaining unclear areas: 2 remaining questions.\\\\nCan
      you provide more information about how the MVC components are split into separate
      files?\\\"\\n  },\\n  {\\n    \\\"role\\\": \\\"user\\\",\\n    \\\"content\\\":
      \\\"{{Make your own assumptions and state them explicitly before starting}}\\\"\\n
      \ }\\n]\\nThen after these clarification, the agent moved into the code writing
      mode with a different system message.\\nSystem message:\\n\\nYou will get instructions
      for code to write.\\nYou will write a very long answer. Make sure that every
      detail of the architecture is, in the end, implemented as code.\\nMake sure
      that every detail of the architecture is, in the end, implemented as code.\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\nThen
      you will output the content of each file including ALL code.\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\nFILENAME is the lowercase file name including the file
      extension,\\nLANG is the markup code block language for the code\u2019s language,
      and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the \u201Centrypoint\u201D
      file, then go to the ones that are imported by that file, and so on.\\nPlease
      note that the code should be fully functional. No placeholders.\\nFollow a language
      and framework appropriate best practice file naming convention.\\nMake sure
      that files contain all imports, types etc. Make sure that code in different
      files are compatible with each other.\\nEnsure to implement all code, if you
      are unsure, write a plausible implementation.\\nInclude module dependency or
      package manager dependency definition file.\\nBefore you finish, double check
      that all parts of the architecture is present in the files.\\nUseful to know:\\nYou
      almost always put different classes in different files.\\nFor Python, you always
      create an appropriate requirements.txt file.\\nFor NodeJS, you always create
      an appropriate package.json file.\\nYou always add a comment briefly describing
      the purpose of the function definition.\\nYou try to add comments explaining
      very complex bits of logic.\\nYou always follow the best practices for the requested
      languages in terms of describing the code written as a defined\\npackage/project.\\nPython
      toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n
      \ {\\n    \\\"role\\\": \\\"system\\\",\\n    \\\"content\\\": \\\"You will
      get instructions for code to write.\\\\nYou will write a very long answer. Make
      sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake
      sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nYou will first lay out the names of the core classes, functions,
      methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nPlease note that the code should be
      fully functional. No placeholders.\\\\n\\\\nFollow a language and framework
      appropriate best practice file naming convention.\\\\nMake sure that files contain
      all imports, types etc. Make sure that code in different files are compatible
      with each other.\\\\nEnsure to implement all code, if you are unsure, write
      a plausible implementation.\\\\nInclude module dependency or package manager
      dependency definition file.\\\\nBefore you finish, double check that all parts
      of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou
      almost always put different classes in different files.\\\\nFor Python, you
      always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always
      create an appropriate package.json file.\\\\nYou always add a comment briefly
      describing the purpose of the function definition.\\\\nYou try to add comments
      explaining very complex bits of logic.\\\\nYou always follow the best practices
      for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython
      toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\\\"\\n  },\\n #  \u2026
      same conversation as earlier, ended with \\\"Make your own assumptions and state
      them explicitly before starting\\\".\\n  {\\n    \\\"role\\\": \\\"assistant\\\",\\n
      \   \\\"content\\\": \\\"Assumptions:\\\\n1. Model: The model will contain the
      game's data, such as level information, character states, and enemy positions.\\\\n2.
      View: The view will handle the game's visuals, including rendering the game
      objects, backgrounds, and updating the display.\\\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\\\"\\n  },\\n  {\\n    \\\"role\\\":
      \\\"user\\\",\\n    \\\"content\\\": \\\"Please now remember the steps:\\\\n\\\\nThink
      step by step and reason yourself to the right decisions to make sure we get
      it right.\\\\nFirst lay out the names of the core classes, functions, methods
      that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen
      you will output the content of each file including ALL code.\\\\nEach file must
      strictly follow a markdown code block format, where the following tokens must
      be replaced such that\\\\nFILENAME is the lowercase file name including the
      file extension,\\\\nLANG is the markup code block language for the code's language,
      and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease
      note that the code should be fully functional. No placeholders.\\\\n\\\\nYou
      will start with the \\\\\\\"entrypoint\\\\\\\" file, then go to the ones that
      are imported by that file, and so on.\\\\nFollow a language and framework appropriate
      best practice file naming convention.\\\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\\\n\\\"\\n  }\\n]\\nChallenges#\\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\\n\\n\\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\\n\\n\\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited
      as:\\n\\nWeng, Lilian. (Jun 2023). \u201CLLM-powered Autonomous Agents\u201D.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n
      \ title   = \\\"LLM-powered Autonomous Agents\\\",\\n  author  = \\\"Weng, Lilian\\\",\\n
      \ journal = \\\"lilianweng.github.io\\\",\\n  year    = \\\"2023\\\",\\n  month
      \  = \\\"Jun\\\",\\n  url     = \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\"\\n}\\nReferences#\\n[1]
      Wei et al. \u201CChain of thought prompting elicits reasoning in large language
      models.\u201D NeurIPS 2022\\n[2] Yao et al. \u201CTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201D arXiv preprint arXiv:2305.10601
      (2023).\\n[3] Liu et al. \u201CChain of Hindsight Aligns Language Models with
      Feedback\\n\u201C arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. \u201CLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201D arXiv
      preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. \u201CReAct: Synergizing
      reasoning and acting in language models.\u201D ICLR 2023.\\n[6] Google Blog.
      \u201CAnnouncing ScaNN: Efficient Vector Similarity Search\u201D July 28, 2020.\\n[7]
      https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn
      & Labash. \u201CReflexion: an autonomous agent with dynamic memory and self-reflection\u201D
      arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. \u201CIn-context
      Reinforcement Learning with Algorithm Distillation\u201D ICLR 2023.\\n[10] Karpas
      et al. \u201CMRKL Systems A modular, neuro-symbolic architecture that combines
      large language models, external knowledge sources and discrete reasoning.\u201D
      arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. \u201CWebgpt: Browser-assisted
      question-answering with human feedback.\u201D arXiv preprint arXiv:2112.09332
      (2021).\\n[12] Parisi et al. \u201CTALM: Tool Augmented Language Models\u201D\\n[13]
      Schick et al. \u201CToolformer: Language Models Can Teach Themselves to Use
      Tools.\u201D arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why
      is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. \u201CAPI-Bank: A Benchmark
      for Tool-Augmented LLMs\u201D arXiv preprint arXiv:2304.08244 (2023).\\n[16]
      Shen et al. \u201CHuggingGPT: Solving AI Tasks with ChatGPT and its Friends
      in HuggingFace\u201D arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et
      al. \u201CChemCrow: Augmenting large-language models with chemistry tools.\u201D
      arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. \u201CEmergent autonomous
      scientific research capabilities of large language models.\u201D arXiv preprint
      arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. \u201CGenerative Agents:
      Interactive Simulacra of Human Behavior.\u201D arXiv preprint arXiv:2304.03442
      (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n\xAB
      \\n\\nAdversarial Attacks on LLMs\\n\\n\\n \xBB\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\xA9
      2024 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"type\":\"system\"}}]]},\"run_type\":\"llm\"}]}"
    headers:
      Accept:
      - application/json
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '461289'
      Content-Type:
      - application/json
      User-Agent:
      - langsmith-py/0.1.128
    method: POST
    uri: https://api.smith.langchain.com/runs/batch
  response:
    body:
      string: '{"detail":"Forbidden"}'
    headers:
      Access-Control-Allow-Credentials:
      - 'true'
      Access-Control-Allow-Headers:
      - '*'
      Access-Control-Allow-Methods:
      - '*'
      Access-Control-Allow-Origin:
      - ''
      Access-Control-Expose-Headers:
      - '*'
      Access-Control-Max-Age:
      - '600'
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      Connection:
      - close
      Content-Length:
      - '22'
      Via:
      - 1.1 google
      content-type:
      - application/json
      date:
      - Wed, 25 Sep 2024 22:31:14 GMT
      server:
      - uvicorn
    status:
      code: 403
      message: Forbidden
- request:
    body: '{"messages": [{"content": "Write a concise summary of the following:\\n\\n\n\n\n\n\n\nLLM
      Powered Autonomous Agents | Lil''Log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLil''Log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\n\n\nArchive\n\n\n\n\nSearch\n\n\n\n\nTags\n\n\n\n\nFAQ\n\n\n\n\nemojisearch.app\n\n\n\n\n\n\n\n\n\n      LLM
      Powered Autonomous Agents\n    \nDate: June 23, 2023  |  Estimated Reading Time:
      31 min  |  Author: Lilian Weng\n\n\n \n\n\nTable of Contents\n\n\n\nAgent System
      Overview\n\nComponent One: Planning\n\nTask Decomposition\n\nSelf-Reflection\n\n\nComponent
      Two: Memory\n\nTypes of Memory\n\nMaximum Inner Product Search (MIPS)\n\n\nComponent
      Three: Tool Use\n\nCase Studies\n\nScientific Discovery Agent\n\nGenerative
      Agents Simulation\n\nProof-of-Concept Examples\n\n\nChallenges\n\nCitation\n\nReferences\n\n\n\n\n\nBuilding
      agents with LLM (large language model) as its core controller is a cool concept.
      Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI,
      serve as inspiring examples. The potentiality of LLM extends beyond generating
      well-written copies, stories, essays and programs; it can be framed as a powerful
      general problem solver.\nAgent System Overview#\nIn a LLM-powered autonomous
      agent system, LLM functions as the agent\u2019s brain, complemented by several
      key components:\n\nPlanning\n\nSubgoal and decomposition: The agent breaks down
      large tasks into smaller, manageable subgoals, enabling efficient handling of
      complex tasks.\nReflection and refinement: The agent can do self-criticism and
      self-reflection over past actions, learn from mistakes and refine them for future
      steps, thereby improving the quality of final results.\n\n\nMemory\n\nShort-term
      memory: I would consider all the in-context learning (See Prompt Engineering)
      as utilizing short-term memory of the model to learn.\nLong-term memory: This
      provides the agent with the capability to retain and recall (infinite) information
      over extended periods, often by leveraging an external vector store and fast
      retrieval.\n\n\nTool use\n\nThe agent learns to call external APIs for extra
      information that is missing from the model weights (often hard to change after
      pre-training), including current information, code execution capability, access
      to proprietary information sources and more.\n\n\n\n\nFig. 1. Overview of a
      LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated
      task usually involves many steps. An agent needs to know what they are and plan
      ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become
      a standard prompting technique for enhancing model performance on complex tasks.
      The model is instructed to \u201cthink step by step\u201d to utilize more test-time
      computation to decompose hard tasks into smaller and simpler steps. CoT transforms
      big tasks into multiple manageable tasks and shed lights into an interpretation
      of the model\u2019s thinking process.\nTree of Thoughts (Yao et al. 2023) extends
      CoT by exploring multiple reasoning possibilities at each step. It first decomposes
      the problem into multiple thought steps and generates multiple thoughts per
      step, creating a tree structure. The search process can be BFS (breadth-first
      search) or DFS (depth-first search) with each state evaluated by a classifier
      (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM
      with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals
      for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write
      a story outline.\" for writing a novel, or (3) with human inputs.\nAnother quite
      distinct approach, LLM+P (Liu et al. 2023), involves relying on an external
      classical planner to do long-horizon planning. This approach utilizes the Planning
      Domain Definition Language (PDDL) as an intermediate interface to describe the
      planning problem. In this process, LLM (1) translates the problem into \u201cProblem
      PDDL\u201d, then (2) requests a classical planner to generate a PDDL plan based
      on an existing \u201cDomain PDDL\u201d, and finally (3) translates the PDDL
      plan back into natural language. Essentially, the planning step is outsourced
      to an external tool, assuming the availability of domain-specific PDDL and a
      suitable planner which is common in certain robotic setups but not in many other
      domains.\nSelf-Reflection#\nSelf-reflection is a vital aspect that allows autonomous
      agents to improve iteratively by refining past action decisions and correcting
      previous mistakes. It plays a crucial role in real-world tasks where trial and
      error are inevitable.\nReAct (Yao et al. 2023) integrates reasoning and acting
      within LLM by extending the action space to be a combination of task-specific
      discrete actions and the language space. The former enables LLM to interact
      with the environment (e.g. use Wikipedia search API), while the latter prompting
      LLM to generate reasoning traces in natural language.\nThe ReAct prompt template
      incorporates explicit steps for LLM to think, roughly formatted as:\nThought:
      ...\nAction: ...\nObservation: ...\n... (Repeated many times)\n\nFig. 2.  Examples
      of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER)
      and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et
      al. 2023).\nIn both experiments on knowledge-intensive tasks and decision-making
      tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step
      is removed.\nReflexion (Shinn & Labash 2023) is a framework to equips agents
      with dynamic memory and self-reflection capabilities to improve reasoning skills.
      Reflexion has a standard RL setup, in which the reward model provides a simple
      binary reward and the action space follows the setup in ReAct where the task-specific
      action space is augmented with language to enable complex reasoning steps. After
      each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide
      to reset the environment to start a new trial depending on the self-reflection
      results.\n\nFig. 3. Illustration of the Reflexion framework. (Image source:
      Shinn & Labash, 2023)\nThe heuristic function determines when the trajectory
      is inefficient or contains hallucination and should be stopped. Inefficient
      planning refers to trajectories that take too long without success. Hallucination
      is defined as encountering a sequence of consecutive identical actions that
      lead to the same observation in the environment.\nSelf-reflection is created
      by showing two-shot examples to LLM and each example is a pair of (failed trajectory,
      ideal reflection for guiding future changes in the plan). Then reflections are
      added into the agent\u2019s working memory, up to three, to be used as context
      for querying LLM.\n\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination
      is a more common failure than inefficient planning in AlfWorld. (Image source:
      Shinn & Labash, 2023)\nChain of Hindsight (CoH; Liu et al. 2023) encourages
      the model to improve on its own outputs by explicitly presenting it with a sequence
      of past outputs, each annotated with feedback. Human feedback data is a collection
      of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each
      $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is
      the corresponding human-provided hindsight feedback. Assume the feedback tuples
      are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process
      is supervised fine-tuning where the data is a sequence in the form of $\\tau_h
      = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$.
      The model is finetuned to only predict $y_n$ where conditioned on the sequence
      prefix, such that the model can self-reflect to produce better output based
      on the feedback sequence. The model can optionally receive multiple rounds of
      instructions with human annotators at test time.\nTo avoid overfitting, CoH
      adds a regularization term to maximize the log-likelihood of the pre-training
      dataset. To avoid shortcutting and copying (because there are many common words
      in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\nThe
      training dataset in their experiments is a combination of WebGPT comparisons,
      summarization from human feedback and human preference dataset.\n\nFig. 5. After
      fine-tuning with CoH, the model can follow instructions to produce outputs with
      incremental improvement in a sequence. (Image source: Liu et al. 2023)\nThe
      idea of CoH is to present a history of sequentially improved outputs  in context
      and train the model to take on the trend to produce better outputs. Algorithm
      Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode
      trajectories in reinforcement learning tasks, where an algorithm is encapsulated
      in a long history-conditioned policy. Considering that an agent interacts with
      the environment many times and in each episode the agent gets a little better,
      AD concatenates this learning history and feeds that into the model. Hence we
      should expect the next predicted action to lead to better performance than previous
      trials. The goal is to learn the process of RL instead of training a task-specific
      policy itself.\n\nFig. 6. Illustration of how Algorithm Distillation (AD) works.
      (Image source: Laskin et al. 2023).\nThe paper hypothesizes that any algorithm
      that generates a set of learning histories can be distilled into a neural network
      by performing behavioral cloning over actions. The history data is generated
      by a set of source policies, each trained for a specific task. At the training
      stage, during each RL run, a random task is sampled and a subsequence of multi-episode
      history is used for training, such that the learned policy is task-agnostic.\nIn
      reality, the model has limited context window length, so episodes should be
      short enough to construct multi-episode history. Multi-episodic contexts of
      2-4 episodes are necessary to learn a near-optimal in-context RL algorithm.
      The emergence of in-context RL requires long enough context.\nIn comparison
      with three baselines, including ED (expert distillation, behavior cloning with
      expert trajectories instead of learning history), source policy (used for generating
      trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper
      bound since it needs online RL), AD demonstrates in-context RL with performance
      getting close to RL^2 despite only using offline RL and learns much faster than
      other baselines. When conditioned on partial training history of the source
      policy, AD also improves much faster than ED baseline.\n\nFig. 7. Comparison
      of AD, ED, source policy and RL^2 on environments that require memory and exploration.
      Only binary reward is assigned. The source policies are trained with A3C for
      \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\nComponent
      Two: Memory#\n(Big thank you to ChatGPT for helping me draft this section. I\u2019ve
      learned a lot about the human brain and data structure for fast MIPS in my conversations
      with ChatGPT.)\nTypes of Memory#\nMemory can be defined as the processes used
      to acquire, store, retain, and later retrieve information. There are several
      types of memory in human brains.\n\n\nSensory Memory: This is the earliest stage
      of memory, providing the ability to retain impressions of sensory information
      (visual, auditory, etc) after the original stimuli have ended. Sensory memory
      typically only lasts for up to a few seconds. Subcategories include iconic memory
      (visual), echoic memory (auditory), and haptic memory (touch).\n\n\nShort-Term
      Memory (STM) or Working Memory: It stores information that we are currently
      aware of and needed to carry out complex cognitive tasks such as learning and
      reasoning. Short-term memory is believed to have the capacity of about 7 items
      (Miller 1956) and lasts for 20-30 seconds.\n\n\nLong-Term Memory (LTM): Long-term
      memory can store information for a remarkably long time, ranging from a few
      days to decades, with an essentially unlimited storage capacity. There are two
      subtypes of LTM:\n\nExplicit / declarative memory: This is memory of facts and
      events, and refers to those memories that can be consciously recalled, including
      episodic memory (events and experiences) and semantic memory (facts and concepts).\nImplicit
      / procedural memory: This type of memory is unconscious and involves skills
      and routines that are performed automatically, like riding a bike or typing
      on a keyboard.\n\n\n\n\nFig. 8. Categorization of human memory.\nWe can roughly
      consider the following mappings:\n\nSensory memory as learning embedding representations
      for raw inputs, including text, image or other modalities;\nShort-term memory
      as in-context learning. It is short and finite, as it is restricted by the finite
      context window length of Transformer.\nLong-term memory as the external vector
      store that the agent can attend to at query time, accessible via fast retrieval.\n\nMaximum
      Inner Product Search (MIPS)#\nThe external memory can alleviate the restriction
      of finite attention span.  A standard practice is to save the embedding representation
      of information into a vector store database that can support fast maximum inner-product
      search (MIPS). To optimize the retrieval speed, the common choice is the approximate
      nearest neighbors (ANN)\u200b algorithm to return approximately top k nearest
      neighbors to trade off a little accuracy lost for a huge speedup.\nA couple
      common choices of ANN algorithms for fast MIPS:\n\nLSH (Locality-Sensitive Hashing):
      It introduces a hashing function such that similar input items are mapped to
      the same buckets with high probability, where the number of buckets is much
      smaller than the number of inputs.\nANNOY (Approximate Nearest Neighbors Oh
      Yeah): The core data structure are random projection trees, a set of binary
      trees where each non-leaf node represents a hyperplane splitting the input space
      into half and each leaf stores one data point. Trees are built independently
      and at random, so to some extent, it mimics a hashing function. ANNOY search
      happens in all the trees to iteratively search through the half that is closest
      to the query and then aggregates the results. The idea is quite related to KD
      tree but a lot more scalable.\nHNSW (Hierarchical Navigable Small World): It
      is inspired by the idea of small world networks where most nodes can be reached
      by any other nodes within a small number of steps; e.g. \u201csix degrees of
      separation\u201d feature of social networks. HNSW builds hierarchical layers
      of these small-world graphs, where the bottom layers contain the actual data
      points. The layers in the middle create shortcuts to speed up search. When performing
      a search, HNSW starts from a random node in the top layer and navigates towards
      the target. When it can\u2019t get any closer, it moves down to the next layer,
      until it reaches the bottom layer. Each move in the upper layers can potentially
      cover a large distance in the data space, and each move in the lower layers
      refines the search quality.\nFAISS (Facebook AI Similarity Search): It operates
      on the assumption that in high dimensional space, distances between nodes follow
      a Gaussian distribution and thus there should exist clustering of data points.
      FAISS applies vector quantization by partitioning the vector space into clusters
      and then refining the quantization within clusters. Search first looks for cluster
      candidates with coarse quantization and then further looks into each cluster
      with finer quantization.\nScaNN (Scalable Nearest Neighbors): The main innovation
      in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$
      to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is
      as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible,
      instead of picking the closet quantization centroid points.\n\n\nFig. 9. Comparison
      of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\nCheck
      more MIPS algorithms and performance comparison in ann-benchmarks.com.\nComponent
      Three: Tool Use#\nTool use is a remarkable and distinguishing characteristic
      of human beings. We create, modify and utilize external objects to do things
      that go beyond our physical and cognitive limits. Equipping LLMs with external
      tools can significantly extend the model capabilities.\n\nFig. 10. A picture
      of a sea otter using rock to crack open a seashell, while floating in the water.
      While some other animals can use tools, the complexity is not comparable with
      humans. (Image source: Animals using tools)\nMRKL (Karpas et al. 2022), short
      for \u201cModular Reasoning, Knowledge and Language\u201d, is a neuro-symbolic
      architecture for autonomous agents. A MRKL system is proposed to contain a collection
      of \u201cexpert\u201d modules and the general-purpose LLM works as a router
      to route inquiries to the best suitable expert module. These modules can be
      neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency
      converter, weather API).\nThey did an experiment on fine-tuning LLM to call
      a calculator, using arithmetic as a test case. Their experiments showed that
      it was harder to solve verbal math problems than explicitly stated math problems
      because LLMs (7B Jurassic1-large model) failed to extract the right arguments
      for the basic arithmetic reliably. The results highlight when the external symbolic
      tools can work reliably, knowing when to and how to use the tools are crucial,
      determined by the LLM capability.\nBoth TALM (Tool Augmented Language Models;
      Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn
      to use external tool APIs. The dataset is expanded based on whether a newly
      added API call annotation can improve the quality of model outputs. See more
      details in the \u201cExternal APIs\u201d section of Prompt Engineering.\nChatGPT
      Plugins and OpenAI API  function calling are good examples of LLMs augmented
      with tool use capability working in practice. The collection of tool APIs can
      be provided by other developers (as in Plugins) or self-defined (as in function
      calls).\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the
      task planner to select models available in HuggingFace platform according to
      the model descriptions and summarize the response based on the execution results.\n\nFig.
      11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\nThe
      system comprises of 4 stages:\n(1) Task planning: LLM works as the brain and
      parses the user requests into multiple tasks. There are four attributes associated
      with each task: task type, ID, dependencies, and arguments. They use few-shot
      examples to guide LLM to do task parsing and planning.\nInstruction:\n\nThe
      AI assistant can parse user input to several tasks: [{\"task\": task, \"id\",
      task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\":
      URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of
      the previous task which generates a new resource that the current task relies
      on. A special tag \"-task_id\" refers to the generated text image, audio and
      video in the dependency task with id as task_id. The task MUST be selected from
      the following options: {{ Available Task List }}. There is a logical relationship
      between tasks, please note their order. If the user input can''t be parsed,
      you need to reply empty JSON. Here are several cases for your reference: {{
      Demonstrations }}. The chat history is recorded as {{ Chat History }}. From
      this chat history, you can find the path of the user-mentioned resources for
      your task planning.\n\n(2) Model selection: LLM distributes the tasks to expert
      models, where the request is framed as a multiple-choice question. LLM is presented
      with a list of models to choose from. Due to the limited context length, task
      type based filtration is needed.\nInstruction:\n\nGiven the user request and
      the call command, the AI assistant helps the user to select a suitable model
      from a list of models to process the user request. The AI assistant merely outputs
      the model id of the most appropriate model. The output must be in a strict JSON
      format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We
      have a list of models for you to choose from {{ Candidate Models }}. Please
      select one model from the list.\n\n(3) Task execution: Expert models execute
      on the specific tasks and log results.\nInstruction:\n\nWith the input and the
      inference results, the AI assistant needs to describe the process and results.
      The previous stages can be formed as - User Input: {{ User Input }}, Task Planning:
      {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions
      }}. You must first answer the user''s request in a straightforward manner. Then
      describe the task process and show your analysis and model inference results
      to the user in the first person. If inference results contain a file path, must
      tell the user the complete file path.\n\n(4) Response generation: LLM receives
      the execution results and provides summarized results to users.\nTo put HuggingGPT
      into real world usage, a couple challenges need to solve: (1) Efficiency improvement
      is needed as both LLM inference rounds and interactions with other models slow
      down the process; (2) It relies on a long context window to communicate over
      complicated task content; (3) Stability improvement of LLM outputs and external
      model services.\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the
      performance of tool-augmented LLMs. It contains 53 commonly used API tools,
      a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve
      568 API calls. The selection of APIs is quite diverse, including search engines,
      calculator, calendar queries, smart home control, schedule management, health
      data management, account authentication workflow and more. Because there are
      a large number of APIs, LLM first has access to API search engine to find the
      right API to call and then uses the corresponding documentation to make a call.\n\nFig.
      12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li
      et al. 2023)\nIn the API-Bank workflow, LLMs need to make a couple of decisions
      and at each step we can evaluate how accurate that decision is. Decisions include:\n\nWhether
      an API call is needed.\nIdentify the right API to call: if not good enough,
      LLMs need to iteratively modify the API inputs (e.g. deciding search keywords
      for Search Engine API).\nResponse based on the API results: the model can choose
      to refine and call again if results are not satisfied.\n\nThis benchmark evaluates
      the agent\u2019s tool use capabilities at three levels:\n\nLevel-1 evaluates
      the ability to call the API. Given an API\u2019s description, the model needs
      to determine whether to call a given API, call it correctly, and respond properly
      to API returns.\nLevel-2 examines the ability to retrieve the API. The model
      needs to search for possible APIs that may solve the user\u2019s requirement
      and learn how to use them by reading documentation.\nLevel-3 assesses the ability
      to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule
      group meetings, book flight/hotel/restaurant for a trip), the model may have
      to conduct multiple API calls to solve it.\n\nCase Studies#\nScientific Discovery
      Agent#\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM
      is augmented with 13 expert-designed tools to accomplish tasks across organic
      synthesis, drug discovery, and materials design. The workflow, implemented in
      LangChain, reflects what was previously described in the ReAct and MRKLs and
      combines CoT reasoning with tools relevant to the tasks:\n\nThe LLM is provided
      with a list of tool names, descriptions of their utility, and details about
      the expected input/output.\nIt is then instructed to answer a user-given prompt
      using the tools provided when necessary. The instruction suggests the model
      to follow the ReAct format - Thought, Action, Action Input, Observation.\n\nOne
      interesting observation is that while the LLM-based evaluation concluded that
      GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts
      oriented towards the completion and chemical correctness of the solutions showed
      that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential
      problem with using LLM to evaluate its own performance on domains that requires
      deep expertise. The lack of expertise may cause LLMs not knowing its flaws and
      thus cannot well judge the correctness of task results.\nBoiko et al. (2023)
      also looked into LLM-empowered agents for scientific discovery, to handle autonomous
      design, planning, and performance of complex scientific experiments. This agent
      can use tools to browse the Internet, read documentation, execute code, call
      robotics experimentation APIs and leverage other LLMs.\nFor example, when requested
      to \"develop a novel anticancer drug\", the model came up with the following
      reasoning steps:\n\ninquired about current trends in anticancer drug discovery;\nselected
      a target;\nrequested a scaffold targeting these compounds;\nOnce the compound
      was identified, the model attempted its synthesis.\n\nThey also discussed the
      risks, especially with illicit drugs and bioweapons. They developed a test set
      containing a list of known chemical weapon agents and asked the agent to synthesize
      them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution
      and the agent attempted to consult documentation to execute the procedure. 7
      out of 11 were rejected and among these 7 rejected cases, 5 happened after a
      Web search while 2 were rejected based on prompt only.\nGenerative Agents Simulation#\nGenerative
      Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters,
      each controlled by a LLM-powered agent, are living and interacting in a sandbox
      environment, inspired by The Sims. Generative agents create believable simulacra
      of human behavior for interactive applications.\nThe design of generative agents
      combines LLM with memory, planning and reflection mechanisms to enable agents
      to behave conditioned on past experience, as well as to interact with other
      agents.\n\nMemory stream: is a long-term memory module (external database) that
      records a comprehensive list of agents\u2019 experience in natural language.\n\nEach
      element is an observation, an event directly provided by the agent.\n- Inter-agent
      communication can trigger new natural language statements.\n\n\nRetrieval model:
      surfaces the context to inform the agent\u2019s behavior, according to relevance,
      recency and importance.\n\nRecency: recent events have higher scores\nImportance:
      distinguish mundane from core memories. Ask LM directly.\nRelevance: based on
      how related it is to the current situation / query.\n\n\nReflection mechanism:
      synthesizes memories into higher level inferences over time and guides the agent\u2019s
      future behavior. They are higher-level summaries of past events (<- note that
      this is a bit different from self-reflection above)\n\nPrompt LM with 100 most
      recent observations and to generate 3 most salient high-level questions given
      a set of observations/statements. Then ask LM to answer those questions.\n\n\nPlanning
      & Reacting: translate the reflections and the environment information into actions\n\nPlanning
      is essentially in order to optimize believability at the moment vs in time.\nPrompt
      template: {Intro of an agent X}. Here is X''s plan today in broad strokes: 1)\nRelationships
      between agents and observations of one agent by another are all taken into consideration
      for planning and reacting.\nEnvironment information is present in a tree structure.\n\n\n\n\nFig.
      13. The generative agent architecture. (Image source: Park et al. 2023)\nThis
      fun simulation results in emergent social behavior, such as information diffusion,
      relationship memory (e.g. two agents continuing the conversation topic) and
      coordination of social events (e.g. host a party and invite many others).\nProof-of-Concept
      Examples#\nAutoGPT has drawn a lot of attention into the possibility of setting
      up autonomous agents with LLM as the main controller. It has quite a lot of
      reliability issues given the natural language interface, but nevertheless a
      cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\nHere
      is the system message used by AutoGPT, where {{...}} are user inputs:\nYou are
      {{ai-name}}, {{user-provided AI bot description}}.\nYour decisions must always
      be made independently without seeking user assistance. Play to your strengths
      as an LLM and pursue simple strategies with no legal complications.\n\nGOALS:\n\n1.
      {{user-provided goal 1}}\n2. {{user-provided goal 2}}\n3. ...\n4. ...\n5. ...\n\nConstraints:\n1.
      ~4000 word limit for short term memory. Your short term memory is short, so
      immediately save important information to files.\n2. If you are unsure how you
      previously did something or want to recall past events, thinking about similar
      events will help you remember.\n3. No user assistance\n4. Exclusively use the
      commands listed in double quotes e.g. \"command name\"\n5. Use subprocesses
      for commands that will not terminate within a few minutes\n\nCommands:\n1. Google
      Search: \"google\", args: \"input\": \"<search>\"\n2. Browse Website: \"browse_website\",
      args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n3.
      Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\",
      \"prompt\": \"<prompt>\"\n4. Message GPT Agent: \"message_agent\", args: \"key\":
      \"<key>\", \"message\": \"<message>\"\n5. List GPT Agents: \"list_agents\",
      args:\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n7. Clone
      Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\":
      \"<directory>\"\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\",
      \"text\": \"<text>\"\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\n10.
      Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n11.
      Delete file: \"delete_file\", args: \"file\": \"<file>\"\n12. Search Files:
      \"search_files\", args: \"directory\": \"<directory>\"\n13. Analyze Code: \"analyze_code\",
      args: \"code\": \"<full_code_string>\"\n14. Get Improved Code: \"improve_code\",
      args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n15.
      Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\":
      \"<list_of_focus_areas>\"\n16. Execute Python File: \"execute_python_file\",
      args: \"file\": \"<file>\"\n17. Generate Image: \"generate_image\", args: \"prompt\":
      \"<prompt>\"\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n19.
      Do Nothing: \"do_nothing\", args:\n20. Task Complete (Shutdown): \"task_complete\",
      args: \"reason\": \"<reason>\"\n\nResources:\n1. Internet access for searches
      and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered
      Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1.
      Continuously review and analyze your actions to ensure you are performing to
      the best of your abilities.\n2. Constructively self-criticize your big-picture
      behavior constantly.\n3. Reflect on past decisions and strategies to refine
      your approach.\n4. Every command has a cost, so be smart and efficient. Aim
      to complete tasks in the least number of steps.\n\nYou should only respond in
      JSON format as described below\nResponse Format:\n{\n    \"thoughts\": {\n        \"text\":
      \"thought\",\n        \"reasoning\": \"reasoning\",\n        \"plan\": \"- short
      bulleted\\n- list that conveys\\n- long-term plan\",\n        \"criticism\":
      \"constructive self-criticism\",\n        \"speak\": \"thoughts summary to say
      to user\"\n    },\n    \"command\": {\n        \"name\": \"command name\",\n        \"args\":
      {\n            \"arg name\": \"value\"\n        }\n    }\n}\nEnsure the response
      can be parsed by Python json.loads\nGPT-Engineer is another project to create
      a whole repository of code given a task specified in natural language. The GPT-Engineer
      is instructed to think over a list of smaller components to build and ask for
      user input to clarify questions as needed.\nHere are a sample conversation for
      task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer.
      The user inputs are wrapped in {{user input text}}.\n[\n  {\n    \"role\": \"system\",\n    \"content\":
      \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically
      you will first summarise a list of super short bullets of areas that need clarification.\\nThen
      you will pick one clarifying question, and wait for an answer from the user.\\n\"\n  },\n  {\n    \"role\":
      \"user\",\n    \"content\": \"We are writing {{a Super Mario game in python.
      MVC components split in separate files. Keyboard control.}}\\n\"\n  },\n  {\n    \"role\":
      \"assistant\",\n    \"content\": \"Summary of areas that need clarification:\\n1.
      Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2.
      Details about the MVC components (e.g. which components are in each file)\\n3.
      Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying
      question:\\nCan you provide more details about the Super Mario game, such as
      level design, characters, and gameplay mechanics?\"\n  },\n  {\n    \"role\":
      \"user\",\n    \"content\": \"{{There are 10 levels in total. The main character
      is a plumber named Mario, who can walk and jump. It is a classical platform
      game just like Super Mario. The main character moves from left to right, trying
      to get to the destination, where there are many obstacles and attacks from enemies
      in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the
      form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf
      everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n  },\n  {\n    \"role\":
      \"assistant\",\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan
      you provide more information about how the MVC components are split into separate
      files?\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"{{Make your
      own assumptions and state them explicitly before starting}}\"\n  }\n]\nThen
      after these clarification, the agent moved into the code writing mode with a
      different system message.\nSystem message:\n\nYou will get instructions for
      code to write.\nYou will write a very long answer. Make sure that every detail
      of the architecture is, in the end, implemented as code.\nMake sure that every
      detail of the architecture is, in the end, implemented as code.\nThink step
      by step and reason yourself to the right decisions to make sure we get it right.\nYou
      will first lay out the names of the core classes, functions, methods that will
      be necessary, as well as a quick comment on their purpose.\nThen you will output
      the content of each file including ALL code.\nEach file must strictly follow
      a markdown code block format, where the following tokens must be replaced such
      that\nFILENAME is the lowercase file name including the file extension,\nLANG
      is the markup code block language for the code\u2019s language, and CODE is
      the code:\nFILENAME\nCODE\nYou will start with the \u201centrypoint\u201d file,
      then go to the ones that are imported by that file, and so on.\nPlease note
      that the code should be fully functional. No placeholders.\nFollow a language
      and framework appropriate best practice file naming convention.\nMake sure that
      files contain all imports, types etc. Make sure that code in different files
      are compatible with each other.\nEnsure to implement all code, if you are unsure,
      write a plausible implementation.\nInclude module dependency or package manager
      dependency definition file.\nBefore you finish, double check that all parts
      of the architecture is present in the files.\nUseful to know:\nYou almost always
      put different classes in different files.\nFor Python, you always create an
      appropriate requirements.txt file.\nFor NodeJS, you always create an appropriate
      package.json file.\nYou always add a comment briefly describing the purpose
      of the function definition.\nYou try to add comments explaining very complex
      bits of logic.\nYou always follow the best practices for the requested languages
      in terms of describing the code written as a defined\npackage/project.\nPython
      toolbelt preferences:\n\npytest\ndataclasses\n\n\nConversatin samples:\n[\n  {\n    \"role\":
      \"system\",\n    \"content\": \"You will get instructions for code to write.\\nYou
      will write a very long answer. Make sure that every detail of the architecture
      is, in the end, implemented as code.\\nMake sure that every detail of the architecture
      is, in the end, implemented as code.\\n\\nThink step by step and reason yourself
      to the right decisions to make sure we get it right.\\nYou will first lay out
      the names of the core classes, functions, methods that will be necessary, as
      well as a quick comment on their purpose.\\n\\nThen you will output the content
      of each file including ALL code.\\nEach file must strictly follow a markdown
      code block format, where the following tokens must be replaced such that\\nFILENAME
      is the lowercase file name including the file extension,\\nLANG is the markup
      code block language for the code''s language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou
      will start with the \\\"entrypoint\\\" file, then go to the ones that are imported
      by that file, and so on.\\nPlease note that the code should be fully functional.
      No placeholders.\\n\\nFollow a language and framework appropriate best practice
      file naming convention.\\nMake sure that files contain all imports, types etc.
      Make sure that code in different files are compatible with each other.\\nEnsure
      to implement all code, if you are unsure, write a plausible implementation.\\nInclude
      module dependency or package manager dependency definition file.\\nBefore you
      finish, double check that all parts of the architecture is present in the files.\\n\\nUseful
      to know:\\nYou almost always put different classes in different files.\\nFor
      Python, you always create an appropriate requirements.txt file.\\nFor NodeJS,
      you always create an appropriate package.json file.\\nYou always add a comment
      briefly describing the purpose of the function definition.\\nYou try to add
      comments explaining very complex bits of logic.\\nYou always follow the best
      practices for the requested languages in terms of describing the code written
      as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n-
      dataclasses\\n\"\n  },\n #  \u2026 same conversation as earlier, ended with
      \"Make your own assumptions and state them explicitly before starting\".\n  {\n    \"role\":
      \"assistant\",\n    \"content\": \"Assumptions:\\n1. Model: The model will contain
      the game''s data, such as level information, character states, and enemy positions.\\n2.
      View: The view will handle the game''s visuals, including rendering the game
      objects, backgrounds, and updating the display.\\n3. Controller: The controller
      will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith
      these assumptions in mind, is there any specific aspect of the keyboard control
      implementation you would like me to clarify?\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\":
      \"Please now remember the steps:\\n\\nThink step by step and reason yourself
      to the right decisions to make sure we get it right.\\nFirst lay out the names
      of the core classes, functions, methods that will be necessary, As well as a
      quick comment on their purpose.\\n\\nThen you will output the content of each
      file including ALL code.\\nEach file must strictly follow a markdown code block
      format, where the following tokens must be replaced such that\\nFILENAME is
      the lowercase file name including the file extension,\\nLANG is the markup code
      block language for the code''s language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease
      note that the code should be fully functional. No placeholders.\\n\\nYou will
      start with the \\\"entrypoint\\\" file, then go to the ones that are imported
      by that file, and so on.\\nFollow a language and framework appropriate best
      practice file naming convention.\\nMake sure that files contain all imports,
      types etc. The code should be fully functional. Make sure that code in different
      files are compatible with each other.\\nBefore you finish, double check that
      all parts of the architecture is present in the files.\\n\"\n  }\n]\nChallenges#\nAfter
      going through key ideas and demos of building LLM-centered agents, I start to
      see a couple common limitations:\n\n\nFinite context length: The restricted
      context capacity limits the inclusion of historical information, detailed instructions,
      API call context, and responses. The design of the system has to work with this
      limited communication bandwidth, while mechanisms like self-reflection to learn
      from past mistakes would benefit a lot from long or infinite context windows.
      Although vector stores and retrieval can provide access to a larger knowledge
      pool, their representation power is not as powerful as full attention.\n\n\nChallenges
      in long-term planning and task decomposition: Planning over a lengthy history
      and effectively exploring the solution space remain challenging. LLMs struggle
      to adjust plans when faced with unexpected errors, making them less robust compared
      to humans who learn from trial and error.\n\n\nReliability of natural language
      interface: Current agent system relies on natural language as an interface between
      LLMs and external components such as memory and tools. However, the reliability
      of model outputs is questionable, as LLMs may make formatting errors and occasionally
      exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently,
      much of the agent demo code focuses on parsing model output.\n\n\nCitation#\nCited
      as:\n\nWeng, Lilian. (Jun 2023). \u201cLLM-powered Autonomous Agents\u201d.
      Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n\nOr\n@article{weng2023agent,\n  title   =
      \"LLM-powered Autonomous Agents\",\n  author  = \"Weng, Lilian\",\n  journal
      = \"lilianweng.github.io\",\n  year    = \"2023\",\n  month   = \"Jun\",\n  url     =
      \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n}\nReferences#\n[1]
      Wei et al. \u201cChain of thought prompting elicits reasoning in large language
      models.\u201d NeurIPS 2022\n[2] Yao et al. \u201cTree of Thoughts: Dliberate
      Problem Solving with Large Language Models.\u201d arXiv preprint arXiv:2305.10601
      (2023).\n[3] Liu et al. \u201cChain of Hindsight Aligns Language Models with
      Feedback\n\u201c arXiv preprint arXiv:2302.02676 (2023).\n[4] Liu et al. \u201cLLM+P:
      Empowering Large Language Models with Optimal Planning Proficiency\u201d arXiv
      preprint arXiv:2304.11477 (2023).\n[5] Yao et al. \u201cReAct: Synergizing reasoning
      and acting in language models.\u201d ICLR 2023.\n[6] Google Blog. \u201cAnnouncing
      ScaNN: Efficient Vector Similarity Search\u201d July 28, 2020.\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n[8]
      Shinn & Labash. \u201cReflexion: an autonomous agent with dynamic memory and
      self-reflection\u201d arXiv preprint arXiv:2303.11366 (2023).\n[9] Laskin et
      al. \u201cIn-context Reinforcement Learning with Algorithm Distillation\u201d
      ICLR 2023.\n[10] Karpas et al. \u201cMRKL Systems A modular, neuro-symbolic
      architecture that combines large language models, external knowledge sources
      and discrete reasoning.\u201d arXiv preprint arXiv:2205.00445 (2022).\n[11]
      Nakano et al. \u201cWebgpt: Browser-assisted question-answering with human feedback.\u201d
      arXiv preprint arXiv:2112.09332 (2021).\n[12] Parisi et al. \u201cTALM: Tool
      Augmented Language Models\u201d\n[13] Schick et al. \u201cToolformer: Language
      Models Can Teach Themselves to Use Tools.\u201d arXiv preprint arXiv:2302.04761
      (2023).\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n[15]
      Li et al. \u201cAPI-Bank: A Benchmark for Tool-Augmented LLMs\u201d arXiv preprint
      arXiv:2304.08244 (2023).\n[16] Shen et al. \u201cHuggingGPT: Solving AI Tasks
      with ChatGPT and its Friends in HuggingFace\u201d arXiv preprint arXiv:2303.17580
      (2023).\n[17] Bran et al. \u201cChemCrow: Augmenting large-language models with
      chemistry tools.\u201d arXiv preprint arXiv:2304.05376 (2023).\n[18] Boiko et
      al. \u201cEmergent autonomous scientific research capabilities of large language
      models.\u201d arXiv preprint arXiv:2304.05332 (2023).\n[19] Joon Sung Park,
      et al. \u201cGenerative Agents: Interactive Simulacra of Human Behavior.\u201d
      arXiv preprint arXiv:2304.03442 (2023).\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n[21]
      GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n\n\n\nnlp\nlanguage-model\nagent\nsteerability\nprompting\n\n\n\n\u00ab
      \n\nAdversarial Attacks on LLMs\n\n\n \u00bb\n\nPrompt Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a9
      2024 Lil''Log\n\n        Powered by\n        Hugo &\n        PaperMod\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "role": "system"}], "model": "gpt-4o-mini", "n": 1, "stream": false, "temperature":
      0.0}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '45469'
      content-type:
      - application/json
      host:
      - api.openai.com
      user-agent:
      - OpenAI/Python 1.45.0
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.45.0
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.11.7
    method: POST
    uri: https://api.openai.com/v1/chat/completions
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAAwAAAP//dFXbbhtHDH33VxD7lBiS4IsSX97cIGiSyojhKGjRujCoWe4u41nOdjgr
        yQny7wVndUuRvgjQcHk7PDz8dgRQcFlcQ+EaTK7t/Pjml8/dx3P+cHd1J3+8+3Cjb8Pll+n9/Z/p
        04UvRuYRFl/Ipa3XxIW285Q4yGB2kTCRRT29OLs4PzmfXkyzoQ0leXOruzSehnHLwuOzk7Pp+ORi
        fHq58W4CO9LiGv46AgD4ln+tTilpXVzDyWj70pIq1lRc7z4CKGLw9lKgKmtCScVob3RBEkkufd4Q
        YEzsPMFDMZvdwl1YUaQSbvoUJLShV7ipSZI+FLB4hhl7RoHfSWooWV2vSgqpIShpST50LUkClBIc
        drhgz4lJIVSA+4CYA0K3SbV4Bo+xJvAodY81QcZI4cVsdqsvJ/A+QeiTZyEFBH3WRC2EJcUl0wpS
        gwlYnO/LXEokghZZwEYSxFJdQ+dRhKUeQUttiM+jXGMKwUOvNIEHeZDTCRwf320+PD6+3jQOJeVI
        SjAMeQ0J9UmBJQVoUbAmXHgC7Rd1QK85NEltnbCAkq/GkSpPzugBKQC3XQxLMtw4WnxWDjJu8Yml
        hi4GR4brBObkGuF/elLw/ETwprHGQgXzJvR1k+DFmzB/mRPOre+9RYGkQXGUi931wLkE+z4SarBO
        J9b7mfV+m6Gxzg95UbImlrpnbUhhQWlFJKBNiGmcKLY5mg9SD/+2+K44NYdfDe+wIGvRGs1AyDiz
        cZ3AE0bJtp+Egz6x569mpnWiKOhhSS6FCJpCJIUqRGCpQmwxtxgpRaYl+gnc4prbvoX3IhThLoay
        dwk+EUbXwIvb93efXgL6OkROTauAkXbULnNcqip2bMTeVIPOBpSBOzfg5sajz0pb6FgS1XEoJFT7
        ko1vCuh9WCkYuQ0DsxoXMxd+2JoFPYedpYs0ThFZqIQnCStPZU0TeLtG46RuNwBu73+bbXZER7Bq
        2DXG2wULDSnzYGjdUUy2aL0nHbahitjSKsSnDdne9XXNUv96Nx92rEJnlWHacGq7UwPf1+R6azij
        csgf9Bqg4brxnHnpGvSepLahoRv2fza7HW/lYFCHEWjvGkCFioWT7d6WJ1KnZgQl21R6n5FiOaDM
        ftdxQA8ieR5gfbZ5CKY+ot/rjc0rWjGaxcaFrZpkrBwqgaa+tEwltUE02XClzsG7iC6xQw/YdZ5d
        HnuWvNSQ0lbtTAkyibhilwlmCvaca8z5LcqSQLnt/RAjQ/lxSRG9H+VcW0yp7RpU/rrR3i6YoDN6
        y5qHjAo1iXmanCw8taDBLynqFhSOsIUkBah6GdQJ9SdKnZpoomKLQEOV/9XTjQ6a+h9Ka97a3Mzk
        8AJFqnpFu4LSe795/747aT7UVrRu7Lt3Y4I2j4Ny2fnSFLoiW78fAfydT2f/wzUsuhjaLj2m8ERi
        Aa9eXZ0OAYv9yd6bz66mG2sKCf2B3+Xlq//zeywpIXs9uMHFTl73IU52leZWi2FJHyuWmmIXeTjJ
        Vfd4ulhMX5++vqiuiqPvR/8CAAD//wMA/R6HyqAIAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8c8e769b1cb89044-BOS
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 25 Sep 2024 22:31:18 GMT
      Server:
      - cloudflare
      Set-Cookie:
      - __cf_bm=NmdAizdz.69EDUO2j_u1k94FBhUh6rQxTh9wiU42d44-1727303478-1.0.1.1-dJYnQ5CG33lJXKMhr42mMc9sbzBuf49pgMrm6LV3duhPNWYG3CAD2iiEY9pS.qm.X_fDfz21REWUUKI0ieqOSA;
        path=/; expires=Wed, 25-Sep-24 23:01:18 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=aZTGr7WQB9pD7cUPNdO2yQPue790AorTqZ2f.K.Z4Dk-1727303478761-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      Transfer-Encoding:
      - chunked
      X-Content-Type-Options:
      - nosniff
      access-control-expose-headers:
      - X-Request-ID
      openai-organization:
      - user-wzxwdcuddhvwm09z43ibeucf
      openai-processing-ms:
      - '4012'
      openai-version:
      - '2020-10-01'
      strict-transport-security:
      - max-age=31536000; includeSubDomains; preload
      x-ratelimit-limit-requests:
      - '5000'
      x-ratelimit-limit-tokens:
      - '4000000'
      x-ratelimit-remaining-requests:
      - '4999'
      x-ratelimit-remaining-tokens:
      - '3988977'
      x-ratelimit-reset-requests:
      - 12ms
      x-ratelimit-reset-tokens:
      - 165ms
      x-request-id:
      - req_7a7649dd8652f4793d0bb74dfb4c80e2
    status:
      code: 200
      message: OK
version: 1
