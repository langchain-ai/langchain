{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7f95e4",
   "metadata": {},
   "source": [
    "# Chatbots\n",
    "\n",
    "## Use case\n",
    "\n",
    "---\n",
    "\n",
    "Chat is one of the central LLM use-cases.\n",
    "\n",
    "Aside from the base prompts and LLMs, memory is an important concept in Chatbots. \n",
    "\n",
    "Many chat based applications rely on remembering what happened in previous interactions using memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5bbf",
   "metadata": {},
   "source": [
    "![Image description](../../../docs_skeleton/static/img/chat_use_case.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48f490",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "---\n",
    "\n",
    "The chat model interface is based around messages rather than raw text.\n",
    "\n",
    "There components are important to consider for chat:\n",
    "\n",
    "* `chat_model`: See [here](https://integrations.langchain.com/) for a list of chat model integrations and [here](https://python.langchain.com/docs/modules/model_io/models/chat) for documentation on chat models\n",
    "* `prompt`: Chat prompt can supply a system message to the LLM along with user and AI keys\n",
    "* `memory`: [See here](https://langchain-git-francisco-extractiondocsrefactor-langchain.vercel.app/docs/modules/memory/) is in-depth documentation on memory types.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "---\n",
    "\n",
    "Here we can pass a prompt to a general LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850eb5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nJ'adore la programmation.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI() \n",
    "llm(\"Translate this sentence from English to French: I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88197b95",
   "metadata": {},
   "source": [
    "With a chat model, we can get chat completions by [passing one or more messages](https://python.langchain.com/docs/modules/model_io/models/chat). \n",
    "\n",
    "The response will be a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0d84ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI()\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to French: I love programming.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935d9a5",
   "metadata": {},
   "source": [
    "We can pass a set of chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afd27a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1d169",
   "metadata": {},
   "source": [
    "We can use a `ConversationChain`, which has built-in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdb05d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je adore la programmation.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain  \n",
    "  \n",
    "conversation = ConversationChain(llm=chat)  \n",
    "conversation.run(\"Translate this sentence from English to French: I love programming.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d801a173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich liebe Programmieren.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Translate it to German.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86788c",
   "metadata": {},
   "source": [
    "## Memory \n",
    "\n",
    "---\n",
    "\n",
    "`ConversationBufferMemory` is an simple form of memory that keeps a list of chat messages in a buffer.\n",
    "\n",
    "It passes those into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1380a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0db09f",
   "metadata": {},
   "source": [
    "## Chat \n",
    "\n",
    "---\n",
    "\n",
    "We can unpack what goes under the hood with `ConversationChain`.\n",
    "\n",
    "We can specify our memory, `ConversationBufferMemory`.\n",
    "\n",
    "We can specify the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fccd6995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hi',\n",
       " 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False)],\n",
       " 'text': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Prompt \n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb0cadfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: Translate this sentence from English to French: I love programming.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Translate this sentence from English to French: I love programming.',\n",
       " 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Je suis désolé, mais je ne peux pas effectuer de traduction en temps réel. Cependant, \"I love programming\" se traduit en français par \"J\\'adore programmer.\"', additional_kwargs={}, example=False)],\n",
       " 'text': 'Je suis désolé, mais je ne peux pas effectuer de traduction en temps réel. Cependant, \"I love programming\" se traduit en français par \"J\\'adore programmer.\"'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"Translate this sentence from English to French: I love programming.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c56d6219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: Translate this sentence from English to French: I love programming.\n",
      "AI: Je suis désolé, mais je ne peux pas effectuer de traduction en temps réel. Cependant, \"I love programming\" se traduit en français par \"J'adore programmer.\"\n",
      "Human: Translate it to German.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Translate it to German.',\n",
       " 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Je suis désolé, mais je ne peux pas effectuer de traduction en temps réel. Cependant, \"I love programming\" se traduit en français par \"J\\'adore programmer.\"', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Translate it to German.', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\"I love programming\" in German is \"Ich liebe Programmieren.\"', additional_kwargs={}, example=False)],\n",
       " 'text': '\"I love programming\" in German is \"Ich liebe Programmieren.\"'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"Translate it to German.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35cc16",
   "metadata": {},
   "source": [
    "## Chat Retrieval\n",
    "\n",
    "---\n",
    "\n",
    "It's common to combine chat with a retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b99b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058f1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ecf1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89fd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28503423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "retriever=vectorstore.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(llm,retriever=retriever,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c3bd5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How do agents use Task decomposition?',\n",
       " 'chat_history': [HumanMessage(content='How do agents use Task decomposition?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Agents use task decomposition to break down larger, complex tasks into smaller, manageable subgoals. This allows them to plan ahead and efficiently handle the overall task. Task decomposition can be done through various methods such as using simple prompts, task-specific instructions, or human inputs. By decomposing tasks, agents can reflect on past actions, learn from mistakes, and refine their approach for future steps, ultimately improving the quality of the final results.', additional_kwargs={}, example=False)],\n",
       " 'answer': 'Agents use task decomposition to break down larger, complex tasks into smaller, manageable subgoals. This allows them to plan ahead and efficiently handle the overall task. Task decomposition can be done through various methods such as using simple prompts, task-specific instructions, or human inputs. By decomposing tasks, agents can reflect on past actions, learn from mistakes, and refine their approach for future steps, ultimately improving the quality of the final results.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"How do agents use Task decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a29a7713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the various ways to implemet memory to support it?',\n",
       " 'chat_history': [HumanMessage(content='How do agents use Task decomposition?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Agents use task decomposition to break down larger, complex tasks into smaller, manageable subgoals. This allows them to plan ahead and efficiently handle the overall task. Task decomposition can be done through various methods such as using simple prompts, task-specific instructions, or human inputs. By decomposing tasks, agents can reflect on past actions, learn from mistakes, and refine their approach for future steps, ultimately improving the quality of the final results.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What are the various ways to implemet memory to support it?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='To implement memory and support task decomposition, the following methods can be used:\\n\\n1. Long-Term Memory (LTM) Management: This involves storing and retrieving information from a long-term memory system. It allows the agent to remember past actions, outcomes, and knowledge, which can be used for task decomposition and planning.\\n\\n2. Internet Access: The agent can utilize internet access to search for information and gather resources related to the task at hand. This can aid in task decomposition by providing additional knowledge and insights.\\n\\n3. GPT-3.5 Powered Agents: These agents, powered by GPT-3.5, can be used to delegate simple tasks to. By offloading simpler tasks to these agents, the main agent can focus on higher-level task decomposition and planning.\\n\\n4. File Output: The agent can generate and save files as output, which can serve as a form of memory. These files can contain information, plans, or progress updates related to the task, supporting task decomposition and providing a reference for future steps.\\n\\nOverall, memory management, internet access, delegated agents, and file output are methods that can be utilized to implement memory and support task decomposition.', additional_kwargs={}, example=False)],\n",
       " 'answer': 'To implement memory and support task decomposition, the following methods can be used:\\n\\n1. Long-Term Memory (LTM) Management: This involves storing and retrieving information from a long-term memory system. It allows the agent to remember past actions, outcomes, and knowledge, which can be used for task decomposition and planning.\\n\\n2. Internet Access: The agent can utilize internet access to search for information and gather resources related to the task at hand. This can aid in task decomposition by providing additional knowledge and insights.\\n\\n3. GPT-3.5 Powered Agents: These agents, powered by GPT-3.5, can be used to delegate simple tasks to. By offloading simpler tasks to these agents, the main agent can focus on higher-level task decomposition and planning.\\n\\n4. File Output: The agent can generate and save files as output, which can serve as a form of memory. These files can contain information, plans, or progress updates related to the task, supporting task decomposition and providing a reference for future steps.\\n\\nOverall, memory management, internet access, delegated agents, and file output are methods that can be utilized to implement memory and support task decomposition.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"What are the various ways to implemet memory to support it?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
