From 01d0ef02c1850b23f6a1aaf49c229dc0406c1fff Mon Sep 17 00:00:00 2001
From: Paul <paul.mirroros@hotmail.com>
Date: Tue, 9 Dec 2025 11:12:02 +0530
Subject: [PATCH] fix(chat_models): Add 'from_model_id' to ChatHuggingFace and
 update init_chat_model for flexibility

---
 .../langchain_classic/chat_models/base.py     |  5 +--
 .../chat_models/huggingface.py                | 44 +++++++++++++++++++
 2 files changed, 46 insertions(+), 3 deletions(-)

diff --git a/libs/langchain/langchain_classic/chat_models/base.py b/libs/langchain/langchain_classic/chat_models/base.py
index 8135d3b103..e637b866f0 100644
--- a/libs/langchain/langchain_classic/chat_models/base.py
+++ b/libs/langchain/langchain_classic/chat_models/base.py
@@ -444,10 +444,9 @@ def _init_chat_model_helper(
 
     if model_provider == "huggingface":
         _check_pkg("langchain_huggingface")
-        from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
+        from langchain_huggingface import ChatHuggingFace
 
-        llm = HuggingFacePipeline.from_model_id(model_id=model, **kwargs)
-        return ChatHuggingFace(llm=llm)
+        return ChatHuggingFace.from_model_id(model_id=model, **kwargs)
 
     if model_provider == "groq":
         _check_pkg("langchain_groq")
diff --git a/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py b/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py
index 2f61bf9ef8..9c8f663ce5 100644
--- a/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py
+++ b/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py
@@ -599,6 +599,50 @@ class ChatHuggingFace(BaseChatModel):
             self.profile = _get_default_model_profile(self.model_id)
         return self
 
+    @classmethod
+    def from_model_id(
+        cls,
+        model_id: str,
+        task: str | None = None,
+        backend: Literal["pipeline", "endpoint", "text-gen"] = "pipeline",
+        **kwargs: Any,
+    ) -> ChatHuggingFace:
+        """Construct a ChatHuggingFace model from a model_id.
+
+        Args:
+            model_id: The model ID of the Hugging Face model.
+            task: The task to perform (e.g., "text-generation").
+            backend: The backend to use. One of "pipeline", "endpoint", "text-gen".
+            **kwargs: Additional arguments to pass to the backend or ChatHuggingFace.
+        """
+        if backend == "pipeline":
+            from langchain_huggingface.llms.huggingface_pipeline import (
+                HuggingFacePipeline,
+            )
+
+            llm = HuggingFacePipeline.from_model_id(
+                model_id=model_id, task=task, **kwargs
+            )
+        elif backend == "endpoint":
+            from langchain_huggingface.llms.huggingface_endpoint import (
+                HuggingFaceEndpoint,
+            )
+
+            llm = HuggingFaceEndpoint(repo_id=model_id, task=task, **kwargs)
+        elif backend == "text-gen":
+            from langchain_community.llms.huggingface_text_gen_inference import (  # type: ignore[import-not-found]
+                HuggingFaceTextGenInference,
+            )
+
+            llm = HuggingFaceTextGenInference(
+                inference_server_url=model_id, **kwargs
+            )
+        else:
+            msg = f"Unknown backend: {backend}"
+            raise ValueError(msg)
+
+        return cls(llm=llm, **kwargs)
+
     def _create_chat_result(self, response: dict) -> ChatResult:
         generations = []
         token_usage = response.get("usage", {})
-- 
2.39.5 (Apple Git-154)

