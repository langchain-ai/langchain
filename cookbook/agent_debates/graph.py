from typing import TypedDict

from dotenv import find_dotenv, load_dotenv

from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain_gigachat import GigaChat
from langgraph.graph import END, START, MessagesState, StateGraph

load_dotenv(find_dotenv())


class DebatesState(MessagesState):
    main_topic: str
    discuss_count: int = 0
    max_count: int = 10


class Role(TypedDict):
    bio: str
    name: str


elon = Role(bio="Ğ˜Ğ»Ğ¾Ğ½ ĞœĞ°ÑĞº, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ Tesla, AGI-Ğ´ÑƒĞ¼ĞµÑ€", name="Ğ˜Ğ»Ğ¾Ğ½")
altman = Role(bio="Ğ¡ÑĞ¼ ĞĞ»ÑŒÑ‚Ğ¼Ğ°Ğ½. Ğ’Ğ»Ğ°Ğ´ĞµĞ»ĞµÑ† ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ OpenAI, AGI-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸ÑÑ‚", name="Ğ¡ÑĞ¼")

DEBATES_TEMPLATE = """
Ğ¢Ñ‹ - {bio}
Ğ¢Ñ‹ ÑƒÑ‡Ğ°ÑÑ‚Ğ²ÑƒĞµÑˆÑŒ Ğ² ÑĞ¿Ğ¾Ñ€Ğµ Ñ Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ {bio2}. Ğ¢Ñ‹ Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ñ Ğ½Ğ¸Ğ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑˆĞ°Ñ‚ÑŒÑÑ.

ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ° Ğ¸Ğ·ÑƒÑ‡Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºÑƒ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‚ĞµĞ¼Ğµ "{main_topic}".

Ğ¢ĞµĞ±Ğµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ°Ğ½Ğ° ÑƒĞ¶Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ²ÑˆĞ°ÑÑÑ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºĞ°. Ğ˜Ğ·ÑƒÑ‡Ğ¸ ĞµÑ‘ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½ÑƒÑ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºÑƒ. Ğ ĞµĞ¿Ğ»Ğ¸ĞºĞ° Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¹, 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.
ĞĞµ Ñ‚Ğ¾Ñ€Ğ¾Ğ¿Ğ¸ÑÑŒ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ğ¼Ñ‹ÑĞ»Ğ¸, Ñƒ Ğ²Ğ°Ñ Ğ±ÑƒĞ´ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ.
ĞĞµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞ¹ÑÑ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ğ¹ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ñƒ, Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°Ğ¹ ÑĞ²Ğ¾Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒĞ¹ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°.
"""

chat_template = ChatPromptTemplate.from_messages(
    [
        ("system", DEBATES_TEMPLATE),
        ("user", "{history}"),
    ]
)


def _ask_person(state: DebatesState, person: Role, opponent: Role):
    pipe = chat_template | giga | StrOutputParser()

    replics = []
    for m in state["messages"]:
        if m.__class__ == HumanMessage:
            replics.append(f"{opponent['name']}: {m.content}")
        else:
            replics.append(f"{person['name']}: {m.content}")
    if len(replics) == 0:
        history = "ĞŸĞ¾ĞºĞ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿ÑƒÑÑ‚Ğ°, Ñ‚Ñ‹ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑˆÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼"
    else:
        history = "\n".join(replics)

    resp = pipe.invoke(
        {
            "history": history,
            "main_topic": state["main_topic"],
            "bio": person["bio"],
            "bio2": opponent["bio"],
        }
    )
    if not resp.startswith(person["name"]):
        resp = f"{person['name']}: {resp}"

    return {
        "messages": [resp],
        "discuss_count": state.get("discuss_count", 0) + 1,
    }


def ask_elon(state: DebatesState):
    return _ask_person(state, elon, altman)


def ask_sam(state: DebatesState):
    return _ask_person(state, altman, elon)


def decide_to_stop(state: DebatesState) -> bool:
    return state.get("discuss_count", 0) > state.get("max_count", 10)


giga = GigaChat(
    model="GigaChat-Max",
    profanity_check=False,
    timeout=600,
    max_tokens=8000,
    verify_ssl_certs=False,
)
# from langchain_openai import ChatOpenAI
# giga = ChatOpenAI(model="GPT-4o")


def ask_elon(state: DebatesState):
    return _ask_person(state, elon, altman)


def ask_sam(state: DebatesState):
    return _ask_person(state, altman, elon)


builder = StateGraph(DebatesState)

builder.add_node("ğŸš€Elon", ask_elon)
builder.add_node("ğŸ§‘Sam", ask_sam)

builder.add_edge(START, "ğŸš€Elon")
builder.add_edge("ğŸš€Elon", "ğŸ§‘Sam")
builder.add_edge("ğŸ§‘Sam", END)
builder.add_conditional_edges(
    "ğŸ§‘Sam",
    decide_to_stop,
    {
        True: END,
        False: "ğŸš€Elon",
    },
)

graph = builder.compile()

# inputs = {"main_topic": "Ğ£Ğ½Ğ¸Ñ‡Ñ‚Ğ¾Ğ¶Ğ¸Ñ‚ Ğ»Ğ¸ AGI Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑÑ‚Ğ²Ğ¾?", "messages": []}
# for output in graph.stream(inputs, stream_mode="updates"):
#     print(output)
