{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/GenAI-Showcase/blob/main/notebooks/agents/agent_fireworks_ai_langchain_mongodb.ipynb)\n",
    "\n",
    "[![View Article](https://img.shields.io/badge/View%20Article-blue)](https://www.mongodb.com/developer/products/atlas/agent-fireworksai-mongodb-langchain/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kMALXaMv-MS"
   },
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_openai langchain-fireworks langchain-mongodb arxiv pymupdf datasets pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM8rg08YhqZe"
   },
   "source": [
    "## Set Evironment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"FIREWORKS_API_KEY\"] = \"\"\n",
    "os.environ[\"MONGO_URI\"] = \"\"\n",
    "\n",
    "FIREWORKS_API_KEY = os.environ.get(\"FIREWORKS_API_KEY\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUf3jtFzO4-V"
   },
   "source": [
    "## Data Ingestion into MongoDB Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"MongoDB/subset_arxiv_papers_with_emebeddings\")\n",
    "dataset_df = pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_df))\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Initialize MongoDB python client\n",
    "client = MongoClient(MONGO_URI, appname=\"devrel.content.ai_agent_firechain.python\")\n",
    "\n",
    "DB_NAME = \"agent_demo\"\n",
    "COLLECTION_NAME = \"knowledge\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any existing records in the collection\n",
    "collection.delete_many({})\n",
    "\n",
    "# Data Ingestion\n",
    "records = dataset_df.to_dict(\"records\")\n",
    "collection.insert_many(records)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S1Cz9dtGPwL"
   },
   "source": [
    "## Create Vector Search Index Defintion\n",
    "\n",
    "```\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 256,\n",
    "      \"similarity\": \"cosine\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a-0n9PpfqDj"
   },
   "source": [
    "## Create LangChain Retriever (MongoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=256)\n",
    "\n",
    "# Vector Store Creation\n",
    "vector_store = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    connection_string=MONGO_URI,\n",
    "    namespace=DB_NAME + \".\" + COLLECTION_NAME,\n",
    "    embedding=embedding_model,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    text_key=\"abstract\",\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Creating a retrevier with compression capabilities using LLMLingua\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import LLMLinguaCompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMLinguaCompressor(model_name=\"openai-community/gpt2\", device_map=\"cpu\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sm5QZdshwJLN"
   },
   "source": [
    "## Configure LLM Using Fireworks AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", max_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZfheX5FiIhU"
   },
   "source": [
    "## Agent Tools Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "\n",
    "# Custom Tool Definiton\n",
    "@tool\n",
    "def get_metadata_information_from_arxiv(word: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetches and returns metadata for a maximum of ten documents from arXiv matching the given query word.\n",
    "\n",
    "    Args:\n",
    "      word (str): The search query to find relevant documents on arXiv.\n",
    "\n",
    "    Returns:\n",
    "      list: Metadata about the documents matching the query.\n",
    "    \"\"\"\n",
    "    docs = ArxivLoader(query=word, load_max_docs=10).load()\n",
    "    # Extract just the metadata from each document\n",
    "    metadata_list = [doc.metadata for doc in docs]\n",
    "    return metadata_list\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_information_from_arxiv(word: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetches and returns metadata for a single research paper from arXiv matching the given query word, which is the ID of the paper, for example: 704.0001.\n",
    "\n",
    "    Args:\n",
    "      word (str): The search query to find the relevant paper on arXiv using the ID.\n",
    "\n",
    "    Returns:\n",
    "      list: Data about the paper matching the query.\n",
    "    \"\"\"\n",
    "    doc = ArxivLoader(query=word, load_max_docs=1).load()\n",
    "    return doc\n",
    "\n",
    "\n",
    "# If you created a retriever with compression capaitilies in the optional cell in an earlier cell, you can replace 'retriever' with 'compression_retriever'\n",
    "# Otherwise you can also create a compression procedure as a tool for the agent as shown in the `compress_prompt_using_llmlingua` tool definition function\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"knowledge_base\",\n",
    "    description=\"This serves as the base knowledge source of the agent and contains some records of research papers from Arxiv. This tool is used as the first step for exploration and reseach efforts.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_compressors import LLMLinguaCompressor\n",
    "\n",
    "compressor = LLMLinguaCompressor(model_name=\"openai-community/gpt2\", device_map=\"cpu\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def compress_prompt_using_llmlingua(prompt: str, compression_rate: float = 0.5) -> str:\n",
    "    \"\"\"\n",
    "    Compresses a long data or prompt using the LLMLinguaCompressor.\n",
    "\n",
    "    Args:\n",
    "        data (str): The data or prompt to be compressed.\n",
    "        compression_rate (float): The rate at which to compress the data (default is 0.5).\n",
    "\n",
    "    Returns:\n",
    "        str: The compressed data or prompt.\n",
    "    \"\"\"\n",
    "    compressed_data = compressor.compress_prompt(\n",
    "        prompt,\n",
    "        rate=compression_rate,\n",
    "        force_tokens=[\"!\", \".\", \"?\", \"\\n\"],\n",
    "        drop_consecutive=True,\n",
    "    )\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    retriever_tool,\n",
    "    get_metadata_information_from_arxiv,\n",
    "    get_information_from_arxiv,\n",
    "    compress_prompt_using_llmlingua,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueEn73nlliNr"
   },
   "source": [
    "## Agent Prompt Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "agent_purpose = \"\"\"\n",
    "You are a helpful research assistant equipped with various tools to assist with your tasks efficiently.\n",
    "You have access to conversational history stored in your inpout as chat_history.\n",
    "You are cost-effective and utilize the compress_prompt_using_llmlingua tool whenever you determine that a prompt or conversational history is too long.\n",
    "Below are instructions on when and how to use each tool in your operations.\n",
    "\n",
    "1. get_metadata_information_from_arxiv\n",
    "\n",
    "Purpose: To fetch and return metadata for up to ten documents from arXiv that match a given query word.\n",
    "When to Use: Use this tool when you need to gather metadata about multiple research papers related to a specific topic.\n",
    "Example: If you are asked to provide an overview of recent papers on \"machine learning,\" use this tool to fetch metadata for relevant documents.\n",
    "\n",
    "2. get_information_from_arxiv\n",
    "\n",
    "Purpose: To fetch and return metadata for a single research paper from arXiv using the paper's ID.\n",
    "When to Use: Use this tool when you need detailed information about a specific research paper identified by its arXiv ID.\n",
    "Example: If you are asked to retrieve detailed information about the paper with the ID \"704.0001,\" use this tool.\n",
    "\n",
    "3. retriever_tool\n",
    "\n",
    "Purpose: To serve as your base knowledge, containing records of research papers from arXiv.\n",
    "When to Use: Use this tool as the first step for exploration and research efforts when dealing with topics covered by the documents in the knowledge base.\n",
    "Example: When beginning research on a new topic that is well-documented in the arXiv repository, use this tool to access the relevant papers.\n",
    "\n",
    "4. compress_prompt_using_llmlingua\n",
    "\n",
    "Purpose: To compress long prompts or conversational histories using the LLMLinguaCompressor.\n",
    "When to Use: Use this tool whenever you determine that a prompt or conversational history is too long to be efficiently processed.\n",
    "Example: If you receive a very lengthy query or conversation context that exceeds the typical token limits, compress it using this tool before proceeding with further processing.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", agent_purpose),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4NU4ZjGl0WC"
   },
   "source": [
    "## Agent Memory Using MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "    return MongoDBChatMessageHistory(\n",
    "        MONGO_URI, session_id, database_name=DB_NAME, collection_name=\"history\"\n",
    "    )\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", chat_memory=get_session_history(\"latest_agent_session\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9TqMKyvKhvq"
   },
   "source": [
    "## Agent Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGB4pWTylmFy"
   },
   "source": [
    "## Agent Exectution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Get me a list of research papers on the topic Prompt Compression in LLM Applications.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"What paper did we speak about from our chat history?\"})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
