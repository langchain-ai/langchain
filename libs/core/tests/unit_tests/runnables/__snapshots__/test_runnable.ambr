# serializer version: 1
# name: test_combining_sequences
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_combining_sequences.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(lambda x: {'question': x[0] + x[1]})"
      },
      "middle": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nicer assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "question"
                      ],
                      "template": "{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate"
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['baz, qux'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_combining_sequences.2
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel"
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "output_parsers",
            "list",
            "CommaSeparatedListOutputParser"
          ],
          "kwargs": {},
          "name": "CommaSeparatedListOutputParser"
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(lambda x: {'question': x[0] + x[1]})"
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nicer assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "question"
                      ],
                      "template": "{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate"
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['baz, qux'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_combining_sequences.3
  list([
    RunTree(id=00000000-0000-4000-8000-000000000000, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_configurable_fields[schema2]
  dict({
    '$defs': dict({
      'Configurable': dict({
        'properties': dict({
          'llm_responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableConfigurableFieldsConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields[schema3]
  dict({
    '$defs': dict({
      'Configurable': dict({
        'properties': dict({
          'prompt_template': dict({
            'default': 'Hello, {name}!',
            'description': 'The prompt template for this chain',
            'title': 'Prompt Template',
            'type': 'string',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableConfigurableFieldsConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields[schema4]
  dict({
    '$defs': dict({
      'Configurable': dict({
        'properties': dict({
          'llm_responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
          'prompt_template': dict({
            'default': 'Hello, {name}!',
            'description': 'The prompt template for this chain',
            'title': 'Prompt Template',
            'type': 'string',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableSequenceConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields[schema5]
  dict({
    '$defs': dict({
      'Configurable': dict({
        'properties': dict({
          'llm_responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
          'other_responses': dict({
            'default': list([
              'a',
            ]),
            'items': dict({
              'type': 'string',
            }),
            'title': 'Other Responses',
            'type': 'array',
          }),
          'prompt_template': dict({
            'default': 'Hello, {name}!',
            'description': 'The prompt template for this chain',
            'title': 'Prompt Template',
            'type': 'string',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableSequenceConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields_example[schema7]
  dict({
    '$defs': dict({
      'Chat_Responses': dict({
        'title': 'Chat Responses',
      }),
      'Configurable': dict({
        'properties': dict({
          'chat_responses': dict({
            'default': list([
              'hello',
              'bye',
            ]),
            'items': dict({
              '$ref': '#/$defs/Chat_Responses',
            }),
            'title': 'Chat Responses',
            'type': 'array',
          }),
          'llm': dict({
            '$ref': '#/$defs/LLM',
            'default': 'default',
          }),
          'llm_responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
          'prompt_template': dict({
            '$ref': '#/$defs/Prompt_Template',
            'default': 'hello',
            'description': 'The prompt template for this chain',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
      'LLM': dict({
        'title': 'LLM',
      }),
      'Prompt_Template': dict({
        'title': 'Prompt Template',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableSequenceConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields_prefix_keys[schema6]
  dict({
    'definitions': dict({
      'Chat_Responses': dict({
        'title': 'Chat Responses',
      }),
      'Configurable': dict({
        'properties': dict({
          'chat_sleep': dict({
            'anyOf': list([
              dict({
                'type': 'number',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chat Sleep',
          }),
          'llm': dict({
            '$ref': '#/definitions/LLM',
            'default': 'default',
          }),
          'llm==chat/responses': dict({
            'default': list([
              'hello',
              'bye',
            ]),
            'items': dict({
              '$ref': '#/definitions/Chat_Responses',
            }),
            'title': 'Chat Responses',
            'type': 'array',
          }),
          'llm==default/responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
          'prompt_template': dict({
            '$ref': '#/definitions/Prompt_Template',
            'default': 'hello',
            'description': 'The prompt template for this chain',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
      'LLM': dict({
        'title': 'LLM',
      }),
      'Prompt_Template': dict({
        'title': 'Prompt Template',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/definitions/Configurable',
      }),
    }),
    'title': 'RunnableSequenceConfig',
    'type': 'object',
  })
# ---
# name: test_each
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeStreamingListLLM"
          ],
          "repr": "FakeStreamingListLLM(responses=['first item, second item, third item'])",
          "name": "FakeStreamingListLLM"
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "tests",
            "unit_tests",
            "runnables",
            "test_runnable",
            "FakeSplitIntoListParser"
          ],
          "kwargs": {},
          "name": "FakeSplitIntoListParser"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableEach"
        ],
        "kwargs": {
          "bound": {
            "lc": 1,
            "type": "not_implemented",
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeStreamingListLLM"
            ],
            "repr": "FakeStreamingListLLM(responses=['this', 'is', 'a', 'test'])",
            "name": "FakeStreamingListLLM"
          }
        },
        "name": "RunnableEach<FakeStreamingListLLM>"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_higher_order_lambda_runnable
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "key": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(lambda x: x['key'])"
            },
            "input": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableParallel"
              ],
              "kwargs": {
                "steps__": {
                  "question": {
                    "lc": 1,
                    "type": "not_implemented",
                    "id": [
                      "langchain_core",
                      "runnables",
                      "base",
                      "RunnableLambda"
                    ],
                    "repr": "RunnableLambda(lambda x: x['question'])"
                  }
                }
              },
              "name": "RunnableParallel<question>"
            }
          }
        },
        "name": "RunnableParallel<key,input>"
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(router)"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_lambda_schemas[schema8]
  dict({
    '$defs': dict({
      'OutputType': dict({
        'properties': dict({
          'bye': dict({
            'title': 'Bye',
            'type': 'string',
          }),
          'byebye': dict({
            'title': 'Byebye',
            'type': 'integer',
          }),
          'hello': dict({
            'title': 'Hello',
            'type': 'string',
          }),
        }),
        'required': list([
          'hello',
          'bye',
          'byebye',
        ]),
        'title': 'OutputType',
        'type': 'object',
      }),
    }),
    '$ref': '#/$defs/OutputType',
    'title': 'aget_values_typed_output',
  })
# ---
# name: test_prompt_with_chat_model
  '''
  ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])
  | FakeListChatModel(responses=['foo'])
  '''
# ---
# name: test_prompt_with_chat_model.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake_chat_models",
          "FakeListChatModel"
        ],
        "repr": "FakeListChatModel(responses=['foo'])",
        "name": "FakeListChatModel"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_chat_model.2
  list([
    RunTree(id=00000000-0000-4000-8000-000000000000, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_chat_model_and_parser
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_chat_model_and_parser.1
  list([
    RunTree(id=00000000-0000-4000-8000-000000000000, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_chat_model_async
  '''
  ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])
  | FakeListChatModel(responses=['foo'])
  '''
# ---
# name: test_prompt_with_chat_model_async.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake_chat_models",
          "FakeListChatModel"
        ],
        "repr": "FakeListChatModel(responses=['foo'])",
        "name": "FakeListChatModel"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_chat_model_async.2
  list([
    RunTree(id=00000000-0000-4000-8000-000000000000, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake",
          "FakeListLLM"
        ],
        "repr": "FakeListLLM(responses=['foo', 'bar'])",
        "name": "FakeListLLM"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_llm.1
  list([
    RunTree(id=00000000-0000-4000-8000-000000000000, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm.2
  list([
    RunTree(id=00000000-0000-4000-8000-000000000000, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
    RunTree(id=00000000-0000-4000-8000-000000000003, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000003'),
  ])
# ---
# name: test_prompt_with_llm_and_async_lambda
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeListLLM"
          ],
          "repr": "FakeListLLM(responses=['foo', 'bar'])",
          "name": "FakeListLLM"
        }
      ],
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(afunc=passthrough)"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_llm_and_async_lambda.1
  list([
    RunTree(id=00000000-0000-4000-8000-000000000000, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm_parser
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeStreamingListLLM"
          ],
          "repr": "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'])",
          "name": "FakeStreamingListLLM"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_llm_parser.1
  list([
    RunTree(id=00000000-0000-4000-8000-000000000000, name='RunnableSequence', run_type='chain', dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_router_runnable
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "key": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            },
            "input": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableParallel"
              ],
              "kwargs": {
                "steps__": {
                  "question": {
                    "lc": 1,
                    "type": "not_implemented",
                    "id": [
                      "langchain_core",
                      "runnables",
                      "base",
                      "RunnableLambda"
                    ],
                    "repr": "RunnableLambda(...)"
                  }
                }
              },
              "name": "RunnableParallel<question>"
            }
          }
        },
        "name": "RunnableParallel<key,input>"
      },
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RouterRunnable"
        ],
        "kwargs": {
          "runnables": {
            "math": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "messages": [
                      {
                        "lc": 1,
                        "type": "constructor",
                        "id": [
                          "langchain",
                          "prompts",
                          "chat",
                          "HumanMessagePromptTemplate"
                        ],
                        "kwargs": {
                          "prompt": {
                            "lc": 1,
                            "type": "constructor",
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "kwargs": {
                              "input_variables": [
                                "question"
                              ],
                              "template": "You are a math genius. Answer the question: {question}",
                              "template_format": "f-string"
                            },
                            "name": "PromptTemplate"
                          }
                        }
                      }
                    ]
                  },
                  "name": "ChatPromptTemplate"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeListLLM"
                  ],
                  "repr": "FakeListLLM(responses=['4'])",
                  "name": "FakeListLLM"
                }
              },
              "name": "RunnableSequence"
            },
            "english": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "messages": [
                      {
                        "lc": 1,
                        "type": "constructor",
                        "id": [
                          "langchain",
                          "prompts",
                          "chat",
                          "HumanMessagePromptTemplate"
                        ],
                        "kwargs": {
                          "prompt": {
                            "lc": 1,
                            "type": "constructor",
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "kwargs": {
                              "input_variables": [
                                "question"
                              ],
                              "template": "You are an english major. Answer the question: {question}",
                              "template_format": "f-string"
                            },
                            "name": "PromptTemplate"
                          }
                        }
                      }
                    ]
                  },
                  "name": "ChatPromptTemplate"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeListLLM"
                  ],
                  "repr": "FakeListLLM(responses=['2'])",
                  "name": "FakeListLLM"
                }
              },
              "name": "RunnableSequence"
            }
          }
        },
        "name": "RouterRunnable"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_schemas[chat_prompt_input_schema]
  dict({
    '$defs': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          An `AIMessage` is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model and standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/$defs/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/$defs/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/$defs/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'description': 'Message chunk from an AI (yielded when streaming).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'chunk_position': dict({
            'anyOf': list([
              dict({
                'const': 'last',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chunk Position',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/$defs/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'items': dict({
              '$ref': '#/$defs/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/$defs/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/$defs/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `FunctionMessage` are an older version of the `ToolMessage` schema, and
          do not contain the `tool_call_id` field.
          
          The `tool_call_id` field is used to associate the tool call request with the
          tool call response. Useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from the user.
          
          A `HumanMessage` is a message that is passed in from a user to the model.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Instantiate a chat model and invoke it with the messages
              model = ...
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InputTokenDetails': dict({
        'description': '''
          Breakdown of input token counts.
          
          Does *not* need to sum to full input token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "cache_creation": 200,
                  "cache_read": 100,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'cache_creation': dict({
            'title': 'Cache Creation',
            'type': 'integer',
          }),
          'cache_read': dict({
            'title': 'Cache Read',
            'type': 'integer',
          }),
        }),
        'title': 'InputTokenDetails',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'extras': dict({
            'title': 'Extras',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'string',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'type',
          'id',
          'name',
          'args',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'OutputTokenDetails': dict({
        'description': '''
          Breakdown of output token counts.
          
          Does *not* need to sum to full output token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "reasoning": 200,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'reasoning': dict({
            'title': 'Reasoning',
            'type': 'integer',
          }),
        }),
        'title': 'OutputTokenDetails',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Define a chat model and invoke it with the messages
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents an AI's request to call a tool.
          
          Example:
              ```python
              {"name": "foo", "args": {"a": 1}, "id": "123"}
              ```
          
              This represents a request to call the tool named `'foo'` with arguments
              `{"a": 1}` and an identifier of `'123'`.
          
          !!! note "Factory function"
          
              `tool_call` may also be used as a factory to create a `ToolCall`. Benefits
              include:
          
              * Required arguments strictly validated at creation time
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (yielded when streaming).
          
          When merging `ToolCallChunk` objects (e.g., via `AIMessageChunk.__add__`), all
          string attributes are concatenated. Chunks are only merged if their values of
          `index` are equal and not `None`.
          
          Example:
          ```python
          left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
          right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
          
          (
              AIMessageChunk(content="", tool_call_chunks=left_chunks)
              + AIMessageChunk(content="", tool_call_chunks=right_chunks)
          ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
          ```
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `ToolMessage` objects contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          `tool_call_id` is used to associate the tool call request with the tool call
          response. Useful in situations where a chat model is able to request multiple tool
          calls in parallel.
          
          Example:
              A `ToolMessage` representing a result of `42` from a tool call with id
          
              ```python
              from langchain_core.messages import ToolMessage
          
              ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
              ```
          
          Example:
              A `ToolMessage` where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              ```python
              from langchain_core.messages import ToolMessage
          
              tool_output = {
                  "stdout": "From the graph we can see that the correlation between "
                  "x and y is ...",
                  "stderr": None,
                  "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
              }
          
              ToolMessage(
                  content=tool_output["stdout"],
                  artifact=tool_output,
                  tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
              )
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
              ```python
              {
                  "input_tokens": 350,
                  "output_tokens": 240,
                  "total_tokens": 590,
                  "input_token_details": {
                      "audio": 10,
                      "cache_creation": 200,
                      "cache_read": 100,
                  },
                  "output_token_details": {
                      "audio": 10,
                      "reasoning": 200,
                  },
              }
              ```
          
          !!! warning "Behavior changed in `langchain-core` 0.3.9"
          
              Added `input_token_details` and `output_token_details`.
          
          !!! note "LangSmith SDK"
          
              The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
              LangSmith's `UsageMetadata` has additional fields to capture cost information
              used by the LangSmith platform.
        ''',
        'properties': dict({
          'input_token_details': dict({
            '$ref': '#/$defs/InputTokenDetails',
          }),
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_token_details': dict({
            '$ref': '#/$defs/OutputTokenDetails',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'history': dict({
        'items': dict({
          'oneOf': list([
            dict({
              '$ref': '#/$defs/AIMessage',
            }),
            dict({
              '$ref': '#/$defs/HumanMessage',
            }),
            dict({
              '$ref': '#/$defs/ChatMessage',
            }),
            dict({
              '$ref': '#/$defs/SystemMessage',
            }),
            dict({
              '$ref': '#/$defs/FunctionMessage',
            }),
            dict({
              '$ref': '#/$defs/ToolMessage',
            }),
            dict({
              '$ref': '#/$defs/AIMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/HumanMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/ChatMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/SystemMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/FunctionMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/ToolMessageChunk',
            }),
          ]),
        }),
        'title': 'History',
        'type': 'array',
      }),
    }),
    'required': list([
      'history',
    ]),
    'title': 'PromptInput',
    'type': 'object',
  })
# ---
# name: test_schemas[chat_prompt_output_schema]
  dict({
    '$defs': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          An `AIMessage` is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model and standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/$defs/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/$defs/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/$defs/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'description': 'Message chunk from an AI (yielded when streaming).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'chunk_position': dict({
            'anyOf': list([
              dict({
                'const': 'last',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chunk Position',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/$defs/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'items': dict({
              '$ref': '#/$defs/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/$defs/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/$defs/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/$defs/AIMessage',
                }),
                dict({
                  '$ref': '#/$defs/HumanMessage',
                }),
                dict({
                  '$ref': '#/$defs/ChatMessage',
                }),
                dict({
                  '$ref': '#/$defs/SystemMessage',
                }),
                dict({
                  '$ref': '#/$defs/FunctionMessage',
                }),
                dict({
                  '$ref': '#/$defs/ToolMessage',
                }),
                dict({
                  '$ref': '#/$defs/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `FunctionMessage` are an older version of the `ToolMessage` schema, and
          do not contain the `tool_call_id` field.
          
          The `tool_call_id` field is used to associate the tool call request with the
          tool call response. Useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from the user.
          
          A `HumanMessage` is a message that is passed in from a user to the model.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Instantiate a chat model and invoke it with the messages
              model = ...
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InputTokenDetails': dict({
        'description': '''
          Breakdown of input token counts.
          
          Does *not* need to sum to full input token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "cache_creation": 200,
                  "cache_read": 100,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'cache_creation': dict({
            'title': 'Cache Creation',
            'type': 'integer',
          }),
          'cache_read': dict({
            'title': 'Cache Read',
            'type': 'integer',
          }),
        }),
        'title': 'InputTokenDetails',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'extras': dict({
            'title': 'Extras',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'string',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'type',
          'id',
          'name',
          'args',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'OutputTokenDetails': dict({
        'description': '''
          Breakdown of output token counts.
          
          Does *not* need to sum to full output token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "reasoning": 200,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'reasoning': dict({
            'title': 'Reasoning',
            'type': 'integer',
          }),
        }),
        'title': 'OutputTokenDetails',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Define a chat model and invoke it with the messages
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents an AI's request to call a tool.
          
          Example:
              ```python
              {"name": "foo", "args": {"a": 1}, "id": "123"}
              ```
          
              This represents a request to call the tool named `'foo'` with arguments
              `{"a": 1}` and an identifier of `'123'`.
          
          !!! note "Factory function"
          
              `tool_call` may also be used as a factory to create a `ToolCall`. Benefits
              include:
          
              * Required arguments strictly validated at creation time
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (yielded when streaming).
          
          When merging `ToolCallChunk` objects (e.g., via `AIMessageChunk.__add__`), all
          string attributes are concatenated. Chunks are only merged if their values of
          `index` are equal and not `None`.
          
          Example:
          ```python
          left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
          right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
          
          (
              AIMessageChunk(content="", tool_call_chunks=left_chunks)
              + AIMessageChunk(content="", tool_call_chunks=right_chunks)
          ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
          ```
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `ToolMessage` objects contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          `tool_call_id` is used to associate the tool call request with the tool call
          response. Useful in situations where a chat model is able to request multiple tool
          calls in parallel.
          
          Example:
              A `ToolMessage` representing a result of `42` from a tool call with id
          
              ```python
              from langchain_core.messages import ToolMessage
          
              ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
              ```
          
          Example:
              A `ToolMessage` where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              ```python
              from langchain_core.messages import ToolMessage
          
              tool_output = {
                  "stdout": "From the graph we can see that the correlation between "
                  "x and y is ...",
                  "stderr": None,
                  "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
              }
          
              ToolMessage(
                  content=tool_output["stdout"],
                  artifact=tool_output,
                  tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
              )
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
              ```python
              {
                  "input_tokens": 350,
                  "output_tokens": 240,
                  "total_tokens": 590,
                  "input_token_details": {
                      "audio": 10,
                      "cache_creation": 200,
                      "cache_read": 100,
                  },
                  "output_token_details": {
                      "audio": 10,
                      "reasoning": 200,
                  },
              }
              ```
          
          !!! warning "Behavior changed in `langchain-core` 0.3.9"
          
              Added `input_token_details` and `output_token_details`.
          
          !!! note "LangSmith SDK"
          
              The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
              LangSmith's `UsageMetadata` has additional fields to capture cost information
              used by the LangSmith platform.
        ''',
        'properties': dict({
          'input_token_details': dict({
            '$ref': '#/$defs/InputTokenDetails',
          }),
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_token_details': dict({
            '$ref': '#/$defs/OutputTokenDetails',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'anyOf': list([
      dict({
        '$ref': '#/$defs/StringPromptValue',
      }),
      dict({
        '$ref': '#/$defs/ChatPromptValueConcrete',
      }),
    ]),
    'title': 'ChatPromptTemplateOutput',
  })
# ---
# name: test_schemas[fake_chat_input_schema]
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
      dict({
        'items': dict({
          'oneOf': list([
            dict({
              '$ref': '#/definitions/AIMessage',
            }),
            dict({
              '$ref': '#/definitions/HumanMessage',
            }),
            dict({
              '$ref': '#/definitions/ChatMessage',
            }),
            dict({
              '$ref': '#/definitions/SystemMessage',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessage',
            }),
            dict({
              '$ref': '#/definitions/ToolMessage',
            }),
            dict({
              '$ref': '#/definitions/AIMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/HumanMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/ChatMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/SystemMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/ToolMessageChunk',
            }),
          ]),
        }),
        'type': 'array',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          An `AIMessage` is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model and standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'description': 'Message chunk from an AI (yielded when streaming).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'chunk_position': dict({
            'anyOf': list([
              dict({
                'const': 'last',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chunk Position',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
                dict({
                  '$ref': '#/definitions/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `FunctionMessage` are an older version of the `ToolMessage` schema, and
          do not contain the `tool_call_id` field.
          
          The `tool_call_id` field is used to associate the tool call request with the
          tool call response. Useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from the user.
          
          A `HumanMessage` is a message that is passed in from a user to the model.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Instantiate a chat model and invoke it with the messages
              model = ...
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InputTokenDetails': dict({
        'description': '''
          Breakdown of input token counts.
          
          Does *not* need to sum to full input token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "cache_creation": 200,
                  "cache_read": 100,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'cache_creation': dict({
            'title': 'Cache Creation',
            'type': 'integer',
          }),
          'cache_read': dict({
            'title': 'Cache Read',
            'type': 'integer',
          }),
        }),
        'title': 'InputTokenDetails',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'extras': dict({
            'title': 'Extras',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'string',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'type',
          'id',
          'name',
          'args',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'OutputTokenDetails': dict({
        'description': '''
          Breakdown of output token counts.
          
          Does *not* need to sum to full output token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "reasoning": 200,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'reasoning': dict({
            'title': 'Reasoning',
            'type': 'integer',
          }),
        }),
        'title': 'OutputTokenDetails',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Define a chat model and invoke it with the messages
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents an AI's request to call a tool.
          
          Example:
              ```python
              {"name": "foo", "args": {"a": 1}, "id": "123"}
              ```
          
              This represents a request to call the tool named `'foo'` with arguments
              `{"a": 1}` and an identifier of `'123'`.
          
          !!! note "Factory function"
          
              `tool_call` may also be used as a factory to create a `ToolCall`. Benefits
              include:
          
              * Required arguments strictly validated at creation time
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (yielded when streaming).
          
          When merging `ToolCallChunk` objects (e.g., via `AIMessageChunk.__add__`), all
          string attributes are concatenated. Chunks are only merged if their values of
          `index` are equal and not `None`.
          
          Example:
          ```python
          left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
          right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
          
          (
              AIMessageChunk(content="", tool_call_chunks=left_chunks)
              + AIMessageChunk(content="", tool_call_chunks=right_chunks)
          ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
          ```
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `ToolMessage` objects contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          `tool_call_id` is used to associate the tool call request with the tool call
          response. Useful in situations where a chat model is able to request multiple tool
          calls in parallel.
          
          Example:
              A `ToolMessage` representing a result of `42` from a tool call with id
          
              ```python
              from langchain_core.messages import ToolMessage
          
              ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
              ```
          
          Example:
              A `ToolMessage` where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              ```python
              from langchain_core.messages import ToolMessage
          
              tool_output = {
                  "stdout": "From the graph we can see that the correlation between "
                  "x and y is ...",
                  "stderr": None,
                  "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
              }
          
              ToolMessage(
                  content=tool_output["stdout"],
                  artifact=tool_output,
                  tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
              )
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
              ```python
              {
                  "input_tokens": 350,
                  "output_tokens": 240,
                  "total_tokens": 590,
                  "input_token_details": {
                      "audio": 10,
                      "cache_creation": 200,
                      "cache_read": 100,
                  },
                  "output_token_details": {
                      "audio": 10,
                      "reasoning": 200,
                  },
              }
              ```
          
          !!! warning "Behavior changed in `langchain-core` 0.3.9"
          
              Added `input_token_details` and `output_token_details`.
          
          !!! note "LangSmith SDK"
          
              The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
              LangSmith's `UsageMetadata` has additional fields to capture cost information
              used by the LangSmith platform.
        ''',
        'properties': dict({
          'input_token_details': dict({
            '$ref': '#/definitions/InputTokenDetails',
          }),
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_token_details': dict({
            '$ref': '#/definitions/OutputTokenDetails',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'FakeListChatModelInput',
  })
# ---
# name: test_schemas[fake_chat_output_schema]
  dict({
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          An `AIMessage` is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model and standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'description': 'Message chunk from an AI (yielded when streaming).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'chunk_position': dict({
            'anyOf': list([
              dict({
                'const': 'last',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chunk Position',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `FunctionMessage` are an older version of the `ToolMessage` schema, and
          do not contain the `tool_call_id` field.
          
          The `tool_call_id` field is used to associate the tool call request with the
          tool call response. Useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from the user.
          
          A `HumanMessage` is a message that is passed in from a user to the model.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Instantiate a chat model and invoke it with the messages
              model = ...
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InputTokenDetails': dict({
        'description': '''
          Breakdown of input token counts.
          
          Does *not* need to sum to full input token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "cache_creation": 200,
                  "cache_read": 100,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'cache_creation': dict({
            'title': 'Cache Creation',
            'type': 'integer',
          }),
          'cache_read': dict({
            'title': 'Cache Read',
            'type': 'integer',
          }),
        }),
        'title': 'InputTokenDetails',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'extras': dict({
            'title': 'Extras',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'string',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'type',
          'id',
          'name',
          'args',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'OutputTokenDetails': dict({
        'description': '''
          Breakdown of output token counts.
          
          Does *not* need to sum to full output token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "reasoning": 200,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'reasoning': dict({
            'title': 'Reasoning',
            'type': 'integer',
          }),
        }),
        'title': 'OutputTokenDetails',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Define a chat model and invoke it with the messages
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents an AI's request to call a tool.
          
          Example:
              ```python
              {"name": "foo", "args": {"a": 1}, "id": "123"}
              ```
          
              This represents a request to call the tool named `'foo'` with arguments
              `{"a": 1}` and an identifier of `'123'`.
          
          !!! note "Factory function"
          
              `tool_call` may also be used as a factory to create a `ToolCall`. Benefits
              include:
          
              * Required arguments strictly validated at creation time
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (yielded when streaming).
          
          When merging `ToolCallChunk` objects (e.g., via `AIMessageChunk.__add__`), all
          string attributes are concatenated. Chunks are only merged if their values of
          `index` are equal and not `None`.
          
          Example:
          ```python
          left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
          right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
          
          (
              AIMessageChunk(content="", tool_call_chunks=left_chunks)
              + AIMessageChunk(content="", tool_call_chunks=right_chunks)
          ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
          ```
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `ToolMessage` objects contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          `tool_call_id` is used to associate the tool call request with the tool call
          response. Useful in situations where a chat model is able to request multiple tool
          calls in parallel.
          
          Example:
              A `ToolMessage` representing a result of `42` from a tool call with id
          
              ```python
              from langchain_core.messages import ToolMessage
          
              ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
              ```
          
          Example:
              A `ToolMessage` where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              ```python
              from langchain_core.messages import ToolMessage
          
              tool_output = {
                  "stdout": "From the graph we can see that the correlation between "
                  "x and y is ...",
                  "stderr": None,
                  "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
              }
          
              ToolMessage(
                  content=tool_output["stdout"],
                  artifact=tool_output,
                  tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
              )
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
              ```python
              {
                  "input_tokens": 350,
                  "output_tokens": 240,
                  "total_tokens": 590,
                  "input_token_details": {
                      "audio": 10,
                      "cache_creation": 200,
                      "cache_read": 100,
                  },
                  "output_token_details": {
                      "audio": 10,
                      "reasoning": 200,
                  },
              }
              ```
          
          !!! warning "Behavior changed in `langchain-core` 0.3.9"
          
              Added `input_token_details` and `output_token_details`.
          
          !!! note "LangSmith SDK"
          
              The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
              LangSmith's `UsageMetadata` has additional fields to capture cost information
              used by the LangSmith platform.
        ''',
        'properties': dict({
          'input_token_details': dict({
            '$ref': '#/definitions/InputTokenDetails',
          }),
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_token_details': dict({
            '$ref': '#/definitions/OutputTokenDetails',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'oneOf': list([
      dict({
        '$ref': '#/definitions/AIMessage',
      }),
      dict({
        '$ref': '#/definitions/HumanMessage',
      }),
      dict({
        '$ref': '#/definitions/ChatMessage',
      }),
      dict({
        '$ref': '#/definitions/SystemMessage',
      }),
      dict({
        '$ref': '#/definitions/FunctionMessage',
      }),
      dict({
        '$ref': '#/definitions/ToolMessage',
      }),
      dict({
        '$ref': '#/definitions/AIMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/HumanMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/ChatMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/SystemMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/FunctionMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/ToolMessageChunk',
      }),
    ]),
    'title': 'FakeListChatModelOutput',
  })
# ---
# name: test_schemas[fake_llm_input_schema]
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
      dict({
        'items': dict({
          'oneOf': list([
            dict({
              '$ref': '#/definitions/AIMessage',
            }),
            dict({
              '$ref': '#/definitions/HumanMessage',
            }),
            dict({
              '$ref': '#/definitions/ChatMessage',
            }),
            dict({
              '$ref': '#/definitions/SystemMessage',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessage',
            }),
            dict({
              '$ref': '#/definitions/ToolMessage',
            }),
            dict({
              '$ref': '#/definitions/AIMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/HumanMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/ChatMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/SystemMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/ToolMessageChunk',
            }),
          ]),
        }),
        'type': 'array',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          An `AIMessage` is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model and standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'description': 'Message chunk from an AI (yielded when streaming).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'chunk_position': dict({
            'anyOf': list([
              dict({
                'const': 'last',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chunk Position',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
                dict({
                  '$ref': '#/definitions/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `FunctionMessage` are an older version of the `ToolMessage` schema, and
          do not contain the `tool_call_id` field.
          
          The `tool_call_id` field is used to associate the tool call request with the
          tool call response. Useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from the user.
          
          A `HumanMessage` is a message that is passed in from a user to the model.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Instantiate a chat model and invoke it with the messages
              model = ...
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InputTokenDetails': dict({
        'description': '''
          Breakdown of input token counts.
          
          Does *not* need to sum to full input token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "cache_creation": 200,
                  "cache_read": 100,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'cache_creation': dict({
            'title': 'Cache Creation',
            'type': 'integer',
          }),
          'cache_read': dict({
            'title': 'Cache Read',
            'type': 'integer',
          }),
        }),
        'title': 'InputTokenDetails',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'extras': dict({
            'title': 'Extras',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'string',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'type',
          'id',
          'name',
          'args',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'OutputTokenDetails': dict({
        'description': '''
          Breakdown of output token counts.
          
          Does *not* need to sum to full output token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "reasoning": 200,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'reasoning': dict({
            'title': 'Reasoning',
            'type': 'integer',
          }),
        }),
        'title': 'OutputTokenDetails',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Define a chat model and invoke it with the messages
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents an AI's request to call a tool.
          
          Example:
              ```python
              {"name": "foo", "args": {"a": 1}, "id": "123"}
              ```
          
              This represents a request to call the tool named `'foo'` with arguments
              `{"a": 1}` and an identifier of `'123'`.
          
          !!! note "Factory function"
          
              `tool_call` may also be used as a factory to create a `ToolCall`. Benefits
              include:
          
              * Required arguments strictly validated at creation time
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (yielded when streaming).
          
          When merging `ToolCallChunk` objects (e.g., via `AIMessageChunk.__add__`), all
          string attributes are concatenated. Chunks are only merged if their values of
          `index` are equal and not `None`.
          
          Example:
          ```python
          left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
          right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
          
          (
              AIMessageChunk(content="", tool_call_chunks=left_chunks)
              + AIMessageChunk(content="", tool_call_chunks=right_chunks)
          ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
          ```
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `ToolMessage` objects contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          `tool_call_id` is used to associate the tool call request with the tool call
          response. Useful in situations where a chat model is able to request multiple tool
          calls in parallel.
          
          Example:
              A `ToolMessage` representing a result of `42` from a tool call with id
          
              ```python
              from langchain_core.messages import ToolMessage
          
              ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
              ```
          
          Example:
              A `ToolMessage` where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              ```python
              from langchain_core.messages import ToolMessage
          
              tool_output = {
                  "stdout": "From the graph we can see that the correlation between "
                  "x and y is ...",
                  "stderr": None,
                  "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
              }
          
              ToolMessage(
                  content=tool_output["stdout"],
                  artifact=tool_output,
                  tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
              )
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
              ```python
              {
                  "input_tokens": 350,
                  "output_tokens": 240,
                  "total_tokens": 590,
                  "input_token_details": {
                      "audio": 10,
                      "cache_creation": 200,
                      "cache_read": 100,
                  },
                  "output_token_details": {
                      "audio": 10,
                      "reasoning": 200,
                  },
              }
              ```
          
          !!! warning "Behavior changed in `langchain-core` 0.3.9"
          
              Added `input_token_details` and `output_token_details`.
          
          !!! note "LangSmith SDK"
          
              The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
              LangSmith's `UsageMetadata` has additional fields to capture cost information
              used by the LangSmith platform.
        ''',
        'properties': dict({
          'input_token_details': dict({
            '$ref': '#/definitions/InputTokenDetails',
          }),
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_token_details': dict({
            '$ref': '#/definitions/OutputTokenDetails',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'FakeListLLMInput',
  })
# ---
# name: test_schemas[list_parser_input_schema]
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        'oneOf': list([
          dict({
            '$ref': '#/definitions/AIMessage',
          }),
          dict({
            '$ref': '#/definitions/HumanMessage',
          }),
          dict({
            '$ref': '#/definitions/ChatMessage',
          }),
          dict({
            '$ref': '#/definitions/SystemMessage',
          }),
          dict({
            '$ref': '#/definitions/FunctionMessage',
          }),
          dict({
            '$ref': '#/definitions/ToolMessage',
          }),
          dict({
            '$ref': '#/definitions/AIMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/HumanMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/ChatMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/SystemMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/FunctionMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/ToolMessageChunk',
          }),
        ]),
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          An `AIMessage` is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model and standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'description': 'Message chunk from an AI (yielded when streaming).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'chunk_position': dict({
            'anyOf': list([
              dict({
                'const': 'last',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chunk Position',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `FunctionMessage` are an older version of the `ToolMessage` schema, and
          do not contain the `tool_call_id` field.
          
          The `tool_call_id` field is used to associate the tool call request with the
          tool call response. Useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from the user.
          
          A `HumanMessage` is a message that is passed in from a user to the model.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Instantiate a chat model and invoke it with the messages
              model = ...
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InputTokenDetails': dict({
        'description': '''
          Breakdown of input token counts.
          
          Does *not* need to sum to full input token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "cache_creation": 200,
                  "cache_read": 100,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'cache_creation': dict({
            'title': 'Cache Creation',
            'type': 'integer',
          }),
          'cache_read': dict({
            'title': 'Cache Read',
            'type': 'integer',
          }),
        }),
        'title': 'InputTokenDetails',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'extras': dict({
            'title': 'Extras',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'string',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'type',
          'id',
          'name',
          'args',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'OutputTokenDetails': dict({
        'description': '''
          Breakdown of output token counts.
          
          Does *not* need to sum to full output token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "reasoning": 200,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'reasoning': dict({
            'title': 'Reasoning',
            'type': 'integer',
          }),
        }),
        'title': 'OutputTokenDetails',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Define a chat model and invoke it with the messages
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents an AI's request to call a tool.
          
          Example:
              ```python
              {"name": "foo", "args": {"a": 1}, "id": "123"}
              ```
          
              This represents a request to call the tool named `'foo'` with arguments
              `{"a": 1}` and an identifier of `'123'`.
          
          !!! note "Factory function"
          
              `tool_call` may also be used as a factory to create a `ToolCall`. Benefits
              include:
          
              * Required arguments strictly validated at creation time
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (yielded when streaming).
          
          When merging `ToolCallChunk` objects (e.g., via `AIMessageChunk.__add__`), all
          string attributes are concatenated. Chunks are only merged if their values of
          `index` are equal and not `None`.
          
          Example:
          ```python
          left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
          right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
          
          (
              AIMessageChunk(content="", tool_call_chunks=left_chunks)
              + AIMessageChunk(content="", tool_call_chunks=right_chunks)
          ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
          ```
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `ToolMessage` objects contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          `tool_call_id` is used to associate the tool call request with the tool call
          response. Useful in situations where a chat model is able to request multiple tool
          calls in parallel.
          
          Example:
              A `ToolMessage` representing a result of `42` from a tool call with id
          
              ```python
              from langchain_core.messages import ToolMessage
          
              ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
              ```
          
          Example:
              A `ToolMessage` where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              ```python
              from langchain_core.messages import ToolMessage
          
              tool_output = {
                  "stdout": "From the graph we can see that the correlation between "
                  "x and y is ...",
                  "stderr": None,
                  "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
              }
          
              ToolMessage(
                  content=tool_output["stdout"],
                  artifact=tool_output,
                  tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
              )
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
              ```python
              {
                  "input_tokens": 350,
                  "output_tokens": 240,
                  "total_tokens": 590,
                  "input_token_details": {
                      "audio": 10,
                      "cache_creation": 200,
                      "cache_read": 100,
                  },
                  "output_token_details": {
                      "audio": 10,
                      "reasoning": 200,
                  },
              }
              ```
          
          !!! warning "Behavior changed in `langchain-core` 0.3.9"
          
              Added `input_token_details` and `output_token_details`.
          
          !!! note "LangSmith SDK"
          
              The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
              LangSmith's `UsageMetadata` has additional fields to capture cost information
              used by the LangSmith platform.
        ''',
        'properties': dict({
          'input_token_details': dict({
            '$ref': '#/definitions/InputTokenDetails',
          }),
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_token_details': dict({
            '$ref': '#/definitions/OutputTokenDetails',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'CommaSeparatedListOutputParserInput',
  })
# ---
# name: test_schemas[prompt_mapper_output_schema]
  dict({
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          An `AIMessage` is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model and standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'description': 'Message chunk from an AI (yielded when streaming).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'chunk_position': dict({
            'anyOf': list([
              dict({
                'const': 'last',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chunk Position',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
                dict({
                  '$ref': '#/definitions/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `FunctionMessage` are an older version of the `ToolMessage` schema, and
          do not contain the `tool_call_id` field.
          
          The `tool_call_id` field is used to associate the tool call request with the
          tool call response. Useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from the user.
          
          A `HumanMessage` is a message that is passed in from a user to the model.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Instantiate a chat model and invoke it with the messages
              model = ...
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InputTokenDetails': dict({
        'description': '''
          Breakdown of input token counts.
          
          Does *not* need to sum to full input token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "cache_creation": 200,
                  "cache_read": 100,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'cache_creation': dict({
            'title': 'Cache Creation',
            'type': 'integer',
          }),
          'cache_read': dict({
            'title': 'Cache Read',
            'type': 'integer',
          }),
        }),
        'title': 'InputTokenDetails',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'extras': dict({
            'title': 'Extras',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'string',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'type',
          'id',
          'name',
          'args',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'OutputTokenDetails': dict({
        'description': '''
          Breakdown of output token counts.
          
          Does *not* need to sum to full output token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "reasoning": 200,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'reasoning': dict({
            'title': 'Reasoning',
            'type': 'integer',
          }),
        }),
        'title': 'OutputTokenDetails',
        'type': 'object',
      }),
      'PromptTemplateOutput': dict({
        'anyOf': list([
          dict({
            '$ref': '#/definitions/StringPromptValue',
          }),
          dict({
            '$ref': '#/definitions/ChatPromptValueConcrete',
          }),
        ]),
        'title': 'PromptTemplateOutput',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Define a chat model and invoke it with the messages
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents an AI's request to call a tool.
          
          Example:
              ```python
              {"name": "foo", "args": {"a": 1}, "id": "123"}
              ```
          
              This represents a request to call the tool named `'foo'` with arguments
              `{"a": 1}` and an identifier of `'123'`.
          
          !!! note "Factory function"
          
              `tool_call` may also be used as a factory to create a `ToolCall`. Benefits
              include:
          
              * Required arguments strictly validated at creation time
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (yielded when streaming).
          
          When merging `ToolCallChunk` objects (e.g., via `AIMessageChunk.__add__`), all
          string attributes are concatenated. Chunks are only merged if their values of
          `index` are equal and not `None`.
          
          Example:
          ```python
          left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
          right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
          
          (
              AIMessageChunk(content="", tool_call_chunks=left_chunks)
              + AIMessageChunk(content="", tool_call_chunks=right_chunks)
          ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
          ```
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `ToolMessage` objects contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          `tool_call_id` is used to associate the tool call request with the tool call
          response. Useful in situations where a chat model is able to request multiple tool
          calls in parallel.
          
          Example:
              A `ToolMessage` representing a result of `42` from a tool call with id
          
              ```python
              from langchain_core.messages import ToolMessage
          
              ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
              ```
          
          Example:
              A `ToolMessage` where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              ```python
              from langchain_core.messages import ToolMessage
          
              tool_output = {
                  "stdout": "From the graph we can see that the correlation between "
                  "x and y is ...",
                  "stderr": None,
                  "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
              }
          
              ToolMessage(
                  content=tool_output["stdout"],
                  artifact=tool_output,
                  tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
              )
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
              ```python
              {
                  "input_tokens": 350,
                  "output_tokens": 240,
                  "total_tokens": 590,
                  "input_token_details": {
                      "audio": 10,
                      "cache_creation": 200,
                      "cache_read": 100,
                  },
                  "output_token_details": {
                      "audio": 10,
                      "reasoning": 200,
                  },
              }
              ```
          
          !!! warning "Behavior changed in `langchain-core` 0.3.9"
          
              Added `input_token_details` and `output_token_details`.
          
          !!! note "LangSmith SDK"
          
              The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
              LangSmith's `UsageMetadata` has additional fields to capture cost information
              used by the LangSmith platform.
        ''',
        'properties': dict({
          'input_token_details': dict({
            '$ref': '#/definitions/InputTokenDetails',
          }),
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_token_details': dict({
            '$ref': '#/definitions/OutputTokenDetails',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'items': dict({
      '$ref': '#/definitions/PromptTemplateOutput',
    }),
    'title': 'RunnableEach<PromptTemplate>Output',
    'type': 'array',
  })
# ---
# name: test_schemas[prompt_output_schema]
  dict({
    'anyOf': list([
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          An `AIMessage` is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model and standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'description': 'Message chunk from an AI (yielded when streaming).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'chunk_position': dict({
            'anyOf': list([
              dict({
                'const': 'last',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chunk Position',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
                dict({
                  '$ref': '#/definitions/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `FunctionMessage` are an older version of the `ToolMessage` schema, and
          do not contain the `tool_call_id` field.
          
          The `tool_call_id` field is used to associate the tool call request with the
          tool call response. Useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from the user.
          
          A `HumanMessage` is a message that is passed in from a user to the model.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Instantiate a chat model and invoke it with the messages
              model = ...
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InputTokenDetails': dict({
        'description': '''
          Breakdown of input token counts.
          
          Does *not* need to sum to full input token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "cache_creation": 200,
                  "cache_read": 100,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'cache_creation': dict({
            'title': 'Cache Creation',
            'type': 'integer',
          }),
          'cache_read': dict({
            'title': 'Cache Read',
            'type': 'integer',
          }),
        }),
        'title': 'InputTokenDetails',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'extras': dict({
            'title': 'Extras',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'string',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'type',
          'id',
          'name',
          'args',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'OutputTokenDetails': dict({
        'description': '''
          Breakdown of output token counts.
          
          Does *not* need to sum to full output token count. Does *not* need to have all keys.
          
          Example:
              ```python
              {
                  "audio": 10,
                  "reasoning": 200,
              }
              ```
          
          May also hold extra provider-specific keys.
          
          !!! version-added "Added in `langchain-core` 0.3.9"
        ''',
        'properties': dict({
          'audio': dict({
            'title': 'Audio',
            'type': 'integer',
          }),
          'reasoning': dict({
            'title': 'Reasoning',
            'type': 'integer',
          }),
        }),
        'title': 'OutputTokenDetails',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
              ```python
              from langchain_core.messages import HumanMessage, SystemMessage
          
              messages = [
                  SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                  HumanMessage(content="What is your name?"),
              ]
          
              # Define a chat model and invoke it with the messages
              print(model.invoke(messages))
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents an AI's request to call a tool.
          
          Example:
              ```python
              {"name": "foo", "args": {"a": 1}, "id": "123"}
              ```
          
              This represents a request to call the tool named `'foo'` with arguments
              `{"a": 1}` and an identifier of `'123'`.
          
          !!! note "Factory function"
          
              `tool_call` may also be used as a factory to create a `ToolCall`. Benefits
              include:
          
              * Required arguments strictly validated at creation time
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (yielded when streaming).
          
          When merging `ToolCallChunk` objects (e.g., via `AIMessageChunk.__add__`), all
          string attributes are concatenated. Chunks are only merged if their values of
          `index` are equal and not `None`.
          
          Example:
          ```python
          left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
          right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
          
          (
              AIMessageChunk(content="", tool_call_chunks=left_chunks)
              + AIMessageChunk(content="", tool_call_chunks=right_chunks)
          ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
          ```
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          `ToolMessage` objects contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          `tool_call_id` is used to associate the tool call request with the tool call
          response. Useful in situations where a chat model is able to request multiple tool
          calls in parallel.
          
          Example:
              A `ToolMessage` representing a result of `42` from a tool call with id
          
              ```python
              from langchain_core.messages import ToolMessage
          
              ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
              ```
          
          Example:
              A `ToolMessage` where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              ```python
              from langchain_core.messages import ToolMessage
          
              tool_output = {
                  "stdout": "From the graph we can see that the correlation between "
                  "x and y is ...",
                  "stderr": None,
                  "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
              }
          
              ToolMessage(
                  content=tool_output["stdout"],
                  artifact=tool_output,
                  tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
              )
              ```
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
              ```python
              {
                  "input_tokens": 350,
                  "output_tokens": 240,
                  "total_tokens": 590,
                  "input_token_details": {
                      "audio": 10,
                      "cache_creation": 200,
                      "cache_read": 100,
                  },
                  "output_token_details": {
                      "audio": 10,
                      "reasoning": 200,
                  },
              }
              ```
          
          !!! warning "Behavior changed in `langchain-core` 0.3.9"
          
              Added `input_token_details` and `output_token_details`.
          
          !!! note "LangSmith SDK"
          
              The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,
              LangSmith's `UsageMetadata` has additional fields to capture cost information
              used by the LangSmith platform.
        ''',
        'properties': dict({
          'input_token_details': dict({
            '$ref': '#/definitions/InputTokenDetails',
          }),
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_token_details': dict({
            '$ref': '#/definitions/OutputTokenDetails',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'PromptTemplateOutput',
  })
# ---
# name: test_seq_dict_prompt_llm
  '''
  {
    question: RunnablePassthrough[str]()
              | RunnableLambda(...),
    documents: RunnableLambda(...)
               | FakeRetriever(),
    just_to_test_lambda: RunnableLambda(...)
  }
  | ChatPromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='Context:\n{documents}\n\nQuestion:\n{question}'), additional_kwargs={})])
  | FakeListChatModel(responses=['foo, bar'])
  | CommaSeparatedListOutputParser()
  '''
# ---
# name: test_seq_dict_prompt_llm.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "question": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "schema",
                    "runnable",
                    "RunnablePassthrough"
                  ],
                  "kwargs": {},
                  "name": "RunnablePassthrough"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "runnables",
                    "base",
                    "RunnableLambda"
                  ],
                  "repr": "RunnableLambda(...)"
                }
              },
              "name": "RunnableSequence"
            },
            "documents": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "runnables",
                    "base",
                    "RunnableLambda"
                  ],
                  "repr": "RunnableLambda(...)"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "tests",
                    "unit_tests",
                    "runnables",
                    "test_runnable",
                    "FakeRetriever"
                  ],
                  "repr": "FakeRetriever()",
                  "name": "FakeRetriever"
                }
              },
              "name": "RunnableSequence"
            },
            "just_to_test_lambda": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            }
          }
        },
        "name": "RunnableParallel<question,documents,just_to_test_lambda>"
      },
      "middle": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "documents",
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nice assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "documents",
                        "question"
                      ],
                      "template": "Context:\n{documents}\n\nQuestion:\n{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate"
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_seq_prompt_dict
  '''
  ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])
  | RunnableLambda(...)
  | {
      chat: FakeListChatModel(responses=["i'm a chatbot"]),
      llm: FakeListLLM(responses=["i'm a textbot"])
    }
  '''
# ---
# name: test_seq_prompt_dict.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(...)"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "chat": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake_chat_models",
                "FakeListChatModel"
              ],
              "repr": "FakeListChatModel(responses=[\"i'm a chatbot\"])",
              "name": "FakeListChatModel"
            },
            "llm": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake",
                "FakeListLLM"
              ],
              "repr": "FakeListLLM(responses=[\"i'm a textbot\"])",
              "name": "FakeListLLM"
            }
          }
        },
        "name": "RunnableParallel<chat,llm>"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_seq_prompt_map
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(...)"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "chat": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableBinding"
              ],
              "kwargs": {
                "bound": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "repr": "FakeListChatModel(responses=[\"i'm a chatbot\"])",
                  "name": "FakeListChatModel"
                },
                "kwargs": {
                  "stop": [
                    "Thought:"
                  ]
                },
                "config": {}
              },
              "name": "FakeListChatModel"
            },
            "llm": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake",
                "FakeListLLM"
              ],
              "repr": "FakeListLLM(responses=[\"i'm a textbot\"])",
              "name": "FakeListLLM"
            },
            "passthrough": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            }
          }
        },
        "name": "RunnableParallel<chat,llm,passthrough>"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
