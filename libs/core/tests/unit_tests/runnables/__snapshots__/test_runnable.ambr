# serializer version: 1
# name: test_combining_sequences
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeListChatModelInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "name": "FakeListChatModel"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeListChatModelOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "output_parsers",
                  "list",
                  "CommaSeparatedListOutputParser"
                ],
                "name": "CommaSeparatedListOutputParser"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "output_parsers",
              "list",
              "CommaSeparatedListOutputParser"
            ],
            "name": "CommaSeparatedListOutputParser"
          }
        },
        {
          "id": 4,
          "type": "schema",
          "data": "CommaSeparatedListOutputParserOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 3,
          "target": 4
        },
        {
          "source": 2,
          "target": 3
        }
      ]
    }
  }
  '''
# ---
# name: test_combining_sequences.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(lambda x: {'question': x[0] + x[1]})"
      },
      "middle": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nicer assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate",
                    "graph": {
                      "nodes": [
                        {
                          "id": 0,
                          "type": "schema",
                          "data": "PromptInput"
                        },
                        {
                          "id": 1,
                          "type": "runnable",
                          "data": {
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "name": "PromptTemplate"
                          }
                        },
                        {
                          "id": 2,
                          "type": "schema",
                          "data": "PromptTemplateOutput"
                        }
                      ],
                      "edges": [
                        {
                          "source": 0,
                          "target": 1
                        },
                        {
                          "source": 1,
                          "target": 2
                        }
                      ]
                    }
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "question"
                      ],
                      "template": "{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate",
                    "graph": {
                      "nodes": [
                        {
                          "id": 0,
                          "type": "schema",
                          "data": "PromptInput"
                        },
                        {
                          "id": 1,
                          "type": "runnable",
                          "data": {
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "name": "PromptTemplate"
                          }
                        },
                        {
                          "id": 2,
                          "type": "schema",
                          "data": "PromptTemplateOutput"
                        }
                      ],
                      "edges": [
                        {
                          "source": 0,
                          "target": 1
                        },
                        {
                          "source": 1,
                          "target": 2
                        }
                      ]
                    }
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "PromptInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "name": "ChatPromptTemplate"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "ChatPromptTemplateOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['baz, qux'])",
          "name": "FakeListChatModel",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeListChatModelInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "name": "FakeListChatModel"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeListChatModelOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "output_parsers",
                  "list",
                  "CommaSeparatedListOutputParser"
                ],
                "name": "CommaSeparatedListOutputParser"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "LambdaInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 4,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "output_parsers",
              "list",
              "CommaSeparatedListOutputParser"
            ],
            "name": "CommaSeparatedListOutputParser"
          }
        },
        {
          "id": 5,
          "type": "schema",
          "data": "CommaSeparatedListOutputParserOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 2,
          "target": 3
        },
        {
          "source": 4,
          "target": 5
        },
        {
          "source": 3,
          "target": 4
        }
      ]
    }
  }
  '''
# ---
# name: test_combining_sequences.2
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeListChatModelInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "name": "FakeListChatModel"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeListChatModelOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "output_parsers",
            "list",
            "CommaSeparatedListOutputParser"
          ],
          "kwargs": {},
          "name": "CommaSeparatedListOutputParser",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "CommaSeparatedListOutputParserInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain",
                    "output_parsers",
                    "list",
                    "CommaSeparatedListOutputParser"
                  ],
                  "name": "CommaSeparatedListOutputParser"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "CommaSeparatedListOutputParserOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(lambda x: {'question': x[0] + x[1]})"
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nicer assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate",
                    "graph": {
                      "nodes": [
                        {
                          "id": 0,
                          "type": "schema",
                          "data": "PromptInput"
                        },
                        {
                          "id": 1,
                          "type": "runnable",
                          "data": {
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "name": "PromptTemplate"
                          }
                        },
                        {
                          "id": 2,
                          "type": "schema",
                          "data": "PromptTemplateOutput"
                        }
                      ],
                      "edges": [
                        {
                          "source": 0,
                          "target": 1
                        },
                        {
                          "source": 1,
                          "target": 2
                        }
                      ]
                    }
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "question"
                      ],
                      "template": "{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate",
                    "graph": {
                      "nodes": [
                        {
                          "id": 0,
                          "type": "schema",
                          "data": "PromptInput"
                        },
                        {
                          "id": 1,
                          "type": "runnable",
                          "data": {
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "name": "PromptTemplate"
                          }
                        },
                        {
                          "id": 2,
                          "type": "schema",
                          "data": "PromptTemplateOutput"
                        }
                      ],
                      "edges": [
                        {
                          "source": 0,
                          "target": 1
                        },
                        {
                          "source": 1,
                          "target": 2
                        }
                      ]
                    }
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "PromptInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "name": "ChatPromptTemplate"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "ChatPromptTemplateOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['baz, qux'])",
          "name": "FakeListChatModel",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeListChatModelInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "name": "FakeListChatModel"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeListChatModelOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "output_parsers",
                  "list",
                  "CommaSeparatedListOutputParser"
                ],
                "name": "CommaSeparatedListOutputParser"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "output_parsers",
              "list",
              "CommaSeparatedListOutputParser"
            ],
            "name": "CommaSeparatedListOutputParser"
          }
        },
        {
          "id": 4,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 5,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 6,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 7,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "output_parsers",
              "list",
              "CommaSeparatedListOutputParser"
            ],
            "name": "CommaSeparatedListOutputParser"
          }
        },
        {
          "id": 8,
          "type": "schema",
          "data": "CommaSeparatedListOutputParserOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 2,
          "target": 3
        },
        {
          "source": 3,
          "target": 4
        },
        {
          "source": 4,
          "target": 5
        },
        {
          "source": 5,
          "target": 6
        },
        {
          "source": 7,
          "target": 8
        },
        {
          "source": 6,
          "target": 7
        }
      ]
    }
  }
  '''
# ---
# name: test_combining_sequences.3
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'middle': [{'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo, bar'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': "RunnableLambda(lambda x: {'question': x[0] + x[1]})"}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nicer assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['baz, qux'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'Lambda'}}, {'id': 5, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 6, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 8, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}, {'source': 2, 'target': 3}, {'source': 3, 'target': 4}, {'source': 4, 'target': 5}, {'source': 5, 'target': 6}, {'source': 7, 'target': 8}, {'source': 6, 'target': 7}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ['baz', 'qux']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo, bar'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo, bar'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo, bar', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'foo, bar', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000002-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': AIMessage(content='foo, bar', id='00000000-0000-4000-8000-000000000004')}, outputs={'output': ['foo', 'bar']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003'), Run(id=UUID('00000000-0000-4000-8000-000000000005'), name='RunnableLambda', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': "RunnableLambda(lambda x: {'question': x[0] + x[1]})"}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': ['foo', 'bar']}, outputs={'question': 'foobar'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:4'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000005'), Run(id=UUID('00000000-0000-4000-8000-000000000006'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nicer assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'foobar'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nicer assistant.'), HumanMessage(content='foobar')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:5'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000006'), Run(id=UUID('00000000-0000-4000-8000-000000000007'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['baz, qux'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['baz, qux'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nicer assistant.\nHuman: foobar']}, outputs={'generations': [[{'text': 'baz, qux', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'baz, qux', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000006-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:6'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000007'), Run(id=UUID('00000000-0000-4000-8000-000000000008'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': AIMessage(content='baz, qux', id='00000000-0000-4000-8000-000000000009')}, outputs={'output': ['baz', 'qux']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:7'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000008')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_each
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeStreamingListLLM"
          ],
          "repr": "FakeStreamingListLLM(responses=['first item, second item, third item'])",
          "name": "FakeStreamingListLLM",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeStreamingListLLMInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeStreamingListLLM"
                  ],
                  "name": "FakeStreamingListLLM"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeStreamingListLLMOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "tests",
            "unit_tests",
            "runnables",
            "test_runnable",
            "FakeSplitIntoListParser"
          ],
          "kwargs": {},
          "name": "FakeSplitIntoListParser",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeSplitIntoListParserInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "tests",
                    "unit_tests",
                    "runnables",
                    "test_runnable",
                    "FakeSplitIntoListParser"
                  ],
                  "name": "FakeSplitIntoListParser"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeSplitIntoListParserOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableEach"
        ],
        "kwargs": {
          "bound": {
            "lc": 1,
            "type": "not_implemented",
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeStreamingListLLM"
            ],
            "repr": "FakeStreamingListLLM(responses=['this', 'is', 'a', 'test'])",
            "name": "FakeStreamingListLLM",
            "graph": {
              "nodes": [
                {
                  "id": 0,
                  "type": "schema",
                  "data": "FakeStreamingListLLMInput"
                },
                {
                  "id": 1,
                  "type": "runnable",
                  "data": {
                    "id": [
                      "langchain_core",
                      "language_models",
                      "fake",
                      "FakeStreamingListLLM"
                    ],
                    "name": "FakeStreamingListLLM"
                  }
                },
                {
                  "id": 2,
                  "type": "schema",
                  "data": "FakeStreamingListLLMOutput"
                }
              ],
              "edges": [
                {
                  "source": 0,
                  "target": 1
                },
                {
                  "source": 1,
                  "target": 2
                }
              ]
            }
          }
        },
        "name": "RunnableEach<FakeStreamingListLLM>",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "FakeStreamingListLLMInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "language_models",
                  "fake",
                  "FakeStreamingListLLM"
                ],
                "name": "FakeStreamingListLLM"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "FakeStreamingListLLMOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeStreamingListLLM"
            ],
            "name": "FakeStreamingListLLM"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "tests",
              "unit_tests",
              "runnables",
              "test_runnable",
              "FakeSplitIntoListParser"
            ],
            "name": "FakeSplitIntoListParser"
          }
        },
        {
          "id": 4,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeStreamingListLLM"
            ],
            "name": "FakeStreamingListLLM"
          }
        },
        {
          "id": 5,
          "type": "schema",
          "data": "FakeStreamingListLLMOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 2,
          "target": 3
        },
        {
          "source": 4,
          "target": 5
        },
        {
          "source": 3,
          "target": 4
        }
      ]
    }
  }
  '''
# ---
# name: test_higher_order_lambda_runnable
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "key": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(lambda x: x['key'])"
            },
            "input": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableParallel"
              ],
              "kwargs": {
                "steps__": {
                  "question": {
                    "lc": 1,
                    "type": "not_implemented",
                    "id": [
                      "langchain_core",
                      "runnables",
                      "base",
                      "RunnableLambda"
                    ],
                    "repr": "RunnableLambda(lambda x: x['question'])"
                  }
                }
              },
              "name": "RunnableParallel<question>",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "Parallel<question>Input"
                  },
                  {
                    "id": 1,
                    "type": "schema",
                    "data": "Parallel<question>Output"
                  },
                  {
                    "id": 2,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "runnables",
                        "base",
                        "RunnableLambda"
                      ],
                      "name": "Lambda"
                    }
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 2
                  },
                  {
                    "source": 2,
                    "target": 1
                  }
                ]
              }
            }
          }
        },
        "name": "RunnableParallel<key,input>",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "Parallel<key,input>Input"
            },
            {
              "id": 1,
              "type": "schema",
              "data": "Parallel<key,input>Output"
            },
            {
              "id": 2,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "runnables",
                  "base",
                  "RunnableLambda"
                ],
                "name": "Lambda"
              }
            },
            {
              "id": 3,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "runnables",
                  "base",
                  "RunnableLambda"
                ],
                "name": "Lambda"
              }
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 2
            },
            {
              "source": 2,
              "target": 1
            },
            {
              "source": 0,
              "target": 3
            },
            {
              "source": 3,
              "target": 1
            }
          ]
        }
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(router)"
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "Parallel<key,input>Input"
        },
        {
          "id": 1,
          "type": "schema",
          "data": "Parallel<key,input>Output"
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 4,
          "type": "schema",
          "data": "router_input"
        },
        {
          "id": 5,
          "type": "schema",
          "data": "router_output"
        },
        {
          "id": 6,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 7,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeListLLM"
            ],
            "name": "FakeListLLM"
          }
        },
        {
          "id": 8,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 9,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeListLLM"
            ],
            "name": "FakeListLLM"
          }
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 2
        },
        {
          "source": 2,
          "target": 1
        },
        {
          "source": 0,
          "target": 3
        },
        {
          "source": 3,
          "target": 1
        },
        {
          "source": 6,
          "target": 7
        },
        {
          "source": 4,
          "target": 6
        },
        {
          "source": 7,
          "target": 5
        },
        {
          "source": 8,
          "target": 9
        },
        {
          "source": 4,
          "target": 8
        },
        {
          "source": 9,
          "target": 5
        },
        {
          "source": 1,
          "target": 4
        }
      ]
    }
  }
  '''
# ---
# name: test_prompt_with_chat_model
  '''
  ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a nice assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])
  | FakeListChatModel(responses=['foo'])
  '''
# ---
# name: test_prompt_with_chat_model.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake_chat_models",
          "FakeListChatModel"
        ],
        "repr": "FakeListChatModel(responses=['foo'])",
        "name": "FakeListChatModel",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "FakeListChatModelInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "language_models",
                  "fake_chat_models",
                  "FakeListChatModel"
                ],
                "name": "FakeListChatModel"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "FakeListChatModelOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 3,
          "type": "schema",
          "data": "FakeListChatModelOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 2,
          "target": 3
        },
        {
          "source": 1,
          "target": 2
        }
      ]
    }
  }
  '''
# ---
# name: test_prompt_with_chat_model.2
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 3, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': AIMessage(content='foo', id='00000000-0000-4000-8000-000000000003')}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'foo', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000002-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_chat_model_and_parser
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeListChatModelInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "name": "FakeListChatModel"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeListChatModelOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "output_parsers",
                  "list",
                  "CommaSeparatedListOutputParser"
                ],
                "name": "CommaSeparatedListOutputParser"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "output_parsers",
              "list",
              "CommaSeparatedListOutputParser"
            ],
            "name": "CommaSeparatedListOutputParser"
          }
        },
        {
          "id": 4,
          "type": "schema",
          "data": "CommaSeparatedListOutputParserOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 3,
          "target": 4
        },
        {
          "source": 2,
          "target": 3
        }
      ]
    }
  }
  '''
# ---
# name: test_prompt_with_chat_model_and_parser.1
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'middle': [{'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo, bar'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 4, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}, {'source': 3, 'target': 4}, {'source': 2, 'target': 3}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ['foo', 'bar']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo, bar'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo, bar'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo, bar', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'foo, bar', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000002-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': AIMessage(content='foo, bar', id='00000000-0000-4000-8000-000000000004')}, outputs={'output': ['foo', 'bar']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_chat_model_async
  '''
  ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a nice assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])
  | FakeListChatModel(responses=['foo'])
  '''
# ---
# name: test_prompt_with_chat_model_async.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake_chat_models",
          "FakeListChatModel"
        ],
        "repr": "FakeListChatModel(responses=['foo'])",
        "name": "FakeListChatModel",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "FakeListChatModelInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "language_models",
                  "fake_chat_models",
                  "FakeListChatModel"
                ],
                "name": "FakeListChatModel"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "FakeListChatModelOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 3,
          "type": "schema",
          "data": "FakeListChatModelOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 2,
          "target": 3
        },
        {
          "source": 1,
          "target": 2
        }
      ]
    }
  }
  '''
# ---
# name: test_prompt_with_chat_model_async.2
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 3, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': AIMessage(content='foo', id='00000000-0000-4000-8000-000000000003')}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo'])", 'name': 'FakeListChatModel', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListChatModelInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'name': 'FakeListChatModel'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListChatModelOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'foo', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000002-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake",
          "FakeListLLM"
        ],
        "repr": "FakeListLLM(responses=['foo', 'bar'])",
        "name": "FakeListLLM",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "FakeListLLMInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "language_models",
                  "fake",
                  "FakeListLLM"
                ],
                "name": "FakeListLLM"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "FakeListLLMOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeListLLM"
            ],
            "name": "FakeListLLM"
          }
        },
        {
          "id": 3,
          "type": "schema",
          "data": "FakeListLLMOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 2,
          "target": 3
        },
        {
          "source": 1,
          "target": 2
        }
      ]
    }
  }
  '''
# ---
# name: test_prompt_with_llm.1
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'])", 'name': 'FakeListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 3, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': 'foo'}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo', 'bar'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'])", 'name': 'FakeListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm.2
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'], i=1)", 'name': 'FakeListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 3, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': 'bar'}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo', 'bar'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 2}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'], i=1)", 'name': 'FakeListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'bar', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
    Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'], i=1)", 'name': 'FakeListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 3, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your favorite color?'}, outputs={'output': 'foo'}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000004'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your favorite color?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your favorite color?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000003'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000003'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000003.20230101T000000000000Z00000000-0000-4000-8000-000000000004'), Run(id=UUID('00000000-0000-4000-8000-000000000005'), name='FakeListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo', 'bar'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 2}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'], i=1)", 'name': 'FakeListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your favorite color?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000003'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000003'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000003.20230101T000000000000Z00000000-0000-4000-8000-000000000005')], trace_id=UUID('00000000-0000-4000-8000-000000000003'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000003'),
  ])
# ---
# name: test_prompt_with_llm_and_async_lambda
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeListLLM"
          ],
          "repr": "FakeListLLM(responses=['foo', 'bar'])",
          "name": "FakeListLLM",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeListLLMInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeListLLM"
                  ],
                  "name": "FakeListLLM"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeListLLMOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        }
      ],
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(afunc=passthrough)"
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeListLLM"
            ],
            "name": "FakeListLLM"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "passthrough"
          }
        },
        {
          "id": 4,
          "type": "schema",
          "data": "passthrough_output"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 3,
          "target": 4
        },
        {
          "source": 2,
          "target": 3
        }
      ]
    }
  }
  '''
# ---
# name: test_prompt_with_llm_and_async_lambda.1
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'middle': [{'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'])", 'name': 'FakeListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}], 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': 'RunnableLambda(afunc=passthrough)'}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'passthrough'}}, {'id': 4, 'type': 'schema', 'data': 'passthrough_output'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}, {'source': 3, 'target': 4}, {'source': 2, 'target': 3}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': 'foo'}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo', 'bar'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'])", 'name': 'FakeListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'name': 'FakeListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='passthrough', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': 'RunnableLambda(afunc=passthrough)'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': 'foo'}, outputs={'output': 'foo'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm_parser
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeStreamingListLLM"
          ],
          "repr": "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'])",
          "name": "FakeStreamingListLLM",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeStreamingListLLMInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeStreamingListLLM"
                  ],
                  "name": "FakeStreamingListLLM"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeStreamingListLLMOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "output_parsers",
                  "list",
                  "CommaSeparatedListOutputParser"
                ],
                "name": "CommaSeparatedListOutputParser"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeStreamingListLLM"
            ],
            "name": "FakeStreamingListLLM"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "output_parsers",
              "list",
              "CommaSeparatedListOutputParser"
            ],
            "name": "CommaSeparatedListOutputParser"
          }
        },
        {
          "id": 4,
          "type": "schema",
          "data": "CommaSeparatedListOutputParserOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 3,
          "target": 4
        },
        {
          "source": 2,
          "target": 3
        }
      ]
    }
  }
  '''
# ---
# name: test_prompt_with_llm_parser.1
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'middle': [{'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'])", 'name': 'FakeStreamingListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeStreamingListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeStreamingListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 4, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}, {'source': 3, 'target': 4}, {'source': 2, 'target': 3}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ['bear', 'dog', 'cat']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeStreamingListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['bear, dog, cat', 'tomato, lettuce, onion'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'])", 'name': 'FakeStreamingListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeStreamingListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeStreamingListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'bear, dog, cat', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': 'bear, dog, cat'}, outputs={'output': ['bear', 'dog', 'cat']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm_parser.2
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'middle': [{'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'], i=1)", 'name': 'FakeStreamingListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeStreamingListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeStreamingListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 4, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}, {'source': 3, 'target': 4}, {'source': 2, 'target': 3}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ['tomato', 'lettuce', 'onion']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your name?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeStreamingListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['bear, dog, cat', 'tomato, lettuce, onion'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 2}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'], i=1)", 'name': 'FakeStreamingListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeStreamingListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeStreamingListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'tomato, lettuce, onion', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': 'tomato, lettuce, onion'}, outputs={'output': ['tomato', 'lettuce', 'onion']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
    Run(id=UUID('00000000-0000-4000-8000-000000000004'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'middle': [{'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'], i=1)", 'name': 'FakeStreamingListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeStreamingListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeStreamingListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 4, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}, {'source': 3, 'target': 4}, {'source': 2, 'target': 3}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your favorite color?'}, outputs={'output': ['bear', 'dog', 'cat']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000005'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your favorite color?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.'), HumanMessage(content='What is your favorite color?')])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000004'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000004'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000004.20230101T000000000000Z00000000-0000-4000-8000-000000000005'), Run(id=UUID('00000000-0000-4000-8000-000000000006'), name='FakeStreamingListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['bear, dog, cat', 'tomato, lettuce, onion'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 2}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'], i=1)", 'name': 'FakeStreamingListLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'FakeStreamingListLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'name': 'FakeStreamingListLLM'}}, {'id': 2, 'type': 'schema', 'data': 'FakeStreamingListLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your favorite color?']}, outputs={'generations': [[{'text': 'bear, dog, cat', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000004'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000004'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000004.20230101T000000000000Z00000000-0000-4000-8000-000000000006'), Run(id=UUID('00000000-0000-4000-8000-000000000007'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'kwargs': {}, 'name': 'CommaSeparatedListOutputParser', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'output_parsers', 'list', 'CommaSeparatedListOutputParser'], 'name': 'CommaSeparatedListOutputParser'}}, {'id': 2, 'type': 'schema', 'data': 'CommaSeparatedListOutputParserOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': 'bear, dog, cat'}, outputs={'output': ['bear', 'dog', 'cat']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000004'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000004'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000004.20230101T000000000000Z00000000-0000-4000-8000-000000000007')], trace_id=UUID('00000000-0000-4000-8000-000000000004'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000004'),
  ])
# ---
# name: test_router_runnable
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "key": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            },
            "input": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableParallel"
              ],
              "kwargs": {
                "steps__": {
                  "question": {
                    "lc": 1,
                    "type": "not_implemented",
                    "id": [
                      "langchain_core",
                      "runnables",
                      "base",
                      "RunnableLambda"
                    ],
                    "repr": "RunnableLambda(...)"
                  }
                }
              },
              "name": "RunnableParallel<question>",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "Parallel<question>Input"
                  },
                  {
                    "id": 1,
                    "type": "schema",
                    "data": "Parallel<question>Output"
                  },
                  {
                    "id": 2,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "runnables",
                        "base",
                        "RunnableLambda"
                      ],
                      "name": "Lambda"
                    }
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 2
                  },
                  {
                    "source": 2,
                    "target": 1
                  }
                ]
              }
            }
          }
        },
        "name": "RunnableParallel<key,input>",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "Parallel<key,input>Input"
            },
            {
              "id": 1,
              "type": "schema",
              "data": "Parallel<key,input>Output"
            },
            {
              "id": 2,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "runnables",
                  "base",
                  "RunnableLambda"
                ],
                "name": "Lambda"
              }
            },
            {
              "id": 3,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "runnables",
                  "base",
                  "RunnableLambda"
                ],
                "name": "Lambda"
              }
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 2
            },
            {
              "source": 2,
              "target": 1
            },
            {
              "source": 0,
              "target": 3
            },
            {
              "source": 3,
              "target": 1
            }
          ]
        }
      },
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RouterRunnable"
        ],
        "kwargs": {
          "runnables": {
            "math": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "messages": [
                      {
                        "lc": 1,
                        "type": "constructor",
                        "id": [
                          "langchain",
                          "prompts",
                          "chat",
                          "HumanMessagePromptTemplate"
                        ],
                        "kwargs": {
                          "prompt": {
                            "lc": 1,
                            "type": "constructor",
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "kwargs": {
                              "input_variables": [
                                "question"
                              ],
                              "template": "You are a math genius. Answer the question: {question}",
                              "template_format": "f-string"
                            },
                            "name": "PromptTemplate",
                            "graph": {
                              "nodes": [
                                {
                                  "id": 0,
                                  "type": "schema",
                                  "data": "PromptInput"
                                },
                                {
                                  "id": 1,
                                  "type": "runnable",
                                  "data": {
                                    "id": [
                                      "langchain",
                                      "prompts",
                                      "prompt",
                                      "PromptTemplate"
                                    ],
                                    "name": "PromptTemplate"
                                  }
                                },
                                {
                                  "id": 2,
                                  "type": "schema",
                                  "data": "PromptTemplateOutput"
                                }
                              ],
                              "edges": [
                                {
                                  "source": 0,
                                  "target": 1
                                },
                                {
                                  "source": 1,
                                  "target": 2
                                }
                              ]
                            }
                          }
                        }
                      }
                    ]
                  },
                  "name": "ChatPromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "chat",
                            "ChatPromptTemplate"
                          ],
                          "name": "ChatPromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "ChatPromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeListLLM"
                  ],
                  "repr": "FakeListLLM(responses=['4'])",
                  "name": "FakeListLLM",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "FakeListLLMInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain_core",
                            "language_models",
                            "fake",
                            "FakeListLLM"
                          ],
                          "name": "FakeListLLM"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "FakeListLLMOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              },
              "name": "RunnableSequence",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "PromptInput"
                  },
                  {
                    "id": 1,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain",
                        "prompts",
                        "chat",
                        "ChatPromptTemplate"
                      ],
                      "name": "ChatPromptTemplate"
                    }
                  },
                  {
                    "id": 2,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "language_models",
                        "fake",
                        "FakeListLLM"
                      ],
                      "name": "FakeListLLM"
                    }
                  },
                  {
                    "id": 3,
                    "type": "schema",
                    "data": "FakeListLLMOutput"
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 1
                  },
                  {
                    "source": 2,
                    "target": 3
                  },
                  {
                    "source": 1,
                    "target": 2
                  }
                ]
              }
            },
            "english": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "messages": [
                      {
                        "lc": 1,
                        "type": "constructor",
                        "id": [
                          "langchain",
                          "prompts",
                          "chat",
                          "HumanMessagePromptTemplate"
                        ],
                        "kwargs": {
                          "prompt": {
                            "lc": 1,
                            "type": "constructor",
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "kwargs": {
                              "input_variables": [
                                "question"
                              ],
                              "template": "You are an english major. Answer the question: {question}",
                              "template_format": "f-string"
                            },
                            "name": "PromptTemplate",
                            "graph": {
                              "nodes": [
                                {
                                  "id": 0,
                                  "type": "schema",
                                  "data": "PromptInput"
                                },
                                {
                                  "id": 1,
                                  "type": "runnable",
                                  "data": {
                                    "id": [
                                      "langchain",
                                      "prompts",
                                      "prompt",
                                      "PromptTemplate"
                                    ],
                                    "name": "PromptTemplate"
                                  }
                                },
                                {
                                  "id": 2,
                                  "type": "schema",
                                  "data": "PromptTemplateOutput"
                                }
                              ],
                              "edges": [
                                {
                                  "source": 0,
                                  "target": 1
                                },
                                {
                                  "source": 1,
                                  "target": 2
                                }
                              ]
                            }
                          }
                        }
                      }
                    ]
                  },
                  "name": "ChatPromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "chat",
                            "ChatPromptTemplate"
                          ],
                          "name": "ChatPromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "ChatPromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeListLLM"
                  ],
                  "repr": "FakeListLLM(responses=['2'])",
                  "name": "FakeListLLM",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "FakeListLLMInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain_core",
                            "language_models",
                            "fake",
                            "FakeListLLM"
                          ],
                          "name": "FakeListLLM"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "FakeListLLMOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              },
              "name": "RunnableSequence",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "PromptInput"
                  },
                  {
                    "id": 1,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain",
                        "prompts",
                        "chat",
                        "ChatPromptTemplate"
                      ],
                      "name": "ChatPromptTemplate"
                    }
                  },
                  {
                    "id": 2,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "language_models",
                        "fake",
                        "FakeListLLM"
                      ],
                      "name": "FakeListLLM"
                    }
                  },
                  {
                    "id": 3,
                    "type": "schema",
                    "data": "FakeListLLMOutput"
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 1
                  },
                  {
                    "source": 2,
                    "target": 3
                  },
                  {
                    "source": 1,
                    "target": 2
                  }
                ]
              }
            }
          }
        },
        "name": "RouterRunnable",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "RouterRunnableInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "schema",
                  "runnable",
                  "RouterRunnable"
                ],
                "name": "RouterRunnable"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "RouterRunnableOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "Parallel<key,input>Input"
        },
        {
          "id": 1,
          "type": "schema",
          "data": "Parallel<key,input>Output"
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 4,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "schema",
              "runnable",
              "RouterRunnable"
            ],
            "name": "RouterRunnable"
          }
        },
        {
          "id": 5,
          "type": "schema",
          "data": "RouterRunnableOutput"
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 2
        },
        {
          "source": 2,
          "target": 1
        },
        {
          "source": 0,
          "target": 3
        },
        {
          "source": 3,
          "target": 1
        },
        {
          "source": 4,
          "target": 5
        },
        {
          "source": 1,
          "target": 4
        }
      ]
    }
  }
  '''
# ---
# name: test_schemas
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
      dict({
        'items': dict({
          'anyOf': list([
            dict({
              '$ref': '#/definitions/AIMessage',
            }),
            dict({
              '$ref': '#/definitions/HumanMessage',
            }),
            dict({
              '$ref': '#/definitions/ChatMessage',
            }),
            dict({
              '$ref': '#/definitions/SystemMessage',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessage',
            }),
            dict({
              '$ref': '#/definitions/ToolMessage',
            }),
          ]),
        }),
        'type': 'array',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ai',
            'enum': list([
              'ai',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
          'usage_metadata': dict({
            '$ref': '#/definitions/UsageMetadata',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'default': 'chat',
            'enum': list([
              'chat',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'anyOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ChatPromptValueConcrete',
            'enum': list([
              'ChatPromptValueConcrete',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'function',
            'enum': list([
              'function',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'human',
            'enum': list([
              'human',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'string',
          }),
          'error': dict({
            'title': 'Error',
            'type': 'string',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'invalid_tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'default': 'StringPromptValue',
            'enum': list([
              'StringPromptValue',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'system',
            'enum': list([
              'system',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'ToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to raw_output.
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      raw_output=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'raw_output': dict({
            'title': 'Raw Output',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'default': 'tool',
            'enum': list([
              'tool',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'FakeListLLMInput',
  })
# ---
# name: test_schemas.1
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
      dict({
        'items': dict({
          'anyOf': list([
            dict({
              '$ref': '#/definitions/AIMessage',
            }),
            dict({
              '$ref': '#/definitions/HumanMessage',
            }),
            dict({
              '$ref': '#/definitions/ChatMessage',
            }),
            dict({
              '$ref': '#/definitions/SystemMessage',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessage',
            }),
            dict({
              '$ref': '#/definitions/ToolMessage',
            }),
          ]),
        }),
        'type': 'array',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ai',
            'enum': list([
              'ai',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
          'usage_metadata': dict({
            '$ref': '#/definitions/UsageMetadata',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'default': 'chat',
            'enum': list([
              'chat',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'anyOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ChatPromptValueConcrete',
            'enum': list([
              'ChatPromptValueConcrete',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'function',
            'enum': list([
              'function',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'human',
            'enum': list([
              'human',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'string',
          }),
          'error': dict({
            'title': 'Error',
            'type': 'string',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'invalid_tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'default': 'StringPromptValue',
            'enum': list([
              'StringPromptValue',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'system',
            'enum': list([
              'system',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'ToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to raw_output.
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      raw_output=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'raw_output': dict({
            'title': 'Raw Output',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'default': 'tool',
            'enum': list([
              'tool',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'FakeListChatModelInput',
  })
# ---
# name: test_schemas.2
  dict({
    'anyOf': list([
      dict({
        '$ref': '#/definitions/AIMessage',
      }),
      dict({
        '$ref': '#/definitions/HumanMessage',
      }),
      dict({
        '$ref': '#/definitions/ChatMessage',
      }),
      dict({
        '$ref': '#/definitions/SystemMessage',
      }),
      dict({
        '$ref': '#/definitions/FunctionMessage',
      }),
      dict({
        '$ref': '#/definitions/ToolMessage',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ai',
            'enum': list([
              'ai',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
          'usage_metadata': dict({
            '$ref': '#/definitions/UsageMetadata',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'default': 'chat',
            'enum': list([
              'chat',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'function',
            'enum': list([
              'function',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'human',
            'enum': list([
              'human',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'string',
          }),
          'error': dict({
            'title': 'Error',
            'type': 'string',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'invalid_tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'system',
            'enum': list([
              'system',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'ToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to raw_output.
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      raw_output=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'raw_output': dict({
            'title': 'Raw Output',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'default': 'tool',
            'enum': list([
              'tool',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'FakeListChatModelOutput',
  })
# ---
# name: test_schemas.5
  dict({
    'anyOf': list([
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ai',
            'enum': list([
              'ai',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
          'usage_metadata': dict({
            '$ref': '#/definitions/UsageMetadata',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'default': 'chat',
            'enum': list([
              'chat',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'anyOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ChatPromptValueConcrete',
            'enum': list([
              'ChatPromptValueConcrete',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'function',
            'enum': list([
              'function',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'human',
            'enum': list([
              'human',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'string',
          }),
          'error': dict({
            'title': 'Error',
            'type': 'string',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'invalid_tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'default': 'StringPromptValue',
            'enum': list([
              'StringPromptValue',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'system',
            'enum': list([
              'system',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'ToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to raw_output.
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      raw_output=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'raw_output': dict({
            'title': 'Raw Output',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'default': 'tool',
            'enum': list([
              'tool',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'PromptTemplateOutput',
  })
# ---
# name: test_schemas.6
  dict({
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ai',
            'enum': list([
              'ai',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
          'usage_metadata': dict({
            '$ref': '#/definitions/UsageMetadata',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'default': 'chat',
            'enum': list([
              'chat',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'anyOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ChatPromptValueConcrete',
            'enum': list([
              'ChatPromptValueConcrete',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'function',
            'enum': list([
              'function',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'human',
            'enum': list([
              'human',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'string',
          }),
          'error': dict({
            'title': 'Error',
            'type': 'string',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'invalid_tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'PromptTemplateOutput': dict({
        'anyOf': list([
          dict({
            '$ref': '#/definitions/StringPromptValue',
          }),
          dict({
            '$ref': '#/definitions/ChatPromptValueConcrete',
          }),
        ]),
        'title': 'PromptTemplateOutput',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'default': 'StringPromptValue',
            'enum': list([
              'StringPromptValue',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'system',
            'enum': list([
              'system',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'ToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to raw_output.
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      raw_output=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'raw_output': dict({
            'title': 'Raw Output',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'default': 'tool',
            'enum': list([
              'tool',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'items': dict({
      '$ref': '#/definitions/PromptTemplateOutput',
    }),
    'title': 'RunnableEach<PromptTemplate>Output',
    'type': 'array',
  })
# ---
# name: test_schemas.7
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        '$ref': '#/definitions/AIMessage',
      }),
      dict({
        '$ref': '#/definitions/HumanMessage',
      }),
      dict({
        '$ref': '#/definitions/ChatMessage',
      }),
      dict({
        '$ref': '#/definitions/SystemMessage',
      }),
      dict({
        '$ref': '#/definitions/FunctionMessage',
      }),
      dict({
        '$ref': '#/definitions/ToolMessage',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ai',
            'enum': list([
              'ai',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
          'usage_metadata': dict({
            '$ref': '#/definitions/UsageMetadata',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'default': 'chat',
            'enum': list([
              'chat',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'function',
            'enum': list([
              'function',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'human',
            'enum': list([
              'human',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'string',
          }),
          'error': dict({
            'title': 'Error',
            'type': 'string',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'invalid_tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'system',
            'enum': list([
              'system',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'ToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to raw_output.
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      raw_output=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'raw_output': dict({
            'title': 'Raw Output',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'default': 'tool',
            'enum': list([
              'tool',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'CommaSeparatedListOutputParserInput',
  })
# ---
# name: test_schemas[chat_prompt_input_schema]
  dict({
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ai',
            'enum': list([
              'ai',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
          'usage_metadata': dict({
            '$ref': '#/definitions/UsageMetadata',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'default': 'chat',
            'enum': list([
              'chat',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'function',
            'enum': list([
              'function',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'human',
            'enum': list([
              'human',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'string',
          }),
          'error': dict({
            'title': 'Error',
            'type': 'string',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'invalid_tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'system',
            'enum': list([
              'system',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'ToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to raw_output.
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      raw_output=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'raw_output': dict({
            'title': 'Raw Output',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'default': 'tool',
            'enum': list([
              'tool',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'history': dict({
        'items': dict({
          'anyOf': list([
            dict({
              '$ref': '#/definitions/AIMessage',
            }),
            dict({
              '$ref': '#/definitions/HumanMessage',
            }),
            dict({
              '$ref': '#/definitions/ChatMessage',
            }),
            dict({
              '$ref': '#/definitions/SystemMessage',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessage',
            }),
            dict({
              '$ref': '#/definitions/ToolMessage',
            }),
          ]),
        }),
        'title': 'History',
        'type': 'array',
      }),
    }),
    'required': list([
      'history',
    ]),
    'title': 'PromptInput',
    'type': 'object',
  })
# ---
# name: test_schemas[chat_prompt_output_schema]
  dict({
    'anyOf': list([
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ai',
            'enum': list([
              'ai',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
          'usage_metadata': dict({
            '$ref': '#/definitions/UsageMetadata',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'default': 'chat',
            'enum': list([
              'chat',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'anyOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'default': 'ChatPromptValueConcrete',
            'enum': list([
              'ChatPromptValueConcrete',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'function',
            'enum': list([
              'function',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'human',
            'enum': list([
              'human',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'string',
          }),
          'error': dict({
            'title': 'Error',
            'type': 'string',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'invalid_tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'default': 'StringPromptValue',
            'enum': list([
              'StringPromptValue',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'default': 'system',
            'enum': list([
              'system',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'ToolCall': dict({
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'enum': list([
              'tool_call',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to raw_output.
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      raw_output=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'title': 'Id',
            'type': 'string',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'raw_output': dict({
            'title': 'Raw Output',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'default': 'tool',
            'enum': list([
              'tool',
            ]),
            'title': 'Type',
            'type': 'string',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'ChatPromptTemplateOutput',
  })
# ---
# name: test_seq_dict_prompt_llm
  '''
  {
    question: RunnablePassthrough()
              | RunnableLambda(...),
    documents: RunnableLambda(...)
               | FakeRetriever(),
    just_to_test_lambda: RunnableLambda(...)
  }
  | ChatPromptTemplate(input_variables=['documents', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a nice assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents', 'question'], template='Context:\n{documents}\n\nQuestion:\n{question}'))])
  | FakeListChatModel(responses=['foo, bar'])
  | CommaSeparatedListOutputParser()
  '''
# ---
# name: test_seq_dict_prompt_llm.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "question": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "schema",
                    "runnable",
                    "RunnablePassthrough"
                  ],
                  "kwargs": {},
                  "name": "RunnablePassthrough",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PassthroughInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "schema",
                            "runnable",
                            "RunnablePassthrough"
                          ],
                          "name": "Passthrough"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PassthroughOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "runnables",
                    "base",
                    "RunnableLambda"
                  ],
                  "repr": "RunnableLambda(...)"
                }
              },
              "name": "RunnableSequence",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "PassthroughInput"
                  },
                  {
                    "id": 1,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain",
                        "schema",
                        "runnable",
                        "RunnablePassthrough"
                      ],
                      "name": "Passthrough"
                    }
                  },
                  {
                    "id": 2,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "runnables",
                        "base",
                        "RunnableLambda"
                      ],
                      "name": "Lambda"
                    }
                  },
                  {
                    "id": 3,
                    "type": "schema",
                    "data": "LambdaOutput"
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 1
                  },
                  {
                    "source": 2,
                    "target": 3
                  },
                  {
                    "source": 1,
                    "target": 2
                  }
                ]
              }
            },
            "documents": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "runnables",
                    "base",
                    "RunnableLambda"
                  ],
                  "repr": "RunnableLambda(...)"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "tests",
                    "unit_tests",
                    "runnables",
                    "test_runnable",
                    "FakeRetriever"
                  ],
                  "repr": "FakeRetriever()",
                  "name": "FakeRetriever",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "FakeRetrieverInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "tests",
                            "unit_tests",
                            "runnables",
                            "test_runnable",
                            "FakeRetriever"
                          ],
                          "name": "FakeRetriever"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "FakeRetrieverOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              },
              "name": "RunnableSequence",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "LambdaInput"
                  },
                  {
                    "id": 1,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "runnables",
                        "base",
                        "RunnableLambda"
                      ],
                      "name": "Lambda"
                    }
                  },
                  {
                    "id": 2,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "tests",
                        "unit_tests",
                        "runnables",
                        "test_runnable",
                        "FakeRetriever"
                      ],
                      "name": "FakeRetriever"
                    }
                  },
                  {
                    "id": 3,
                    "type": "schema",
                    "data": "FakeRetrieverOutput"
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 1
                  },
                  {
                    "source": 2,
                    "target": 3
                  },
                  {
                    "source": 1,
                    "target": 2
                  }
                ]
              }
            },
            "just_to_test_lambda": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            }
          }
        },
        "name": "RunnableParallel<question,documents,just_to_test_lambda>",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "Parallel<question,documents,just_to_test_lambda>Input"
            },
            {
              "id": 1,
              "type": "schema",
              "data": "Parallel<question,documents,just_to_test_lambda>Output"
            },
            {
              "id": 2,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "schema",
                  "runnable",
                  "RunnablePassthrough"
                ],
                "name": "Passthrough"
              }
            },
            {
              "id": 3,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "runnables",
                  "base",
                  "RunnableLambda"
                ],
                "name": "Lambda"
              }
            },
            {
              "id": 4,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "runnables",
                  "base",
                  "RunnableLambda"
                ],
                "name": "Lambda"
              }
            },
            {
              "id": 5,
              "type": "runnable",
              "data": {
                "id": [
                  "tests",
                  "unit_tests",
                  "runnables",
                  "test_runnable",
                  "FakeRetriever"
                ],
                "name": "FakeRetriever"
              }
            },
            {
              "id": 6,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "runnables",
                  "base",
                  "RunnableLambda"
                ],
                "name": "Lambda"
              }
            }
          ],
          "edges": [
            {
              "source": 2,
              "target": 3
            },
            {
              "source": 0,
              "target": 2
            },
            {
              "source": 3,
              "target": 1
            },
            {
              "source": 4,
              "target": 5
            },
            {
              "source": 0,
              "target": 4
            },
            {
              "source": 5,
              "target": 1
            },
            {
              "source": 0,
              "target": 6
            },
            {
              "source": 6,
              "target": 1
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "documents",
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nice assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate",
                    "graph": {
                      "nodes": [
                        {
                          "id": 0,
                          "type": "schema",
                          "data": "PromptInput"
                        },
                        {
                          "id": 1,
                          "type": "runnable",
                          "data": {
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "name": "PromptTemplate"
                          }
                        },
                        {
                          "id": 2,
                          "type": "schema",
                          "data": "PromptTemplateOutput"
                        }
                      ],
                      "edges": [
                        {
                          "source": 0,
                          "target": 1
                        },
                        {
                          "source": 1,
                          "target": 2
                        }
                      ]
                    }
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "documents",
                        "question"
                      ],
                      "template": "Context:\n{documents}\n\nQuestion:\n{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate",
                    "graph": {
                      "nodes": [
                        {
                          "id": 0,
                          "type": "schema",
                          "data": "PromptInput"
                        },
                        {
                          "id": 1,
                          "type": "runnable",
                          "data": {
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "name": "PromptTemplate"
                          }
                        },
                        {
                          "id": 2,
                          "type": "schema",
                          "data": "PromptTemplateOutput"
                        }
                      ],
                      "edges": [
                        {
                          "source": 0,
                          "target": 1
                        },
                        {
                          "source": 1,
                          "target": 2
                        }
                      ]
                    }
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "PromptInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "name": "ChatPromptTemplate"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "ChatPromptTemplateOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel",
          "graph": {
            "nodes": [
              {
                "id": 0,
                "type": "schema",
                "data": "FakeListChatModelInput"
              },
              {
                "id": 1,
                "type": "runnable",
                "data": {
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "name": "FakeListChatModel"
                }
              },
              {
                "id": 2,
                "type": "schema",
                "data": "FakeListChatModelOutput"
              }
            ],
            "edges": [
              {
                "source": 0,
                "target": 1
              },
              {
                "source": 1,
                "target": 2
              }
            ]
          }
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "output_parsers",
                  "list",
                  "CommaSeparatedListOutputParser"
                ],
                "name": "CommaSeparatedListOutputParser"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "CommaSeparatedListOutputParserOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "Parallel<question,documents,just_to_test_lambda>Input"
        },
        {
          "id": 1,
          "type": "schema",
          "data": "Parallel<question,documents,just_to_test_lambda>Output"
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "schema",
              "runnable",
              "RunnablePassthrough"
            ],
            "name": "Passthrough"
          }
        },
        {
          "id": 3,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 4,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 5,
          "type": "runnable",
          "data": {
            "id": [
              "tests",
              "unit_tests",
              "runnables",
              "test_runnable",
              "FakeRetriever"
            ],
            "name": "FakeRetriever"
          }
        },
        {
          "id": 6,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 7,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 8,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 9,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "output_parsers",
              "list",
              "CommaSeparatedListOutputParser"
            ],
            "name": "CommaSeparatedListOutputParser"
          }
        },
        {
          "id": 10,
          "type": "schema",
          "data": "CommaSeparatedListOutputParserOutput"
        }
      ],
      "edges": [
        {
          "source": 2,
          "target": 3
        },
        {
          "source": 0,
          "target": 2
        },
        {
          "source": 3,
          "target": 1
        },
        {
          "source": 4,
          "target": 5
        },
        {
          "source": 0,
          "target": 4
        },
        {
          "source": 5,
          "target": 1
        },
        {
          "source": 0,
          "target": 6
        },
        {
          "source": 6,
          "target": 1
        },
        {
          "source": 1,
          "target": 7
        },
        {
          "source": 7,
          "target": 8
        },
        {
          "source": 9,
          "target": 10
        },
        {
          "source": 8,
          "target": 9
        }
      ]
    }
  }
  '''
# ---
# name: test_seq_prompt_dict
  '''
  ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a nice assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])
  | RunnableLambda(...)
  | {
      chat: FakeListChatModel(responses=["i'm a chatbot"]),
      llm: FakeListLLM(responses=["i'm a textbot"])
    }
  '''
# ---
# name: test_seq_prompt_dict.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(...)"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "chat": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake_chat_models",
                "FakeListChatModel"
              ],
              "repr": "FakeListChatModel(responses=[\"i'm a chatbot\"])",
              "name": "FakeListChatModel",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "FakeListChatModelInput"
                  },
                  {
                    "id": 1,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "language_models",
                        "fake_chat_models",
                        "FakeListChatModel"
                      ],
                      "name": "FakeListChatModel"
                    }
                  },
                  {
                    "id": 2,
                    "type": "schema",
                    "data": "FakeListChatModelOutput"
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 1
                  },
                  {
                    "source": 1,
                    "target": 2
                  }
                ]
              }
            },
            "llm": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake",
                "FakeListLLM"
              ],
              "repr": "FakeListLLM(responses=[\"i'm a textbot\"])",
              "name": "FakeListLLM",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "FakeListLLMInput"
                  },
                  {
                    "id": 1,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "language_models",
                        "fake",
                        "FakeListLLM"
                      ],
                      "name": "FakeListLLM"
                    }
                  },
                  {
                    "id": 2,
                    "type": "schema",
                    "data": "FakeListLLMOutput"
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 1
                  },
                  {
                    "source": 1,
                    "target": 2
                  }
                ]
              }
            }
          }
        },
        "name": "RunnableParallel<chat,llm>",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "Parallel<chat,llm>Input"
            },
            {
              "id": 1,
              "type": "schema",
              "data": "Parallel<chat,llm>Output"
            },
            {
              "id": 2,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "language_models",
                  "fake_chat_models",
                  "FakeListChatModel"
                ],
                "name": "FakeListChatModel"
              }
            },
            {
              "id": 3,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "language_models",
                  "fake",
                  "FakeListLLM"
                ],
                "name": "FakeListLLM"
              }
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 2
            },
            {
              "source": 2,
              "target": 1
            },
            {
              "source": 0,
              "target": 3
            },
            {
              "source": 3,
              "target": 1
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 3,
          "type": "schema",
          "data": "Parallel<chat,llm>Input"
        },
        {
          "id": 4,
          "type": "schema",
          "data": "Parallel<chat,llm>Output"
        },
        {
          "id": 5,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 6,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeListLLM"
            ],
            "name": "FakeListLLM"
          }
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 3,
          "target": 5
        },
        {
          "source": 5,
          "target": 4
        },
        {
          "source": 3,
          "target": 6
        },
        {
          "source": 6,
          "target": 4
        },
        {
          "source": 2,
          "target": 3
        }
      ]
    }
  }
  '''
# ---
# name: test_seq_prompt_map
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "PromptInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain",
                            "prompts",
                            "prompt",
                            "PromptTemplate"
                          ],
                          "name": "PromptTemplate"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "PromptTemplateOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "PromptInput"
            },
            {
              "id": 1,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "ChatPromptTemplate"
                ],
                "name": "ChatPromptTemplate"
              }
            },
            {
              "id": 2,
              "type": "schema",
              "data": "ChatPromptTemplateOutput"
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 1
            },
            {
              "source": 1,
              "target": 2
            }
          ]
        }
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(...)"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "chat": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableBinding"
              ],
              "kwargs": {
                "bound": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "repr": "FakeListChatModel(responses=[\"i'm a chatbot\"])",
                  "name": "FakeListChatModel",
                  "graph": {
                    "nodes": [
                      {
                        "id": 0,
                        "type": "schema",
                        "data": "FakeListChatModelInput"
                      },
                      {
                        "id": 1,
                        "type": "runnable",
                        "data": {
                          "id": [
                            "langchain_core",
                            "language_models",
                            "fake_chat_models",
                            "FakeListChatModel"
                          ],
                          "name": "FakeListChatModel"
                        }
                      },
                      {
                        "id": 2,
                        "type": "schema",
                        "data": "FakeListChatModelOutput"
                      }
                    ],
                    "edges": [
                      {
                        "source": 0,
                        "target": 1
                      },
                      {
                        "source": 1,
                        "target": 2
                      }
                    ]
                  }
                },
                "kwargs": {
                  "stop": [
                    "Thought:"
                  ]
                }
              },
              "name": "FakeListChatModel",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "FakeListChatModelInput"
                  },
                  {
                    "id": 1,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "language_models",
                        "fake_chat_models",
                        "FakeListChatModel"
                      ],
                      "name": "FakeListChatModel"
                    }
                  },
                  {
                    "id": 2,
                    "type": "schema",
                    "data": "FakeListChatModelOutput"
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 1
                  },
                  {
                    "source": 1,
                    "target": 2
                  }
                ]
              }
            },
            "llm": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake",
                "FakeListLLM"
              ],
              "repr": "FakeListLLM(responses=[\"i'm a textbot\"])",
              "name": "FakeListLLM",
              "graph": {
                "nodes": [
                  {
                    "id": 0,
                    "type": "schema",
                    "data": "FakeListLLMInput"
                  },
                  {
                    "id": 1,
                    "type": "runnable",
                    "data": {
                      "id": [
                        "langchain_core",
                        "language_models",
                        "fake",
                        "FakeListLLM"
                      ],
                      "name": "FakeListLLM"
                    }
                  },
                  {
                    "id": 2,
                    "type": "schema",
                    "data": "FakeListLLMOutput"
                  }
                ],
                "edges": [
                  {
                    "source": 0,
                    "target": 1
                  },
                  {
                    "source": 1,
                    "target": 2
                  }
                ]
              }
            },
            "passthrough": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            }
          }
        },
        "name": "RunnableParallel<chat,llm,passthrough>",
        "graph": {
          "nodes": [
            {
              "id": 0,
              "type": "schema",
              "data": "Parallel<chat,llm,passthrough>Input"
            },
            {
              "id": 1,
              "type": "schema",
              "data": "Parallel<chat,llm,passthrough>Output"
            },
            {
              "id": 2,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "language_models",
                  "fake_chat_models",
                  "FakeListChatModel"
                ],
                "name": "FakeListChatModel"
              }
            },
            {
              "id": 3,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "language_models",
                  "fake",
                  "FakeListLLM"
                ],
                "name": "FakeListLLM"
              }
            },
            {
              "id": 4,
              "type": "runnable",
              "data": {
                "id": [
                  "langchain_core",
                  "runnables",
                  "base",
                  "RunnableLambda"
                ],
                "name": "Lambda"
              }
            }
          ],
          "edges": [
            {
              "source": 0,
              "target": 2
            },
            {
              "source": 2,
              "target": 1
            },
            {
              "source": 0,
              "target": 3
            },
            {
              "source": 3,
              "target": 1
            },
            {
              "source": 0,
              "target": 4
            },
            {
              "source": 4,
              "target": 1
            }
          ]
        }
      }
    },
    "name": "RunnableSequence",
    "graph": {
      "nodes": [
        {
          "id": 0,
          "type": "schema",
          "data": "PromptInput"
        },
        {
          "id": 1,
          "type": "runnable",
          "data": {
            "id": [
              "langchain",
              "prompts",
              "chat",
              "ChatPromptTemplate"
            ],
            "name": "ChatPromptTemplate"
          }
        },
        {
          "id": 2,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        },
        {
          "id": 3,
          "type": "schema",
          "data": "Parallel<chat,llm,passthrough>Input"
        },
        {
          "id": 4,
          "type": "schema",
          "data": "Parallel<chat,llm,passthrough>Output"
        },
        {
          "id": 5,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake_chat_models",
              "FakeListChatModel"
            ],
            "name": "FakeListChatModel"
          }
        },
        {
          "id": 6,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeListLLM"
            ],
            "name": "FakeListLLM"
          }
        },
        {
          "id": 7,
          "type": "runnable",
          "data": {
            "id": [
              "langchain_core",
              "runnables",
              "base",
              "RunnableLambda"
            ],
            "name": "Lambda"
          }
        }
      ],
      "edges": [
        {
          "source": 0,
          "target": 1
        },
        {
          "source": 1,
          "target": 2
        },
        {
          "source": 3,
          "target": 5
        },
        {
          "source": 5,
          "target": 4
        },
        {
          "source": 3,
          "target": 6
        },
        {
          "source": 6,
          "target": 4
        },
        {
          "source": 3,
          "target": 7
        },
        {
          "source": 7,
          "target": 4
        },
        {
          "source": 2,
          "target": 3
        }
      ]
    }
  }
  '''
# ---
