# serializer version: 1
# name: test_combining_sequences
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_combining_sequences.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(lambda x: {'question': x[0] + x[1]})"
      },
      "middle": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nicer assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "question"
                      ],
                      "template": "{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate"
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['baz, qux'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_combining_sequences.2
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel"
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "output_parsers",
            "list",
            "CommaSeparatedListOutputParser"
          ],
          "kwargs": {},
          "name": "CommaSeparatedListOutputParser"
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(lambda x: {'question': x[0] + x[1]})"
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nicer assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "question"
                      ],
                      "template": "{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate"
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['baz, qux'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_combining_sequences.3
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ['baz', 'qux']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo, bar'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'fakelistchatmodel', 'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo, bar'])", 'name': 'FakeListChatModel'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo, bar', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'foo, bar', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000002-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': AIMessage(content='foo, bar', additional_kwargs={}, response_metadata={}, id='00000000-0000-4000-8000-000000000004')}, outputs={'output': ['foo', 'bar']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003'), Run(id=UUID('00000000-0000-4000-8000-000000000005'), name='RunnableLambda', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': ['foo', 'bar']}, outputs={'question': 'foobar'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:4'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000005'), Run(id=UUID('00000000-0000-4000-8000-000000000006'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nicer assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'foobar'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nicer assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='foobar', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:5'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000006'), Run(id=UUID('00000000-0000-4000-8000-000000000007'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['baz, qux'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'fakelistchatmodel', 'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['baz, qux'])", 'name': 'FakeListChatModel'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nicer assistant.\nHuman: foobar']}, outputs={'generations': [[{'text': 'baz, qux', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'baz, qux', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000006-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:6'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000007'), Run(id=UUID('00000000-0000-4000-8000-000000000008'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': AIMessage(content='baz, qux', additional_kwargs={}, response_metadata={}, id='00000000-0000-4000-8000-000000000009')}, outputs={'output': ['baz', 'qux']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:7'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000008')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_configurable_fields[schema2]
  dict({
    '$defs': dict({
      'Configurable': dict({
        'properties': dict({
          'llm_responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableConfigurableFieldsConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields[schema3]
  dict({
    '$defs': dict({
      'Configurable': dict({
        'properties': dict({
          'prompt_template': dict({
            'default': 'Hello, {name}!',
            'description': 'The prompt template for this chain',
            'title': 'Prompt Template',
            'type': 'string',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableConfigurableFieldsConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields[schema4]
  dict({
    '$defs': dict({
      'Configurable': dict({
        'properties': dict({
          'llm_responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
          'prompt_template': dict({
            'default': 'Hello, {name}!',
            'description': 'The prompt template for this chain',
            'title': 'Prompt Template',
            'type': 'string',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableSequenceConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields[schema5]
  dict({
    '$defs': dict({
      'Configurable': dict({
        'properties': dict({
          'llm_responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
          'other_responses': dict({
            'default': list([
              'a',
            ]),
            'items': dict({
              'type': 'string',
            }),
            'title': 'Other Responses',
            'type': 'array',
          }),
          'prompt_template': dict({
            'default': 'Hello, {name}!',
            'description': 'The prompt template for this chain',
            'title': 'Prompt Template',
            'type': 'string',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableSequenceConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields_example[schema7]
  dict({
    '$defs': dict({
      'Chat_Responses': dict({
        'title': 'Chat Responses',
      }),
      'Configurable': dict({
        'properties': dict({
          'chat_responses': dict({
            'default': list([
              'hello',
              'bye',
            ]),
            'items': dict({
              '$ref': '#/$defs/Chat_Responses',
            }),
            'title': 'Chat Responses',
            'type': 'array',
          }),
          'llm': dict({
            '$ref': '#/$defs/LLM',
            'default': 'default',
          }),
          'llm_responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
          'prompt_template': dict({
            '$ref': '#/$defs/Prompt_Template',
            'default': 'hello',
            'description': 'The prompt template for this chain',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
      'LLM': dict({
        'title': 'LLM',
      }),
      'Prompt_Template': dict({
        'title': 'Prompt Template',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/$defs/Configurable',
      }),
    }),
    'title': 'RunnableSequenceConfig',
    'type': 'object',
  })
# ---
# name: test_configurable_fields_prefix_keys[schema6]
  dict({
    'definitions': dict({
      'Chat_Responses': dict({
        'title': 'Chat Responses',
      }),
      'Configurable': dict({
        'properties': dict({
          'chat_sleep': dict({
            'anyOf': list([
              dict({
                'type': 'number',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Chat Sleep',
          }),
          'llm': dict({
            '$ref': '#/definitions/LLM',
            'default': 'default',
          }),
          'llm==chat/responses': dict({
            'default': list([
              'hello',
              'bye',
            ]),
            'items': dict({
              '$ref': '#/definitions/Chat_Responses',
            }),
            'title': 'Chat Responses',
            'type': 'array',
          }),
          'llm==default/responses': dict({
            'default': list([
              'a',
            ]),
            'description': 'A list of fake responses for this LLM',
            'items': dict({
              'type': 'string',
            }),
            'title': 'LLM Responses',
            'type': 'array',
          }),
          'prompt_template': dict({
            '$ref': '#/definitions/Prompt_Template',
            'default': 'hello',
            'description': 'The prompt template for this chain',
          }),
        }),
        'title': 'Configurable',
        'type': 'object',
      }),
      'LLM': dict({
        'title': 'LLM',
      }),
      'Prompt_Template': dict({
        'title': 'Prompt Template',
      }),
    }),
    'properties': dict({
      'configurable': dict({
        '$ref': '#/definitions/Configurable',
      }),
    }),
    'title': 'RunnableSequenceConfig',
    'type': 'object',
  })
# ---
# name: test_each
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeStreamingListLLM"
          ],
          "repr": "FakeStreamingListLLM(responses=['first item, second item, third item'])",
          "name": "FakeStreamingListLLM"
        },
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "tests",
            "unit_tests",
            "runnables",
            "test_runnable",
            "FakeSplitIntoListParser"
          ],
          "kwargs": {},
          "name": "FakeSplitIntoListParser"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableEach"
        ],
        "kwargs": {
          "bound": {
            "lc": 1,
            "type": "not_implemented",
            "id": [
              "langchain_core",
              "language_models",
              "fake",
              "FakeStreamingListLLM"
            ],
            "repr": "FakeStreamingListLLM(responses=['this', 'is', 'a', 'test'])",
            "name": "FakeStreamingListLLM"
          }
        },
        "name": "RunnableEach<FakeStreamingListLLM>"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_higher_order_lambda_runnable
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "key": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(lambda x: x['key'])"
            },
            "input": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableParallel"
              ],
              "kwargs": {
                "steps__": {
                  "question": {
                    "lc": 1,
                    "type": "not_implemented",
                    "id": [
                      "langchain_core",
                      "runnables",
                      "base",
                      "RunnableLambda"
                    ],
                    "repr": "RunnableLambda(lambda x: x['question'])"
                  }
                }
              },
              "name": "RunnableParallel<question>"
            }
          }
        },
        "name": "RunnableParallel<key,input>"
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(router)"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_lambda_schemas[schema8]
  dict({
    '$defs': dict({
      'OutputType': dict({
        'properties': dict({
          'bye': dict({
            'title': 'Bye',
            'type': 'string',
          }),
          'byebye': dict({
            'title': 'Byebye',
            'type': 'integer',
          }),
          'hello': dict({
            'title': 'Hello',
            'type': 'string',
          }),
        }),
        'required': list([
          'hello',
          'bye',
          'byebye',
        ]),
        'title': 'OutputType',
        'type': 'object',
      }),
    }),
    '$ref': '#/$defs/OutputType',
    'title': 'aget_values_typed_output',
  })
# ---
# name: test_prompt_with_chat_model
  '''
  ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])
  | FakeListChatModel(responses=['foo'])
  '''
# ---
# name: test_prompt_with_chat_model.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake_chat_models",
          "FakeListChatModel"
        ],
        "repr": "FakeListChatModel(responses=['foo'])",
        "name": "FakeListChatModel"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_chat_model.2
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': AIMessage(content='foo', additional_kwargs={}, response_metadata={}, id='00000000-0000-4000-8000-000000000003')}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'fakelistchatmodel', 'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo'])", 'name': 'FakeListChatModel'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'foo', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000002-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_chat_model_and_parser
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_chat_model_and_parser.1
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ['foo', 'bar']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo, bar'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'fakelistchatmodel', 'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo, bar'])", 'name': 'FakeListChatModel'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo, bar', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'foo, bar', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000002-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': AIMessage(content='foo, bar', additional_kwargs={}, response_metadata={}, id='00000000-0000-4000-8000-000000000004')}, outputs={'output': ['foo', 'bar']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_chat_model_async
  '''
  ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])
  | FakeListChatModel(responses=['foo'])
  '''
# ---
# name: test_prompt_with_chat_model_async.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake_chat_models",
          "FakeListChatModel"
        ],
        "repr": "FakeListChatModel(responses=['foo'])",
        "name": "FakeListChatModel"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_chat_model_async.2
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': AIMessage(content='foo', additional_kwargs={}, response_metadata={}, id='00000000-0000-4000-8000-000000000003')}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListChatModel', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo'], '_type': 'fake-list-chat-model', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'fakelistchatmodel', 'ls_model_type': 'chat'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake_chat_models', 'FakeListChatModel'], 'repr': "FakeListChatModel(responses=['foo'])", 'name': 'FakeListChatModel'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'foo', 'type': 'ai', 'id': 'run-00000000-0000-4000-8000-000000000002-0', 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "language_models",
          "fake",
          "FakeListLLM"
        ],
        "repr": "FakeListLLM(responses=['foo', 'bar'])",
        "name": "FakeListLLM"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_llm.1
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': 'foo'}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo', 'bar'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'fakelist', 'ls_model_type': 'llm'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'])", 'name': 'FakeListLLM'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm.2
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': 'bar'}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo', 'bar'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 2, 'metadata': {'ls_provider': 'fakelist', 'ls_model_type': 'llm'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'])", 'name': 'FakeListLLM'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'bar', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
    Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your favorite color?'}, outputs={'output': 'foo'}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000004'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your favorite color?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your favorite color?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000003'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000003'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000003.20230101T000000000000Z00000000-0000-4000-8000-000000000004'), Run(id=UUID('00000000-0000-4000-8000-000000000005'), name='FakeListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo', 'bar'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 2, 'metadata': {'ls_provider': 'fakelist', 'ls_model_type': 'llm'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'])", 'name': 'FakeListLLM'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your favorite color?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000003'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000003'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000003.20230101T000000000000Z00000000-0000-4000-8000-000000000005')], trace_id=UUID('00000000-0000-4000-8000-000000000003'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000003'),
  ])
# ---
# name: test_prompt_with_llm_and_async_lambda
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeListLLM"
          ],
          "repr": "FakeListLLM(responses=['foo', 'bar'])",
          "name": "FakeListLLM"
        }
      ],
      "last": {
        "lc": 1,
        "type": "not_implemented",
        "id": [
          "langchain_core",
          "runnables",
          "base",
          "RunnableLambda"
        ],
        "repr": "RunnableLambda(afunc=passthrough)"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_llm_and_async_lambda.1
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': 'foo'}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['foo', 'bar'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'fakelist', 'ls_model_type': 'llm'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeListLLM'], 'repr': "FakeListLLM(responses=['foo', 'bar'])", 'name': 'FakeListLLM'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'foo', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='passthrough', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': 'foo'}, outputs={'output': 'foo'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm_parser
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeStreamingListLLM"
          ],
          "repr": "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'])",
          "name": "FakeStreamingListLLM"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_prompt_with_llm_parser.1
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ['bear', 'dog', 'cat']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeStreamingListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['bear, dog, cat', 'tomato, lettuce, onion'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'fakestreaminglist', 'ls_model_type': 'llm'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'])", 'name': 'FakeStreamingListLLM'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'bear, dog, cat', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': 'bear, dog, cat'}, outputs={'output': ['bear', 'dog', 'cat']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
  ])
# ---
# name: test_prompt_with_llm_parser.2
  list([
    Run(id=UUID('00000000-0000-4000-8000-000000000000'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ['tomato', 'lettuce', 'onion']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000001'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your name?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000001'), Run(id=UUID('00000000-0000-4000-8000-000000000002'), name='FakeStreamingListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['bear, dog, cat', 'tomato, lettuce, onion'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 2, 'metadata': {'ls_provider': 'fakestreaminglist', 'ls_model_type': 'llm'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'])", 'name': 'FakeStreamingListLLM'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your name?']}, outputs={'generations': [[{'text': 'tomato, lettuce, onion', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000002'), Run(id=UUID('00000000-0000-4000-8000-000000000003'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': 'tomato, lettuce, onion'}, outputs={'output': ['tomato', 'lettuce', 'onion']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000000'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000.20230101T000000000000Z00000000-0000-4000-8000-000000000003')], trace_id=UUID('00000000-0000-4000-8000-000000000000'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000000'),
    Run(id=UUID('00000000-0000-4000-8000-000000000004'), name='RunnableSequence', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='chain', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your favorite color?'}, outputs={'output': ['bear', 'dog', 'cat']}, reference_example_id=None, parent_run_id=None, tags=[], child_runs=[Run(id=UUID('00000000-0000-4000-8000-000000000005'), name='ChatPromptTemplate', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'SystemMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': [], 'template': 'You are a nice assistant.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['question'], 'template': '{question}', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is your favorite color?'}, outputs={'output': ChatPromptValue(messages=[SystemMessage(content='You are a nice assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your favorite color?', additional_kwargs={}, response_metadata={})])}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000004'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000004'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000004.20230101T000000000000Z00000000-0000-4000-8000-000000000005'), Run(id=UUID('00000000-0000-4000-8000-000000000006'), name='FakeStreamingListLLM', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='llm', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'responses': ['bear, dog, cat', 'tomato, lettuce, onion'], '_type': 'fake-list', 'stop': None}, 'options': {'stop': None}, 'batch_size': 2, 'metadata': {'ls_provider': 'fakestreaminglist', 'ls_model_type': 'llm'}}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'language_models', 'fake', 'FakeStreamingListLLM'], 'repr': "FakeStreamingListLLM(responses=['bear, dog, cat', 'tomato, lettuce, onion'])", 'name': 'FakeStreamingListLLM'}, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'prompts': ['System: You are a nice assistant.\nHuman: What is your favorite color?']}, outputs={'generations': [[{'text': 'bear, dog, cat', 'generation_info': None, 'type': 'Generation'}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000004'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000004'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000004.20230101T000000000000Z00000000-0000-4000-8000-000000000006'), Run(id=UUID('00000000-0000-4000-8000-000000000007'), name='CommaSeparatedListOutputParser', start_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), run_type='parser', end_time=FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized=None, events=[{'name': 'start', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': FakeDatetime(2023, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}], inputs={'input': 'bear, dog, cat'}, outputs={'output': ['bear', 'dog', 'cat']}, reference_example_id=None, parent_run_id=UUID('00000000-0000-4000-8000-000000000004'), tags=['seq:step:3'], child_runs=[], trace_id=UUID('00000000-0000-4000-8000-000000000004'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000004.20230101T000000000000Z00000000-0000-4000-8000-000000000007')], trace_id=UUID('00000000-0000-4000-8000-000000000004'), dotted_order='20230101T000000000000Z00000000-0000-4000-8000-000000000004'),
  ])
# ---
# name: test_router_runnable
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "key": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            },
            "input": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableParallel"
              ],
              "kwargs": {
                "steps__": {
                  "question": {
                    "lc": 1,
                    "type": "not_implemented",
                    "id": [
                      "langchain_core",
                      "runnables",
                      "base",
                      "RunnableLambda"
                    ],
                    "repr": "RunnableLambda(...)"
                  }
                }
              },
              "name": "RunnableParallel<question>"
            }
          }
        },
        "name": "RunnableParallel<key,input>"
      },
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RouterRunnable"
        ],
        "kwargs": {
          "runnables": {
            "math": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "messages": [
                      {
                        "lc": 1,
                        "type": "constructor",
                        "id": [
                          "langchain",
                          "prompts",
                          "chat",
                          "HumanMessagePromptTemplate"
                        ],
                        "kwargs": {
                          "prompt": {
                            "lc": 1,
                            "type": "constructor",
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "kwargs": {
                              "input_variables": [
                                "question"
                              ],
                              "template": "You are a math genius. Answer the question: {question}",
                              "template_format": "f-string"
                            },
                            "name": "PromptTemplate"
                          }
                        }
                      }
                    ]
                  },
                  "name": "ChatPromptTemplate"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeListLLM"
                  ],
                  "repr": "FakeListLLM(responses=['4'])",
                  "name": "FakeListLLM"
                }
              },
              "name": "RunnableSequence"
            },
            "english": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "chat",
                    "ChatPromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "messages": [
                      {
                        "lc": 1,
                        "type": "constructor",
                        "id": [
                          "langchain",
                          "prompts",
                          "chat",
                          "HumanMessagePromptTemplate"
                        ],
                        "kwargs": {
                          "prompt": {
                            "lc": 1,
                            "type": "constructor",
                            "id": [
                              "langchain",
                              "prompts",
                              "prompt",
                              "PromptTemplate"
                            ],
                            "kwargs": {
                              "input_variables": [
                                "question"
                              ],
                              "template": "You are an english major. Answer the question: {question}",
                              "template_format": "f-string"
                            },
                            "name": "PromptTemplate"
                          }
                        }
                      }
                    ]
                  },
                  "name": "ChatPromptTemplate"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake",
                    "FakeListLLM"
                  ],
                  "repr": "FakeListLLM(responses=['2'])",
                  "name": "FakeListLLM"
                }
              },
              "name": "RunnableSequence"
            }
          }
        },
        "name": "RouterRunnable"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_schemas[chat_prompt_input_schema]
  dict({
    '$defs': dict({
      'AIMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/$defs/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Message chunk from an AI.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/$defs/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'additionalProperties': True,
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'additionalProperties': True,
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents a request to call a tool.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "name": "foo",
                      "args": {"a": 1},
                      "id": "123"
                  }
          
              This represents a request to call the tool named "foo" with arguments {"a": 1}
              and an identifier of "123".
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (e.g., as part of a stream).
          
          When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
          all string attributes are concatenated. Chunks are only merged if their
          values of `index` are equal and not None.
          
          Example:
          
          .. code-block:: python
          
              left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
              right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
          
              (
                  AIMessageChunk(content="", tool_call_chunks=left_chunks)
                  + AIMessageChunk(content="", tool_call_chunks=right_chunks)
              ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              .. versionadded:: 0.2.17
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      artifact=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "input_tokens": 10,
                      "output_tokens": 20,
                      "total_tokens": 30
                  }
        ''',
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'history': dict({
        'items': dict({
          'oneOf': list([
            dict({
              '$ref': '#/$defs/AIMessage',
            }),
            dict({
              '$ref': '#/$defs/HumanMessage',
            }),
            dict({
              '$ref': '#/$defs/ChatMessage',
            }),
            dict({
              '$ref': '#/$defs/SystemMessage',
            }),
            dict({
              '$ref': '#/$defs/FunctionMessage',
            }),
            dict({
              '$ref': '#/$defs/ToolMessage',
            }),
            dict({
              '$ref': '#/$defs/AIMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/HumanMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/ChatMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/SystemMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/FunctionMessageChunk',
            }),
            dict({
              '$ref': '#/$defs/ToolMessageChunk',
            }),
          ]),
        }),
        'title': 'History',
        'type': 'array',
      }),
    }),
    'required': list([
      'history',
    ]),
    'title': 'PromptInput',
    'type': 'object',
  })
# ---
# name: test_schemas[chat_prompt_output_schema]
  dict({
    '$defs': dict({
      'AIMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/$defs/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Message chunk from an AI.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/$defs/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/$defs/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'additionalProperties': True,
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/$defs/AIMessage',
                }),
                dict({
                  '$ref': '#/$defs/HumanMessage',
                }),
                dict({
                  '$ref': '#/$defs/ChatMessage',
                }),
                dict({
                  '$ref': '#/$defs/SystemMessage',
                }),
                dict({
                  '$ref': '#/$defs/FunctionMessage',
                }),
                dict({
                  '$ref': '#/$defs/ToolMessage',
                }),
                dict({
                  '$ref': '#/$defs/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'additionalProperties': True,
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents a request to call a tool.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "name": "foo",
                      "args": {"a": 1},
                      "id": "123"
                  }
          
              This represents a request to call the tool named "foo" with arguments {"a": 1}
              and an identifier of "123".
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (e.g., as part of a stream).
          
          When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
          all string attributes are concatenated. Chunks are only merged if their
          values of `index` are equal and not None.
          
          Example:
          
          .. code-block:: python
          
              left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
              right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
          
              (
                  AIMessageChunk(content="", tool_call_chunks=left_chunks)
                  + AIMessageChunk(content="", tool_call_chunks=right_chunks)
              ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              .. versionadded:: 0.2.17
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      artifact=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "input_tokens": 10,
                      "output_tokens": 20,
                      "total_tokens": 30
                  }
        ''',
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'anyOf': list([
      dict({
        '$ref': '#/$defs/StringPromptValue',
      }),
      dict({
        '$ref': '#/$defs/ChatPromptValueConcrete',
      }),
    ]),
    'title': 'ChatPromptTemplateOutput',
  })
# ---
# name: test_schemas[fake_chat_input_schema]
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
      dict({
        'items': dict({
          'oneOf': list([
            dict({
              '$ref': '#/definitions/AIMessage',
            }),
            dict({
              '$ref': '#/definitions/HumanMessage',
            }),
            dict({
              '$ref': '#/definitions/ChatMessage',
            }),
            dict({
              '$ref': '#/definitions/SystemMessage',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessage',
            }),
            dict({
              '$ref': '#/definitions/ToolMessage',
            }),
            dict({
              '$ref': '#/definitions/AIMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/HumanMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/ChatMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/SystemMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/ToolMessageChunk',
            }),
          ]),
        }),
        'type': 'array',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Message chunk from an AI.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'additionalProperties': True,
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
                dict({
                  '$ref': '#/definitions/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'additionalProperties': True,
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents a request to call a tool.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "name": "foo",
                      "args": {"a": 1},
                      "id": "123"
                  }
          
              This represents a request to call the tool named "foo" with arguments {"a": 1}
              and an identifier of "123".
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (e.g., as part of a stream).
          
          When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
          all string attributes are concatenated. Chunks are only merged if their
          values of `index` are equal and not None.
          
          Example:
          
          .. code-block:: python
          
              left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
              right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
          
              (
                  AIMessageChunk(content="", tool_call_chunks=left_chunks)
                  + AIMessageChunk(content="", tool_call_chunks=right_chunks)
              ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              .. versionadded:: 0.2.17
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      artifact=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "input_tokens": 10,
                      "output_tokens": 20,
                      "total_tokens": 30
                  }
        ''',
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'FakeListChatModelInput',
  })
# ---
# name: test_schemas[fake_chat_output_schema]
  dict({
    'definitions': dict({
      'AIMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Message chunk from an AI.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'additionalProperties': True,
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'additionalProperties': True,
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents a request to call a tool.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "name": "foo",
                      "args": {"a": 1},
                      "id": "123"
                  }
          
              This represents a request to call the tool named "foo" with arguments {"a": 1}
              and an identifier of "123".
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (e.g., as part of a stream).
          
          When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
          all string attributes are concatenated. Chunks are only merged if their
          values of `index` are equal and not None.
          
          Example:
          
          .. code-block:: python
          
              left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
              right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
          
              (
                  AIMessageChunk(content="", tool_call_chunks=left_chunks)
                  + AIMessageChunk(content="", tool_call_chunks=right_chunks)
              ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              .. versionadded:: 0.2.17
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      artifact=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "input_tokens": 10,
                      "output_tokens": 20,
                      "total_tokens": 30
                  }
        ''',
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'oneOf': list([
      dict({
        '$ref': '#/definitions/AIMessage',
      }),
      dict({
        '$ref': '#/definitions/HumanMessage',
      }),
      dict({
        '$ref': '#/definitions/ChatMessage',
      }),
      dict({
        '$ref': '#/definitions/SystemMessage',
      }),
      dict({
        '$ref': '#/definitions/FunctionMessage',
      }),
      dict({
        '$ref': '#/definitions/ToolMessage',
      }),
      dict({
        '$ref': '#/definitions/AIMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/HumanMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/ChatMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/SystemMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/FunctionMessageChunk',
      }),
      dict({
        '$ref': '#/definitions/ToolMessageChunk',
      }),
    ]),
    'title': 'FakeListChatModelOutput',
  })
# ---
# name: test_schemas[fake_llm_input_schema]
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
      dict({
        'items': dict({
          'oneOf': list([
            dict({
              '$ref': '#/definitions/AIMessage',
            }),
            dict({
              '$ref': '#/definitions/HumanMessage',
            }),
            dict({
              '$ref': '#/definitions/ChatMessage',
            }),
            dict({
              '$ref': '#/definitions/SystemMessage',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessage',
            }),
            dict({
              '$ref': '#/definitions/ToolMessage',
            }),
            dict({
              '$ref': '#/definitions/AIMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/HumanMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/ChatMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/SystemMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/FunctionMessageChunk',
            }),
            dict({
              '$ref': '#/definitions/ToolMessageChunk',
            }),
          ]),
        }),
        'type': 'array',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Message chunk from an AI.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'additionalProperties': True,
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
                dict({
                  '$ref': '#/definitions/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'additionalProperties': True,
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents a request to call a tool.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "name": "foo",
                      "args": {"a": 1},
                      "id": "123"
                  }
          
              This represents a request to call the tool named "foo" with arguments {"a": 1}
              and an identifier of "123".
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (e.g., as part of a stream).
          
          When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
          all string attributes are concatenated. Chunks are only merged if their
          values of `index` are equal and not None.
          
          Example:
          
          .. code-block:: python
          
              left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
              right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
          
              (
                  AIMessageChunk(content="", tool_call_chunks=left_chunks)
                  + AIMessageChunk(content="", tool_call_chunks=right_chunks)
              ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              .. versionadded:: 0.2.17
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      artifact=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "input_tokens": 10,
                      "output_tokens": 20,
                      "total_tokens": 30
                  }
        ''',
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'FakeListLLMInput',
  })
# ---
# name: test_schemas[list_parser_input_schema]
  dict({
    'anyOf': list([
      dict({
        'type': 'string',
      }),
      dict({
        'oneOf': list([
          dict({
            '$ref': '#/definitions/AIMessage',
          }),
          dict({
            '$ref': '#/definitions/HumanMessage',
          }),
          dict({
            '$ref': '#/definitions/ChatMessage',
          }),
          dict({
            '$ref': '#/definitions/SystemMessage',
          }),
          dict({
            '$ref': '#/definitions/FunctionMessage',
          }),
          dict({
            '$ref': '#/definitions/ToolMessage',
          }),
          dict({
            '$ref': '#/definitions/AIMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/HumanMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/ChatMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/SystemMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/FunctionMessageChunk',
          }),
          dict({
            '$ref': '#/definitions/ToolMessageChunk',
          }),
        ]),
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Message chunk from an AI.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'additionalProperties': True,
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'additionalProperties': True,
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents a request to call a tool.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "name": "foo",
                      "args": {"a": 1},
                      "id": "123"
                  }
          
              This represents a request to call the tool named "foo" with arguments {"a": 1}
              and an identifier of "123".
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (e.g., as part of a stream).
          
          When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
          all string attributes are concatenated. Chunks are only merged if their
          values of `index` are equal and not None.
          
          Example:
          
          .. code-block:: python
          
              left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
              right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
          
              (
                  AIMessageChunk(content="", tool_call_chunks=left_chunks)
                  + AIMessageChunk(content="", tool_call_chunks=right_chunks)
              ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              .. versionadded:: 0.2.17
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      artifact=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "input_tokens": 10,
                      "output_tokens": 20,
                      "total_tokens": 30
                  }
        ''',
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'CommaSeparatedListOutputParserInput',
  })
# ---
# name: test_schemas[prompt_mapper_output_schema]
  dict({
    'definitions': dict({
      'AIMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Message chunk from an AI.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'additionalProperties': True,
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
                dict({
                  '$ref': '#/definitions/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'PromptTemplateOutput': dict({
        'anyOf': list([
          dict({
            '$ref': '#/definitions/StringPromptValue',
          }),
          dict({
            '$ref': '#/definitions/ChatPromptValueConcrete',
          }),
        ]),
        'title': 'PromptTemplateOutput',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'additionalProperties': True,
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents a request to call a tool.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "name": "foo",
                      "args": {"a": 1},
                      "id": "123"
                  }
          
              This represents a request to call the tool named "foo" with arguments {"a": 1}
              and an identifier of "123".
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (e.g., as part of a stream).
          
          When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
          all string attributes are concatenated. Chunks are only merged if their
          values of `index` are equal and not None.
          
          Example:
          
          .. code-block:: python
          
              left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
              right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
          
              (
                  AIMessageChunk(content="", tool_call_chunks=left_chunks)
                  + AIMessageChunk(content="", tool_call_chunks=right_chunks)
              ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              .. versionadded:: 0.2.17
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      artifact=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "input_tokens": 10,
                      "output_tokens": 20,
                      "total_tokens": 30
                  }
        ''',
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'items': dict({
      '$ref': '#/definitions/PromptTemplateOutput',
    }),
    'title': 'RunnableEach<PromptTemplate>Output',
    'type': 'array',
  })
# ---
# name: test_schemas[prompt_output_schema]
  dict({
    'anyOf': list([
      dict({
        '$ref': '#/definitions/StringPromptValue',
      }),
      dict({
        '$ref': '#/definitions/ChatPromptValueConcrete',
      }),
    ]),
    'definitions': dict({
      'AIMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from an AI.
          
          AIMessage is returned from a chat model as a response to a prompt.
          
          This message represents the output of the model and consists of both
          the raw output as returned by the model together standardized fields
          (e.g., tool calls, usage metadata) added by the LangChain framework.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ai',
            'default': 'ai',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessage',
        'type': 'object',
      }),
      'AIMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Message chunk from an AI.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'invalid_tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/InvalidToolCall',
            }),
            'title': 'Invalid Tool Calls',
            'type': 'array',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'tool_call_chunks': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCallChunk',
            }),
            'title': 'Tool Call Chunks',
            'type': 'array',
          }),
          'tool_calls': dict({
            'default': list([
            ]),
            'items': dict({
              '$ref': '#/definitions/ToolCall',
            }),
            'title': 'Tool Calls',
            'type': 'array',
          }),
          'type': dict({
            'const': 'AIMessageChunk',
            'default': 'AIMessageChunk',
            'title': 'Type',
          }),
          'usage_metadata': dict({
            'anyOf': list([
              dict({
                '$ref': '#/definitions/UsageMetadata',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'AIMessageChunk',
        'type': 'object',
      }),
      'ChatMessage': dict({
        'additionalProperties': True,
        'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'chat',
            'default': 'chat',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessage',
        'type': 'object',
      }),
      'ChatMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Chat Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'role': dict({
            'title': 'Role',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ChatMessageChunk',
            'default': 'ChatMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'role',
        ]),
        'title': 'ChatMessageChunk',
        'type': 'object',
      }),
      'ChatPromptValueConcrete': dict({
        'description': '''
          Chat prompt value which explicitly lists out the message types it accepts.
          For use in external schemas.
        ''',
        'properties': dict({
          'messages': dict({
            'items': dict({
              'oneOf': list([
                dict({
                  '$ref': '#/definitions/AIMessage',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessage',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessage',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessage',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessage',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessage',
                }),
                dict({
                  '$ref': '#/definitions/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/definitions/ToolMessageChunk',
                }),
              ]),
            }),
            'title': 'Messages',
            'type': 'array',
          }),
          'type': dict({
            'const': 'ChatPromptValueConcrete',
            'default': 'ChatPromptValueConcrete',
            'title': 'Type',
          }),
        }),
        'required': list([
          'messages',
        ]),
        'title': 'ChatPromptValueConcrete',
        'type': 'object',
      }),
      'FunctionMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          FunctionMessage are an older version of the ToolMessage schema, and
          do not contain the tool_call_id field.
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'function',
            'default': 'function',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessage',
        'type': 'object',
      }),
      'FunctionMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Function Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'FunctionMessageChunk',
            'default': 'FunctionMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'name',
        ]),
        'title': 'FunctionMessageChunk',
        'type': 'object',
      }),
      'HumanMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message from a human.
          
          HumanMessages are messages that are passed in from a human to the model.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Instantiate a chat model and invoke it with the messages
                  model = ...
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'human',
            'default': 'human',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessage',
        'type': 'object',
      }),
      'HumanMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Human Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'example': dict({
            'default': False,
            'title': 'Example',
            'type': 'boolean',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'HumanMessageChunk',
            'default': 'HumanMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'HumanMessageChunk',
        'type': 'object',
      }),
      'InvalidToolCall': dict({
        'description': '''
          Allowance for errors made by LLM.
          
          Here we add an `error` key to surface errors made during generation
          (e.g., invalid JSON arguments.)
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'error': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Error',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'invalid_tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'error',
        ]),
        'title': 'InvalidToolCall',
        'type': 'object',
      }),
      'StringPromptValue': dict({
        'description': 'String prompt value.',
        'properties': dict({
          'text': dict({
            'title': 'Text',
            'type': 'string',
          }),
          'type': dict({
            'const': 'StringPromptValue',
            'default': 'StringPromptValue',
            'title': 'Type',
          }),
        }),
        'required': list([
          'text',
        ]),
        'title': 'StringPromptValue',
        'type': 'object',
      }),
      'SystemMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for priming AI behavior.
          
          The system message is usually passed in as the first of a sequence
          of input messages.
          
          Example:
          
              .. code-block:: python
          
                  from langchain_core.messages import HumanMessage, SystemMessage
          
                  messages = [
                      SystemMessage(
                          content="You are a helpful assistant! Your name is Bob."
                      ),
                      HumanMessage(
                          content="What is your name?"
                      )
                  ]
          
                  # Define a chat model and invoke it with the messages
                  print(model.invoke(messages))
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'system',
            'default': 'system',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessage',
        'type': 'object',
      }),
      'SystemMessageChunk': dict({
        'additionalProperties': True,
        'description': 'System Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'type': dict({
            'const': 'SystemMessageChunk',
            'default': 'SystemMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
        ]),
        'title': 'SystemMessageChunk',
        'type': 'object',
      }),
      'ToolCall': dict({
        'description': '''
          Represents a request to call a tool.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "name": "foo",
                      "args": {"a": 1},
                      "id": "123"
                  }
          
              This represents a request to call the tool named "foo" with arguments {"a": 1}
              and an identifier of "123".
        ''',
        'properties': dict({
          'args': dict({
            'title': 'Args',
            'type': 'object',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'name': dict({
            'title': 'Name',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool_call',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
        ]),
        'title': 'ToolCall',
        'type': 'object',
      }),
      'ToolCallChunk': dict({
        'description': '''
          A chunk of a tool call (e.g., as part of a stream).
          
          When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
          all string attributes are concatenated. Chunks are only merged if their
          values of `index` are equal and not None.
          
          Example:
          
          .. code-block:: python
          
              left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
              right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
          
              (
                  AIMessageChunk(content="", tool_call_chunks=left_chunks)
                  + AIMessageChunk(content="", tool_call_chunks=right_chunks)
              ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
        ''',
        'properties': dict({
          'args': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Args',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Id',
          }),
          'index': dict({
            'anyOf': list([
              dict({
                'type': 'integer',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Index',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'title': 'Name',
          }),
          'type': dict({
            'const': 'tool_call_chunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'name',
          'args',
          'id',
          'index',
        ]),
        'title': 'ToolCallChunk',
        'type': 'object',
      }),
      'ToolMessage': dict({
        'additionalProperties': True,
        'description': '''
          Message for passing the result of executing a tool back to a model.
          
          ToolMessages contain the result of a tool invocation. Typically, the result
          is encoded inside the `content` field.
          
          Example: A ToolMessage representing a result of 42 from a tool call with id
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
          
          
          Example: A ToolMessage where only part of the tool output is sent to the model
              and the full output is passed in to artifact.
          
              .. versionadded:: 0.2.17
          
              .. code-block:: python
          
                  from langchain_core.messages import ToolMessage
          
                  tool_output = {
                      "stdout": "From the graph we can see that the correlation between x and y is ...",
                      "stderr": None,
                      "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                  }
          
                  ToolMessage(
                      content=tool_output["stdout"],
                      artifact=tool_output,
                      tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                  )
          
          The tool_call_id field is used to associate the tool call request with the
          tool call response. This is useful in situations where a chat model is able
          to request multiple tool calls in parallel.
        ''',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'tool',
            'default': 'tool',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessage',
        'type': 'object',
      }),
      'ToolMessageChunk': dict({
        'additionalProperties': True,
        'description': 'Tool Message chunk.',
        'properties': dict({
          'additional_kwargs': dict({
            'title': 'Additional Kwargs',
            'type': 'object',
          }),
          'artifact': dict({
            'title': 'Artifact',
          }),
          'content': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'items': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'object',
                    }),
                  ]),
                }),
                'type': 'array',
              }),
            ]),
            'title': 'Content',
          }),
          'id': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Id',
          }),
          'name': dict({
            'anyOf': list([
              dict({
                'type': 'string',
              }),
              dict({
                'type': 'null',
              }),
            ]),
            'default': None,
            'title': 'Name',
          }),
          'response_metadata': dict({
            'title': 'Response Metadata',
            'type': 'object',
          }),
          'status': dict({
            'default': 'success',
            'title': 'Status',
          }),
          'tool_call_id': dict({
            'title': 'Tool Call Id',
            'type': 'string',
          }),
          'type': dict({
            'const': 'ToolMessageChunk',
            'default': 'ToolMessageChunk',
            'title': 'Type',
          }),
        }),
        'required': list([
          'content',
          'tool_call_id',
        ]),
        'title': 'ToolMessageChunk',
        'type': 'object',
      }),
      'UsageMetadata': dict({
        'description': '''
          Usage metadata for a message, such as token counts.
          
          This is a standard representation of token usage that is consistent across models.
          
          Example:
          
              .. code-block:: python
          
                  {
                      "input_tokens": 10,
                      "output_tokens": 20,
                      "total_tokens": 30
                  }
        ''',
        'properties': dict({
          'input_tokens': dict({
            'title': 'Input Tokens',
            'type': 'integer',
          }),
          'output_tokens': dict({
            'title': 'Output Tokens',
            'type': 'integer',
          }),
          'total_tokens': dict({
            'title': 'Total Tokens',
            'type': 'integer',
          }),
        }),
        'required': list([
          'input_tokens',
          'output_tokens',
          'total_tokens',
        ]),
        'title': 'UsageMetadata',
        'type': 'object',
      }),
    }),
    'title': 'PromptTemplateOutput',
  })
# ---
# name: test_seq_dict_prompt_llm
  '''
  {
    question: RunnablePassthrough[str]()
              | RunnableLambda(...),
    documents: RunnableLambda(...)
               | FakeRetriever(),
    just_to_test_lambda: RunnableLambda(...)
  }
  | ChatPromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='Context:\n{documents}\n\nQuestion:\n{question}'), additional_kwargs={})])
  | FakeListChatModel(responses=['foo, bar'])
  | CommaSeparatedListOutputParser()
  '''
# ---
# name: test_seq_dict_prompt_llm.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "question": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "schema",
                    "runnable",
                    "RunnablePassthrough"
                  ],
                  "kwargs": {},
                  "name": "RunnablePassthrough"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "runnables",
                    "base",
                    "RunnableLambda"
                  ],
                  "repr": "RunnableLambda(...)"
                }
              },
              "name": "RunnableSequence"
            },
            "documents": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableSequence"
              ],
              "kwargs": {
                "first": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "runnables",
                    "base",
                    "RunnableLambda"
                  ],
                  "repr": "RunnableLambda(...)"
                },
                "last": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "tests",
                    "unit_tests",
                    "runnables",
                    "test_runnable",
                    "FakeRetriever"
                  ],
                  "repr": "FakeRetriever()",
                  "name": "FakeRetriever"
                }
              },
              "name": "RunnableSequence"
            },
            "just_to_test_lambda": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            }
          }
        },
        "name": "RunnableParallel<question,documents,just_to_test_lambda>"
      },
      "middle": [
        {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "prompts",
            "chat",
            "ChatPromptTemplate"
          ],
          "kwargs": {
            "input_variables": [
              "documents",
              "question"
            ],
            "messages": [
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "SystemMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [],
                      "template": "You are a nice assistant.",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              },
              {
                "lc": 1,
                "type": "constructor",
                "id": [
                  "langchain",
                  "prompts",
                  "chat",
                  "HumanMessagePromptTemplate"
                ],
                "kwargs": {
                  "prompt": {
                    "lc": 1,
                    "type": "constructor",
                    "id": [
                      "langchain",
                      "prompts",
                      "prompt",
                      "PromptTemplate"
                    ],
                    "kwargs": {
                      "input_variables": [
                        "documents",
                        "question"
                      ],
                      "template": "Context:\n{documents}\n\nQuestion:\n{question}",
                      "template_format": "f-string"
                    },
                    "name": "PromptTemplate"
                  }
                }
              }
            ]
          },
          "name": "ChatPromptTemplate"
        },
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "language_models",
            "fake_chat_models",
            "FakeListChatModel"
          ],
          "repr": "FakeListChatModel(responses=['foo, bar'])",
          "name": "FakeListChatModel"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "output_parsers",
          "list",
          "CommaSeparatedListOutputParser"
        ],
        "kwargs": {},
        "name": "CommaSeparatedListOutputParser"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_seq_prompt_dict
  '''
  ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a nice assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])
  | RunnableLambda(...)
  | {
      chat: FakeListChatModel(responses=["i'm a chatbot"]),
      llm: FakeListLLM(responses=["i'm a textbot"])
    }
  '''
# ---
# name: test_seq_prompt_dict.1
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(...)"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "chat": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake_chat_models",
                "FakeListChatModel"
              ],
              "repr": "FakeListChatModel(responses=[\"i'm a chatbot\"])",
              "name": "FakeListChatModel"
            },
            "llm": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake",
                "FakeListLLM"
              ],
              "repr": "FakeListLLM(responses=[\"i'm a textbot\"])",
              "name": "FakeListLLM"
            }
          }
        },
        "name": "RunnableParallel<chat,llm>"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
# name: test_seq_prompt_map
  '''
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "runnable",
      "RunnableSequence"
    ],
    "kwargs": {
      "first": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "prompts",
          "chat",
          "ChatPromptTemplate"
        ],
        "kwargs": {
          "input_variables": [
            "question"
          ],
          "messages": [
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "SystemMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [],
                    "template": "You are a nice assistant.",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            },
            {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "prompts",
                "chat",
                "HumanMessagePromptTemplate"
              ],
              "kwargs": {
                "prompt": {
                  "lc": 1,
                  "type": "constructor",
                  "id": [
                    "langchain",
                    "prompts",
                    "prompt",
                    "PromptTemplate"
                  ],
                  "kwargs": {
                    "input_variables": [
                      "question"
                    ],
                    "template": "{question}",
                    "template_format": "f-string"
                  },
                  "name": "PromptTemplate"
                }
              }
            }
          ]
        },
        "name": "ChatPromptTemplate"
      },
      "middle": [
        {
          "lc": 1,
          "type": "not_implemented",
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "repr": "RunnableLambda(...)"
        }
      ],
      "last": {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "runnable",
          "RunnableParallel"
        ],
        "kwargs": {
          "steps__": {
            "chat": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain",
                "schema",
                "runnable",
                "RunnableBinding"
              ],
              "kwargs": {
                "bound": {
                  "lc": 1,
                  "type": "not_implemented",
                  "id": [
                    "langchain_core",
                    "language_models",
                    "fake_chat_models",
                    "FakeListChatModel"
                  ],
                  "repr": "FakeListChatModel(responses=[\"i'm a chatbot\"])",
                  "name": "FakeListChatModel"
                },
                "kwargs": {
                  "stop": [
                    "Thought:"
                  ]
                }
              },
              "name": "FakeListChatModel"
            },
            "llm": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "language_models",
                "fake",
                "FakeListLLM"
              ],
              "repr": "FakeListLLM(responses=[\"i'm a textbot\"])",
              "name": "FakeListLLM"
            },
            "passthrough": {
              "lc": 1,
              "type": "not_implemented",
              "id": [
                "langchain_core",
                "runnables",
                "base",
                "RunnableLambda"
              ],
              "repr": "RunnableLambda(...)"
            }
          }
        },
        "name": "RunnableParallel<chat,llm,passthrough>"
      }
    },
    "name": "RunnableSequence"
  }
  '''
# ---
