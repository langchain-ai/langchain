# serializer version: 1
# name: test_double_nested_subgraph_mermaid[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	parent_1(parent_1)
  	parent_2(parent_2)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> parent_1;
  	child\3achild_2 --> parent_2;
  	parent_1 --> child\3achild_1\3agrandchild_1;
  	parent_2 --> __end__;
  	subgraph child
  	child\3achild_2(child_2)
  	child\3achild_1\3agrandchild_2 --> child\3achild_2;
  	subgraph child_1
  	child\3achild_1\3agrandchild_1(grandchild_1)
  	child\3achild_1\3agrandchild_2(grandchild_2<hr/><small><em>__interrupt = before</em></small>)
  	child\3achild_1\3agrandchild_1 --> child\3achild_1\3agrandchild_2;
  	end
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_graph_mermaid_duplicate_nodes[mermaid]
  '''
  graph TD;
  	PromptInput --> PromptTemplate_1;
  	Parallel\3cllm1\2cllm2\3eInput --> FakeListLLM_1;
  	FakeListLLM_1 --> Parallel\3cllm1\2cllm2\3eOutput;
  	Parallel\3cllm1\2cllm2\3eInput --> FakeListLLM_2;
  	FakeListLLM_2 --> Parallel\3cllm1\2cllm2\3eOutput;
  	PromptTemplate_1 --> Parallel\3cllm1\2cllm2\3eInput;
  	PromptTemplate_2 --> PromptTemplateOutput;
  	Parallel\3cllm1\2cllm2\3eOutput --> PromptTemplate_2;
  
  '''
# ---
# name: test_graph_mermaid_frontmatter_config[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
    look: handDrawn
    theme: neutral
    themeVariables:
      primaryColor: '#e2e2e2'
  ---
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	my_node([my_node]):::last
  	__start__ --> my_node;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_graph_mermaid_special_chars[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	\5f00\59cb(开始)
  	\7ed3\675f(结束)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> \5f00\59cb;
  	\5f00\59cb --> \7ed3\675f;
  	\7ed3\675f --> __end__;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_graph_sequence[ascii]
  '''
              +-------------+              
              | PromptInput |              
              +-------------+              
                      *                    
                      *                    
                      *                    
             +----------------+            
             | PromptTemplate |            
             +----------------+            
                      *                    
                      *                    
                      *                    
              +-------------+              
              | FakeListLLM |              
              +-------------+              
                      *                    
                      *                    
                      *                    
     +--------------------------------+    
     | CommaSeparatedListOutputParser |    
     +--------------------------------+    
                      *                    
                      *                    
                      *                    
  +--------------------------------------+ 
  | CommaSeparatedListOutputParserOutput | 
  +--------------------------------------+ 
  '''
# ---
# name: test_graph_sequence[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
  	PromptInput([PromptInput]):::first
  	PromptTemplate(PromptTemplate)
  	FakeListLLM(FakeListLLM<hr/><small><em>key = 2</em></small>)
  	CommaSeparatedListOutputParser(CommaSeparatedListOutputParser)
  	CommaSeparatedListOutputParserOutput([CommaSeparatedListOutputParserOutput]):::last
  	PromptInput --> PromptTemplate;
  	PromptTemplate --> FakeListLLM;
  	CommaSeparatedListOutputParser --> CommaSeparatedListOutputParserOutput;
  	FakeListLLM --> CommaSeparatedListOutputParser;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_graph_sequence_map[ascii]
  '''
                                             +-------------+                                                 
                                             | PromptInput |                                                 
                                             +-------------+                                                 
                                                     *                                                       
                                                     *                                                       
                                                     *                                                       
                                            +----------------+                                               
                                            | PromptTemplate |                                               
                                            +----------------+                                               
                                                     *                                                       
                                                     *                                                       
                                                     *                                                       
                                             +-------------+                                                 
                                             | FakeListLLM |                                                 
                                             +-------------+                                                 
                                                     *                                                       
                                                     *                                                       
                                                     *                                                       
                                    +-------------------------------+                                        
                                    | Parallel<as_list,as_str>Input |                                        
                                    +-------------------------------+                                        
                                       *****                         ******                                  
                                    ***                                    ******                            
                                 ***                                             ******                      
            +------------------------------+                                           ****                  
            | conditional_str_parser_input |                                              *                  
            +------------------------------+                                              *                  
                   ***              ***                                                   *                  
                ***                    ***                                                *                  
              **                          **                                              *                  
  +-----------------+               +-----------------+                                   *                  
  | StrOutputParser |               | XMLOutputParser |                                   *                  
  +-----------------+               +-----------------+                                   *                  
                   ***              ***                                                   *                  
                      ***        ***                                                      *                  
                         **    **                                                         *                  
            +-------------------------------+                            +--------------------------------+  
            | conditional_str_parser_output |                            | CommaSeparatedListOutputParser |  
            +-------------------------------+                            +--------------------------------+  
                                       *****                         ******                                  
                                            ***                ******                                        
                                               ***         ****                                              
                                    +--------------------------------+                                       
                                    | Parallel<as_list,as_str>Output |                                       
                                    +--------------------------------+                                       
  '''
# ---
# name: test_graph_sequence_map[graph_no_schemas]
  dict({
    'edges': list([
      dict({
        'source': 0,
        'target': 1,
      }),
      dict({
        'source': 1,
        'target': 2,
      }),
      dict({
        'source': 3,
        'target': 5,
      }),
      dict({
        'source': 5,
        'target': 4,
      }),
      dict({
        'source': 6,
        'target': 8,
      }),
      dict({
        'source': 8,
        'target': 7,
      }),
      dict({
        'source': 6,
        'target': 9,
      }),
      dict({
        'source': 9,
        'target': 7,
      }),
      dict({
        'source': 3,
        'target': 6,
      }),
      dict({
        'source': 7,
        'target': 4,
      }),
      dict({
        'source': 2,
        'target': 3,
      }),
    ]),
    'nodes': list([
      dict({
        'data': 'PromptInput',
        'id': 0,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'prompts',
            'prompt',
            'PromptTemplate',
          ]),
          'name': 'PromptTemplate',
        }),
        'id': 1,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain_core',
            'language_models',
            'fake',
            'FakeListLLM',
          ]),
          'name': 'FakeListLLM',
        }),
        'id': 2,
        'type': 'runnable',
      }),
      dict({
        'data': 'Parallel<as_list,as_str>Input',
        'id': 3,
        'type': 'schema',
      }),
      dict({
        'data': 'Parallel<as_list,as_str>Output',
        'id': 4,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'output_parsers',
            'list',
            'CommaSeparatedListOutputParser',
          ]),
          'name': 'CommaSeparatedListOutputParser',
        }),
        'id': 5,
        'type': 'runnable',
      }),
      dict({
        'data': 'conditional_str_parser_input',
        'id': 6,
        'type': 'schema',
      }),
      dict({
        'data': 'conditional_str_parser_output',
        'id': 7,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'output_parser',
            'StrOutputParser',
          ]),
          'name': 'StrOutputParser',
        }),
        'id': 8,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain_core',
            'output_parsers',
            'xml',
            'XMLOutputParser',
          ]),
          'name': 'XMLOutputParser',
        }),
        'id': 9,
        'type': 'runnable',
      }),
    ]),
  })
# ---
# name: test_graph_sequence_map[graph_with_schema]
  dict({
    'edges': list([
      dict({
        'source': 0,
        'target': 1,
      }),
      dict({
        'source': 1,
        'target': 2,
      }),
      dict({
        'source': 3,
        'target': 5,
      }),
      dict({
        'source': 5,
        'target': 4,
      }),
      dict({
        'source': 6,
        'target': 8,
      }),
      dict({
        'source': 8,
        'target': 7,
      }),
      dict({
        'source': 6,
        'target': 9,
      }),
      dict({
        'source': 9,
        'target': 7,
      }),
      dict({
        'source': 3,
        'target': 6,
      }),
      dict({
        'source': 7,
        'target': 4,
      }),
      dict({
        'source': 2,
        'target': 3,
      }),
    ]),
    'nodes': list([
      dict({
        'data': dict({
          'properties': dict({
            'name': dict({
              'title': 'Name',
              'type': 'string',
            }),
          }),
          'required': list([
            'name',
          ]),
          'title': 'PromptInput',
          'type': 'object',
        }),
        'id': 0,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'prompts',
            'prompt',
            'PromptTemplate',
          ]),
          'name': 'PromptTemplate',
        }),
        'id': 1,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain_core',
            'language_models',
            'fake',
            'FakeListLLM',
          ]),
          'name': 'FakeListLLM',
        }),
        'id': 2,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          '$defs': dict({
            'AIMessage': dict({
              'description': '''
                Message from an AI.
                
                An `AIMessage` is returned from a chat model as a response to a prompt.
                
                This message represents the output of the model and consists of both
                the raw output as returned by the model and standardized fields
                (e.g., tool calls, usage metadata) added by the LangChain framework.
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'invalid_tool_calls': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/InvalidToolCall',
                  }),
                  'title': 'Invalid Tool Calls',
                  'type': 'array',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'tool_calls': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/ToolCall',
                  }),
                  'title': 'Tool Calls',
                  'type': 'array',
                }),
                'type': dict({
                  'const': 'ai',
                  'default': 'ai',
                  'title': 'Type',
                  'type': 'string',
                }),
                'usage_metadata': dict({
                  'anyOf': list([
                    dict({
                      '$ref': '#/$defs/UsageMetadata',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'AIMessage',
              'type': 'object',
            }),
            'AIMessageChunk': dict({
              'description': 'Message chunk from an AI (yielded when streaming).',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'chunk_position': dict({
                  'anyOf': list([
                    dict({
                      'const': 'last',
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Chunk Position',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'invalid_tool_calls': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/InvalidToolCall',
                  }),
                  'title': 'Invalid Tool Calls',
                  'type': 'array',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'tool_call_chunks': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/ToolCallChunk',
                  }),
                  'title': 'Tool Call Chunks',
                  'type': 'array',
                }),
                'tool_calls': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/ToolCall',
                  }),
                  'title': 'Tool Calls',
                  'type': 'array',
                }),
                'type': dict({
                  'const': 'AIMessageChunk',
                  'default': 'AIMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
                'usage_metadata': dict({
                  'anyOf': list([
                    dict({
                      '$ref': '#/$defs/UsageMetadata',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'AIMessageChunk',
              'type': 'object',
            }),
            'ChatMessage': dict({
              'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'role': dict({
                  'title': 'Role',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'chat',
                  'default': 'chat',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'role',
              ]),
              'title': 'ChatMessage',
              'type': 'object',
            }),
            'ChatMessageChunk': dict({
              'description': 'Chat Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'role': dict({
                  'title': 'Role',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'ChatMessageChunk',
                  'default': 'ChatMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'role',
              ]),
              'title': 'ChatMessageChunk',
              'type': 'object',
            }),
            'FunctionMessage': dict({
              'description': '''
                Message for passing the result of executing a tool back to a model.
                
                `FunctionMessage` are an older version of the `ToolMessage` schema, and
                do not contain the `tool_call_id` field.
                
                The `tool_call_id` field is used to associate the tool call request with the
                tool call response. Useful in situations where a chat model is able
                to request multiple tool calls in parallel.
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'title': 'Name',
                  'type': 'string',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'function',
                  'default': 'function',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'name',
              ]),
              'title': 'FunctionMessage',
              'type': 'object',
            }),
            'FunctionMessageChunk': dict({
              'description': 'Function Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'title': 'Name',
                  'type': 'string',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'FunctionMessageChunk',
                  'default': 'FunctionMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'name',
              ]),
              'title': 'FunctionMessageChunk',
              'type': 'object',
            }),
            'HumanMessage': dict({
              'description': '''
                Message from the user.
                
                A `HumanMessage` is a message that is passed in from a user to the model.
                
                Example:
                    ```python
                    from langchain_core.messages import HumanMessage, SystemMessage
                
                    messages = [
                        SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                        HumanMessage(content="What is your name?"),
                    ]
                
                    # Instantiate a chat model and invoke it with the messages
                    model = ...
                    print(model.invoke(messages))
                    ```
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'human',
                  'default': 'human',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'HumanMessage',
              'type': 'object',
            }),
            'HumanMessageChunk': dict({
              'description': 'Human Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'HumanMessageChunk',
                  'default': 'HumanMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'HumanMessageChunk',
              'type': 'object',
            }),
            'InputTokenDetails': dict({
              'description': '''
                Breakdown of input token counts.
                
                Does *not* need to sum to full input token count. Does *not* need to have all keys.
                
                Example:
                    ```python
                    {
                        "audio": 10,
                        "cache_creation": 200,
                        "cache_read": 100,
                    }
                    ```
                
                May also hold extra provider-specific keys.
                
                !!! version-added "Added in version 0.3.9"
              ''',
              'properties': dict({
                'audio': dict({
                  'title': 'Audio',
                  'type': 'integer',
                }),
                'cache_creation': dict({
                  'title': 'Cache Creation',
                  'type': 'integer',
                }),
                'cache_read': dict({
                  'title': 'Cache Read',
                  'type': 'integer',
                }),
              }),
              'title': 'InputTokenDetails',
              'type': 'object',
            }),
            'InvalidToolCall': dict({
              'description': '''
                Allowance for errors made by LLM.
                
                Here we add an `error` key to surface errors made during generation
                (e.g., invalid JSON arguments.)
              ''',
              'properties': dict({
                'args': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Args',
                }),
                'error': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Error',
                }),
                'extras': dict({
                  'title': 'Extras',
                  'type': 'object',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Id',
                }),
                'index': dict({
                  'anyOf': list([
                    dict({
                      'type': 'integer',
                    }),
                    dict({
                      'type': 'string',
                    }),
                  ]),
                  'title': 'Index',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Name',
                }),
                'type': dict({
                  'const': 'invalid_tool_call',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'type',
                'id',
                'name',
                'args',
                'error',
              ]),
              'title': 'InvalidToolCall',
              'type': 'object',
            }),
            'OutputTokenDetails': dict({
              'description': '''
                Breakdown of output token counts.
                
                Does *not* need to sum to full output token count. Does *not* need to have all keys.
                
                Example:
                    ```python
                    {
                        "audio": 10,
                        "reasoning": 200,
                    }
                    ```
                
                May also hold extra provider-specific keys.
                
                !!! version-added "Added in version 0.3.9"
              ''',
              'properties': dict({
                'audio': dict({
                  'title': 'Audio',
                  'type': 'integer',
                }),
                'reasoning': dict({
                  'title': 'Reasoning',
                  'type': 'integer',
                }),
              }),
              'title': 'OutputTokenDetails',
              'type': 'object',
            }),
            'SystemMessage': dict({
              'description': '''
                Message for priming AI behavior.
                
                The system message is usually passed in as the first of a sequence
                of input messages.
                
                Example:
                    ```python
                    from langchain_core.messages import HumanMessage, SystemMessage
                
                    messages = [
                        SystemMessage(content="You are a helpful assistant! Your name is Bob."),
                        HumanMessage(content="What is your name?"),
                    ]
                
                    # Define a chat model and invoke it with the messages
                    print(model.invoke(messages))
                    ```
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'system',
                  'default': 'system',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'SystemMessage',
              'type': 'object',
            }),
            'SystemMessageChunk': dict({
              'description': 'System Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'SystemMessageChunk',
                  'default': 'SystemMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'SystemMessageChunk',
              'type': 'object',
            }),
            'ToolCall': dict({
              'description': '''
                Represents an AI's request to call a tool.
                
                Example:
                    ```python
                    {"name": "foo", "args": {"a": 1}, "id": "123"}
                    ```
                
                    This represents a request to call the tool named `'foo'` with arguments
                    `{"a": 1}` and an identifier of `'123'`.
              ''',
              'properties': dict({
                'args': dict({
                  'title': 'Args',
                  'type': 'object',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Id',
                }),
                'name': dict({
                  'title': 'Name',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'tool_call',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'name',
                'args',
                'id',
              ]),
              'title': 'ToolCall',
              'type': 'object',
            }),
            'ToolCallChunk': dict({
              'description': '''
                A chunk of a tool call (yielded when streaming).
                
                When merging `ToolCallChunk`s (e.g., via `AIMessageChunk.__add__`),
                all string attributes are concatenated. Chunks are only merged if their
                values of `index` are equal and not None.
                
                Example:
                ```python
                left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
                right_chunks = [ToolCallChunk(name=None, args="1}", index=0)]
                
                (
                    AIMessageChunk(content="", tool_call_chunks=left_chunks)
                    + AIMessageChunk(content="", tool_call_chunks=right_chunks)
                ).tool_call_chunks == [ToolCallChunk(name="foo", args='{"a":1}', index=0)]
                ```
              ''',
              'properties': dict({
                'args': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Args',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Id',
                }),
                'index': dict({
                  'anyOf': list([
                    dict({
                      'type': 'integer',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Index',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Name',
                }),
                'type': dict({
                  'const': 'tool_call_chunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'name',
                'args',
                'id',
                'index',
              ]),
              'title': 'ToolCallChunk',
              'type': 'object',
            }),
            'ToolMessage': dict({
              'description': '''
                Message for passing the result of executing a tool back to a model.
                
                `ToolMessage` objects contain the result of a tool invocation. Typically, the result
                is encoded inside the `content` field.
                
                Example: A `ToolMessage` representing a result of `42` from a tool call with id
                
                ```python
                from langchain_core.messages import ToolMessage
                
                ToolMessage(content="42", tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL")
                ```
                
                Example: A `ToolMessage` where only part of the tool output is sent to the model
                and the full output is passed in to artifact.
                
                ```python
                from langchain_core.messages import ToolMessage
                
                tool_output = {
                    "stdout": "From the graph we can see that the correlation between "
                    "x and y is ...",
                    "stderr": None,
                    "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                }
                
                ToolMessage(
                    content=tool_output["stdout"],
                    artifact=tool_output,
                    tool_call_id="call_Jja7J89XsjrOLA5r!MEOW!SL",
                )
                ```
                
                The `tool_call_id` field is used to associate the tool call request with the
                tool call response. Useful in situations where a chat model is able
                to request multiple tool calls in parallel.
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'artifact': dict({
                  'title': 'Artifact',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'status': dict({
                  'default': 'success',
                  'title': 'Status',
                }),
                'tool_call_id': dict({
                  'title': 'Tool Call Id',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'tool',
                  'default': 'tool',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'tool_call_id',
              ]),
              'title': 'ToolMessage',
              'type': 'object',
            }),
            'ToolMessageChunk': dict({
              'description': 'Tool Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'artifact': dict({
                  'title': 'Artifact',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'status': dict({
                  'default': 'success',
                  'title': 'Status',
                }),
                'tool_call_id': dict({
                  'title': 'Tool Call Id',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'ToolMessageChunk',
                  'default': 'ToolMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'tool_call_id',
              ]),
              'title': 'ToolMessageChunk',
              'type': 'object',
            }),
            'UsageMetadata': dict({
              'description': '''
                Usage metadata for a message, such as token counts.
                
                This is a standard representation of token usage that is consistent across models.
                
                Example:
                    ```python
                    {
                        "input_tokens": 350,
                        "output_tokens": 240,
                        "total_tokens": 590,
                        "input_token_details": {
                            "audio": 10,
                            "cache_creation": 200,
                            "cache_read": 100,
                        },
                        "output_token_details": {
                            "audio": 10,
                            "reasoning": 200,
                        },
                    }
                    ```
                
                !!! warning "Behavior changed in 0.3.9"
                    Added `input_token_details` and `output_token_details`.
              ''',
              'properties': dict({
                'input_token_details': dict({
                  '$ref': '#/$defs/InputTokenDetails',
                }),
                'input_tokens': dict({
                  'title': 'Input Tokens',
                  'type': 'integer',
                }),
                'output_token_details': dict({
                  '$ref': '#/$defs/OutputTokenDetails',
                }),
                'output_tokens': dict({
                  'title': 'Output Tokens',
                  'type': 'integer',
                }),
                'total_tokens': dict({
                  'title': 'Total Tokens',
                  'type': 'integer',
                }),
              }),
              'required': list([
                'input_tokens',
                'output_tokens',
                'total_tokens',
              ]),
              'title': 'UsageMetadata',
              'type': 'object',
            }),
          }),
          'anyOf': list([
            dict({
              'type': 'string',
            }),
            dict({
              'oneOf': list([
                dict({
                  '$ref': '#/$defs/AIMessage',
                }),
                dict({
                  '$ref': '#/$defs/HumanMessage',
                }),
                dict({
                  '$ref': '#/$defs/ChatMessage',
                }),
                dict({
                  '$ref': '#/$defs/SystemMessage',
                }),
                dict({
                  '$ref': '#/$defs/FunctionMessage',
                }),
                dict({
                  '$ref': '#/$defs/ToolMessage',
                }),
                dict({
                  '$ref': '#/$defs/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/ToolMessageChunk',
                }),
              ]),
            }),
          ]),
          'title': 'RunnableParallel<as_list,as_str>Input',
        }),
        'id': 3,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'properties': dict({
            'as_list': dict({
              'items': dict({
                'type': 'string',
              }),
              'title': 'As List',
              'type': 'array',
            }),
            'as_str': dict({
              'title': 'As Str',
            }),
          }),
          'required': list([
            'as_list',
            'as_str',
          ]),
          'title': 'RunnableParallel<as_list,as_str>Output',
          'type': 'object',
        }),
        'id': 4,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'output_parsers',
            'list',
            'CommaSeparatedListOutputParser',
          ]),
          'name': 'CommaSeparatedListOutputParser',
        }),
        'id': 5,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'title': 'conditional_str_parser_input',
          'type': 'string',
        }),
        'id': 6,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'title': 'conditional_str_parser_output',
        }),
        'id': 7,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'output_parser',
            'StrOutputParser',
          ]),
          'name': 'StrOutputParser',
        }),
        'id': 8,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain_core',
            'output_parsers',
            'xml',
            'XMLOutputParser',
          ]),
          'name': 'XMLOutputParser',
        }),
        'id': 9,
        'type': 'runnable',
      }),
    ]),
  })
# ---
# name: test_graph_sequence_map[mermaid-simple]
  '''
  graph TD;
  	PromptInput --> PromptTemplate;
  	PromptTemplate --> FakeListLLM;
  	Parallel\3cas_list\2cas_str\3eInput --> CommaSeparatedListOutputParser;
  	CommaSeparatedListOutputParser --> Parallel\3cas_list\2cas_str\3eOutput;
  	conditional_str_parser_input --> StrOutputParser;
  	StrOutputParser --> conditional_str_parser_output;
  	conditional_str_parser_input --> XMLOutputParser;
  	XMLOutputParser --> conditional_str_parser_output;
  	Parallel\3cas_list\2cas_str\3eInput --> conditional_str_parser_input;
  	conditional_str_parser_output --> Parallel\3cas_list\2cas_str\3eOutput;
  	FakeListLLM --> Parallel\3cas_list\2cas_str\3eInput;
  
  '''
# ---
# name: test_graph_sequence_map[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
  	PromptInput([PromptInput]):::first
  	PromptTemplate(PromptTemplate)
  	FakeListLLM(FakeListLLM)
  	Parallel\3cas_list\2cas_str\3eInput(Parallel<as_list,as_str>Input)
  	Parallel\3cas_list\2cas_str\3eOutput([Parallel<as_list,as_str>Output]):::last
  	CommaSeparatedListOutputParser(CommaSeparatedListOutputParser)
  	conditional_str_parser_input(conditional_str_parser_input)
  	conditional_str_parser_output(conditional_str_parser_output)
  	StrOutputParser(StrOutputParser)
  	XMLOutputParser(XMLOutputParser)
  	PromptInput --> PromptTemplate;
  	PromptTemplate --> FakeListLLM;
  	Parallel\3cas_list\2cas_str\3eInput --> CommaSeparatedListOutputParser;
  	CommaSeparatedListOutputParser --> Parallel\3cas_list\2cas_str\3eOutput;
  	conditional_str_parser_input --> StrOutputParser;
  	StrOutputParser --> conditional_str_parser_output;
  	conditional_str_parser_input --> XMLOutputParser;
  	XMLOutputParser --> conditional_str_parser_output;
  	Parallel\3cas_list\2cas_str\3eInput --> conditional_str_parser_input;
  	conditional_str_parser_output --> Parallel\3cas_list\2cas_str\3eOutput;
  	FakeListLLM --> Parallel\3cas_list\2cas_str\3eInput;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_graph_single_runnable[ascii]
  '''
  +----------------------+   
  | StrOutputParserInput |   
  +----------------------+   
              *              
              *              
              *              
     +-----------------+     
     | StrOutputParser |     
     +-----------------+     
              *              
              *              
              *              
  +-----------------------+  
  | StrOutputParserOutput |  
  +-----------------------+  
  '''
# ---
# name: test_graph_single_runnable[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
  	StrOutputParserInput([StrOutputParserInput]):::first
  	StrOutputParser(StrOutputParser)
  	StrOutputParserOutput([StrOutputParserOutput]):::last
  	StrOutputParserInput --> StrOutputParser;
  	StrOutputParser --> StrOutputParserOutput;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_parallel_subgraph_mermaid[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	outer_1(outer_1)
  	outer_2(outer_2)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> outer_1;
  	inner_1\3ainner_2 --> outer_2;
  	inner_2\3ainner_2 --> outer_2;
  	outer_1 --> inner_1\3ainner_1;
  	outer_1 --> inner_2\3ainner_1;
  	outer_2 --> __end__;
  	subgraph inner_1
  	inner_1\3ainner_1(inner_1)
  	inner_1\3ainner_2(inner_2<hr/><small><em>__interrupt = before</em></small>)
  	inner_1\3ainner_1 --> inner_1\3ainner_2;
  	end
  	subgraph inner_2
  	inner_2\3ainner_1(inner_1)
  	inner_2\3ainner_2(inner_2)
  	inner_2\3ainner_1 --> inner_2\3ainner_2;
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_single_node_subgraph_mermaid[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	__end__([<p>__end__</p>]):::last
  	__start__ --> sub\3ameow;
  	sub\3ameow --> __end__;
  	subgraph sub
  	sub\3ameow(meow)
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_trim
  dict({
    'edges': list([
      dict({
        'source': '__start__',
        'target': 'ask_question',
      }),
      dict({
        'source': 'ask_question',
        'target': 'answer_question',
      }),
      dict({
        'conditional': True,
        'source': 'answer_question',
        'target': 'ask_question',
      }),
      dict({
        'conditional': True,
        'source': 'answer_question',
        'target': '__end__',
      }),
    ]),
    'nodes': list([
      dict({
        'data': '__start__',
        'id': '__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'output_parser',
            'StrOutputParser',
          ]),
          'name': 'ask_question',
        }),
        'id': 'ask_question',
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'output_parser',
            'StrOutputParser',
          ]),
          'name': 'answer_question',
        }),
        'id': 'answer_question',
        'type': 'runnable',
      }),
      dict({
        'data': '__end__',
        'id': '__end__',
        'type': 'schema',
      }),
    ]),
  })
# ---
# name: test_triple_nested_subgraph_mermaid[mermaid]
  '''
  ---
  config:
    flowchart:
      curve: linear
  ---
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	parent_1(parent_1)
  	parent_2(parent_2)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> parent_1;
  	child\3achild_2 --> parent_2;
  	parent_1 --> child\3achild_1\3agrandchild_1;
  	parent_2 --> __end__;
  	subgraph child
  	child\3achild_2(child_2)
  	child\3achild_1\3agrandchild_2 --> child\3achild_2;
  	subgraph child_1
  	child\3achild_1\3agrandchild_1(grandchild_1)
  	child\3achild_1\3agrandchild_2(grandchild_2<hr/><small><em>__interrupt = before</em></small>)
  	child\3achild_1\3agrandchild_1\3agreatgrandchild --> child\3achild_1\3agrandchild_2;
  	subgraph grandchild_1
  	child\3achild_1\3agrandchild_1\3agreatgrandchild(greatgrandchild)
  	child\3achild_1\3agrandchild_1 --> child\3achild_1\3agrandchild_1\3agreatgrandchild;
  	end
  	end
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
