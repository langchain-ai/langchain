# serializer version: 1
# name: test_double_nested_subgraph_mermaid[mermaid]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	parent_1(parent_1)
  	parent_2(parent_2)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> parent_1;
  	child_child_2 --> parent_2;
  	parent_1 --> child_child_1_grandchild_1;
  	parent_2 --> __end__;
  	subgraph child
  	child_child_2(child_2)
  	child_child_1_grandchild_2 --> child_child_2;
  	subgraph child_1
  	child_child_1_grandchild_1(grandchild_1)
  	child_child_1_grandchild_2(grandchild_2<hr/><small><em>__interrupt = before</em></small>)
  	child_child_1_grandchild_1 --> child_child_1_grandchild_2;
  	end
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_triple_nested_subgraph_mermaid[mermaid]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	parent_1(parent_1)
  	parent_2(parent_2)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> parent_1;
  	child_child_2 --> parent_2;
  	parent_1 --> child_child_1_grandchild_1;
  	parent_2 --> __end__;
  	subgraph child
  	child_child_2(child_2)
  	child_child_1_grandchild_2 --> child_child_2;
  	subgraph child_1
  	child_child_1_grandchild_1(grandchild_1)
  	child_child_1_grandchild_2(grandchild_2<hr/><small><em>__interrupt = before</em></small>)
  	child_child_1_grandchild_1_greatgrandchild --> child_child_1_grandchild_2;
  	subgraph grandchild_1
  	child_child_1_grandchild_1_greatgrandchild(greatgrandchild)
  	child_child_1_grandchild_1 --> child_child_1_grandchild_1_greatgrandchild;
  	end
  	end
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_graph_mermaid_duplicate_nodes[mermaid]
  '''
  graph TD;
  	PromptInput --> PromptTemplate_1;
  	Parallel_llm1_llm2_Input --> FakeListLLM_1;
  	FakeListLLM_1 --> Parallel_llm1_llm2_Output;
  	Parallel_llm1_llm2_Input --> FakeListLLM_2;
  	FakeListLLM_2 --> Parallel_llm1_llm2_Output;
  	PromptTemplate_1 --> Parallel_llm1_llm2_Input;
  	PromptTemplate_2 --> PromptTemplateOutput;
  	Parallel_llm1_llm2_Output --> PromptTemplate_2;
  
  '''
# ---
# name: test_graph_sequence[ascii]
  '''
              +-------------+              
              | PromptInput |              
              +-------------+              
                      *                    
                      *                    
                      *                    
             +----------------+            
             | PromptTemplate |            
             +----------------+            
                      *                    
                      *                    
                      *                    
              +-------------+              
              | FakeListLLM |              
              +-------------+              
                      *                    
                      *                    
                      *                    
     +--------------------------------+    
     | CommaSeparatedListOutputParser |    
     +--------------------------------+    
                      *                    
                      *                    
                      *                    
  +--------------------------------------+ 
  | CommaSeparatedListOutputParserOutput | 
  +--------------------------------------+ 
  '''
# ---
# name: test_graph_sequence[mermaid]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	PromptInput([PromptInput]):::first
  	PromptTemplate(PromptTemplate)
  	FakeListLLM(FakeListLLM<hr/><small><em>key = 2</em></small>)
  	CommaSeparatedListOutputParser(CommaSeparatedListOutputParser)
  	CommaSeparatedListOutputParserOutput([CommaSeparatedListOutputParserOutput]):::last
  	PromptInput --> PromptTemplate;
  	PromptTemplate --> FakeListLLM;
  	CommaSeparatedListOutputParser --> CommaSeparatedListOutputParserOutput;
  	FakeListLLM --> CommaSeparatedListOutputParser;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_graph_sequence_map[ascii]
  '''
                                             +-------------+                                                 
                                             | PromptInput |                                                 
                                             +-------------+                                                 
                                                     *                                                       
                                                     *                                                       
                                                     *                                                       
                                            +----------------+                                               
                                            | PromptTemplate |                                               
                                            +----------------+                                               
                                                     *                                                       
                                                     *                                                       
                                                     *                                                       
                                             +-------------+                                                 
                                             | FakeListLLM |                                                 
                                             +-------------+                                                 
                                                     *                                                       
                                                     *                                                       
                                                     *                                                       
                                    +-------------------------------+                                        
                                    | Parallel<as_list,as_str>Input |                                        
                                    +-------------------------------+                                        
                                       *****                         ******                                  
                                    ***                                    ******                            
                                 ***                                             ******                      
            +------------------------------+                                           ****                  
            | conditional_str_parser_input |                                              *                  
            +------------------------------+                                              *                  
                   ***              ***                                                   *                  
                ***                    ***                                                *                  
              **                          **                                              *                  
  +-----------------+               +-----------------+                                   *                  
  | StrOutputParser |               | XMLOutputParser |                                   *                  
  +-----------------+               +-----------------+                                   *                  
                   ***              ***                                                   *                  
                      ***        ***                                                      *                  
                         **    **                                                         *                  
            +-------------------------------+                            +--------------------------------+  
            | conditional_str_parser_output |                            | CommaSeparatedListOutputParser |  
            +-------------------------------+                            +--------------------------------+  
                                       *****                         ******                                  
                                            ***                ******                                        
                                               ***         ****                                              
                                    +--------------------------------+                                       
                                    | Parallel<as_list,as_str>Output |                                       
                                    +--------------------------------+                                       
  '''
# ---
# name: test_graph_sequence_map[graph_no_schemas]
  dict({
    'edges': list([
      dict({
        'source': 0,
        'target': 1,
      }),
      dict({
        'source': 1,
        'target': 2,
      }),
      dict({
        'source': 3,
        'target': 5,
      }),
      dict({
        'source': 5,
        'target': 4,
      }),
      dict({
        'source': 6,
        'target': 8,
      }),
      dict({
        'source': 8,
        'target': 7,
      }),
      dict({
        'source': 6,
        'target': 9,
      }),
      dict({
        'source': 9,
        'target': 7,
      }),
      dict({
        'source': 3,
        'target': 6,
      }),
      dict({
        'source': 7,
        'target': 4,
      }),
      dict({
        'source': 2,
        'target': 3,
      }),
    ]),
    'nodes': list([
      dict({
        'data': 'PromptInput',
        'id': 0,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'prompts',
            'prompt',
            'PromptTemplate',
          ]),
          'name': 'PromptTemplate',
        }),
        'id': 1,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain_core',
            'language_models',
            'fake',
            'FakeListLLM',
          ]),
          'name': 'FakeListLLM',
        }),
        'id': 2,
        'type': 'runnable',
      }),
      dict({
        'data': 'Parallel<as_list,as_str>Input',
        'id': 3,
        'type': 'schema',
      }),
      dict({
        'data': 'Parallel<as_list,as_str>Output',
        'id': 4,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'output_parsers',
            'list',
            'CommaSeparatedListOutputParser',
          ]),
          'name': 'CommaSeparatedListOutputParser',
        }),
        'id': 5,
        'type': 'runnable',
      }),
      dict({
        'data': 'conditional_str_parser_input',
        'id': 6,
        'type': 'schema',
      }),
      dict({
        'data': 'conditional_str_parser_output',
        'id': 7,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'output_parser',
            'StrOutputParser',
          ]),
          'name': 'StrOutputParser',
        }),
        'id': 8,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain_core',
            'output_parsers',
            'xml',
            'XMLOutputParser',
          ]),
          'name': 'XMLOutputParser',
        }),
        'id': 9,
        'type': 'runnable',
      }),
    ]),
  })
# ---
# name: test_graph_sequence_map[graph_with_schema]
  dict({
    'edges': list([
      dict({
        'source': 0,
        'target': 1,
      }),
      dict({
        'source': 1,
        'target': 2,
      }),
      dict({
        'source': 3,
        'target': 5,
      }),
      dict({
        'source': 5,
        'target': 4,
      }),
      dict({
        'source': 6,
        'target': 8,
      }),
      dict({
        'source': 8,
        'target': 7,
      }),
      dict({
        'source': 6,
        'target': 9,
      }),
      dict({
        'source': 9,
        'target': 7,
      }),
      dict({
        'source': 3,
        'target': 6,
      }),
      dict({
        'source': 7,
        'target': 4,
      }),
      dict({
        'source': 2,
        'target': 3,
      }),
    ]),
    'nodes': list([
      dict({
        'data': dict({
          'properties': dict({
            'name': dict({
              'title': 'Name',
              'type': 'string',
            }),
          }),
          'required': list([
            'name',
          ]),
          'title': 'PromptInput',
          'type': 'object',
        }),
        'id': 0,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'prompts',
            'prompt',
            'PromptTemplate',
          ]),
          'name': 'PromptTemplate',
        }),
        'id': 1,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain_core',
            'language_models',
            'fake',
            'FakeListLLM',
          ]),
          'name': 'FakeListLLM',
        }),
        'id': 2,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          '$defs': dict({
            'AIMessage': dict({
              'additionalProperties': True,
              'description': '''
                Message from an AI.
                
                AIMessage is returned from a chat model as a response to a prompt.
                
                This message represents the output of the model and consists of both
                the raw output as returned by the model together standardized fields
                (e.g., tool calls, usage metadata) added by the LangChain framework.
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'example': dict({
                  'default': False,
                  'title': 'Example',
                  'type': 'boolean',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'invalid_tool_calls': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/InvalidToolCall',
                  }),
                  'title': 'Invalid Tool Calls',
                  'type': 'array',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'tool_calls': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/ToolCall',
                  }),
                  'title': 'Tool Calls',
                  'type': 'array',
                }),
                'type': dict({
                  'const': 'ai',
                  'default': 'ai',
                  'title': 'Type',
                  'type': 'string',
                }),
                'usage_metadata': dict({
                  'anyOf': list([
                    dict({
                      '$ref': '#/$defs/UsageMetadata',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'AIMessage',
              'type': 'object',
            }),
            'AIMessageChunk': dict({
              'additionalProperties': True,
              'description': 'Message chunk from an AI.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'example': dict({
                  'default': False,
                  'title': 'Example',
                  'type': 'boolean',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'invalid_tool_calls': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/InvalidToolCall',
                  }),
                  'title': 'Invalid Tool Calls',
                  'type': 'array',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'tool_call_chunks': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/ToolCallChunk',
                  }),
                  'title': 'Tool Call Chunks',
                  'type': 'array',
                }),
                'tool_calls': dict({
                  'default': list([
                  ]),
                  'items': dict({
                    '$ref': '#/$defs/ToolCall',
                  }),
                  'title': 'Tool Calls',
                  'type': 'array',
                }),
                'type': dict({
                  'const': 'AIMessageChunk',
                  'default': 'AIMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
                'usage_metadata': dict({
                  'anyOf': list([
                    dict({
                      '$ref': '#/$defs/UsageMetadata',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'AIMessageChunk',
              'type': 'object',
            }),
            'ChatMessage': dict({
              'additionalProperties': True,
              'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'role': dict({
                  'title': 'Role',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'chat',
                  'default': 'chat',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'role',
              ]),
              'title': 'ChatMessage',
              'type': 'object',
            }),
            'ChatMessageChunk': dict({
              'additionalProperties': True,
              'description': 'Chat Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'role': dict({
                  'title': 'Role',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'ChatMessageChunk',
                  'default': 'ChatMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'role',
              ]),
              'title': 'ChatMessageChunk',
              'type': 'object',
            }),
            'FunctionMessage': dict({
              'additionalProperties': True,
              'description': '''
                Message for passing the result of executing a tool back to a model.
                
                FunctionMessage are an older version of the ToolMessage schema, and
                do not contain the tool_call_id field.
                
                The tool_call_id field is used to associate the tool call request with the
                tool call response. This is useful in situations where a chat model is able
                to request multiple tool calls in parallel.
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'title': 'Name',
                  'type': 'string',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'function',
                  'default': 'function',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'name',
              ]),
              'title': 'FunctionMessage',
              'type': 'object',
            }),
            'FunctionMessageChunk': dict({
              'additionalProperties': True,
              'description': 'Function Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'title': 'Name',
                  'type': 'string',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'FunctionMessageChunk',
                  'default': 'FunctionMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'name',
              ]),
              'title': 'FunctionMessageChunk',
              'type': 'object',
            }),
            'HumanMessage': dict({
              'additionalProperties': True,
              'description': '''
                Message from a human.
                
                HumanMessages are messages that are passed in from a human to the model.
                
                Example:
                
                    .. code-block:: python
                
                        from langchain_core.messages import HumanMessage, SystemMessage
                
                        messages = [
                            SystemMessage(
                                content="You are a helpful assistant! Your name is Bob."
                            ),
                            HumanMessage(
                                content="What is your name?"
                            )
                        ]
                
                        # Instantiate a chat model and invoke it with the messages
                        model = ...
                        print(model.invoke(messages))
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'example': dict({
                  'default': False,
                  'title': 'Example',
                  'type': 'boolean',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'human',
                  'default': 'human',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'HumanMessage',
              'type': 'object',
            }),
            'HumanMessageChunk': dict({
              'additionalProperties': True,
              'description': 'Human Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'example': dict({
                  'default': False,
                  'title': 'Example',
                  'type': 'boolean',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'HumanMessageChunk',
                  'default': 'HumanMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'HumanMessageChunk',
              'type': 'object',
            }),
            'InputTokenDetails': dict({
              'description': '''
                Breakdown of input token counts.
                
                Does *not* need to sum to full input token count. Does *not* need to have all keys.
                
                Example:
                
                    .. code-block:: python
                
                        {
                            "audio": 10,
                            "cache_creation": 200,
                            "cache_read": 100,
                        }
                
                .. versionadded:: 0.3.9
              ''',
              'properties': dict({
                'audio': dict({
                  'title': 'Audio',
                  'type': 'integer',
                }),
                'cache_creation': dict({
                  'title': 'Cache Creation',
                  'type': 'integer',
                }),
                'cache_read': dict({
                  'title': 'Cache Read',
                  'type': 'integer',
                }),
              }),
              'title': 'InputTokenDetails',
              'type': 'object',
            }),
            'InvalidToolCall': dict({
              'description': '''
                Allowance for errors made by LLM.
                
                Here we add an `error` key to surface errors made during generation
                (e.g., invalid JSON arguments.)
              ''',
              'properties': dict({
                'args': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Args',
                }),
                'error': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Error',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Name',
                }),
                'type': dict({
                  'const': 'invalid_tool_call',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'name',
                'args',
                'id',
                'error',
              ]),
              'title': 'InvalidToolCall',
              'type': 'object',
            }),
            'OutputTokenDetails': dict({
              'description': '''
                Breakdown of output token counts.
                
                Does *not* need to sum to full output token count. Does *not* need to have all keys.
                
                Example:
                
                    .. code-block:: python
                
                        {
                            "audio": 10,
                            "reasoning": 200,
                        }
                
                .. versionadded:: 0.3.9
              ''',
              'properties': dict({
                'audio': dict({
                  'title': 'Audio',
                  'type': 'integer',
                }),
                'reasoning': dict({
                  'title': 'Reasoning',
                  'type': 'integer',
                }),
              }),
              'title': 'OutputTokenDetails',
              'type': 'object',
            }),
            'SystemMessage': dict({
              'additionalProperties': True,
              'description': '''
                Message for priming AI behavior.
                
                The system message is usually passed in as the first of a sequence
                of input messages.
                
                Example:
                
                    .. code-block:: python
                
                        from langchain_core.messages import HumanMessage, SystemMessage
                
                        messages = [
                            SystemMessage(
                                content="You are a helpful assistant! Your name is Bob."
                            ),
                            HumanMessage(
                                content="What is your name?"
                            )
                        ]
                
                        # Define a chat model and invoke it with the messages
                        print(model.invoke(messages))
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'system',
                  'default': 'system',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'SystemMessage',
              'type': 'object',
            }),
            'SystemMessageChunk': dict({
              'additionalProperties': True,
              'description': 'System Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'type': dict({
                  'const': 'SystemMessageChunk',
                  'default': 'SystemMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
              ]),
              'title': 'SystemMessageChunk',
              'type': 'object',
            }),
            'ToolCall': dict({
              'description': '''
                Represents a request to call a tool.
                
                Example:
                
                    .. code-block:: python
                
                        {
                            "name": "foo",
                            "args": {"a": 1},
                            "id": "123"
                        }
                
                    This represents a request to call the tool named "foo" with arguments {"a": 1}
                    and an identifier of "123".
              ''',
              'properties': dict({
                'args': dict({
                  'title': 'Args',
                  'type': 'object',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Id',
                }),
                'name': dict({
                  'title': 'Name',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'tool_call',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'name',
                'args',
                'id',
              ]),
              'title': 'ToolCall',
              'type': 'object',
            }),
            'ToolCallChunk': dict({
              'description': '''
                A chunk of a tool call (e.g., as part of a stream).
                
                When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
                all string attributes are concatenated. Chunks are only merged if their
                values of `index` are equal and not None.
                
                Example:
                
                .. code-block:: python
                
                    left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
                    right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
                
                    (
                        AIMessageChunk(content="", tool_call_chunks=left_chunks)
                        + AIMessageChunk(content="", tool_call_chunks=right_chunks)
                    ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
              ''',
              'properties': dict({
                'args': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Args',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Id',
                }),
                'index': dict({
                  'anyOf': list([
                    dict({
                      'type': 'integer',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Index',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'title': 'Name',
                }),
                'type': dict({
                  'const': 'tool_call_chunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'name',
                'args',
                'id',
                'index',
              ]),
              'title': 'ToolCallChunk',
              'type': 'object',
            }),
            'ToolMessage': dict({
              'additionalProperties': True,
              'description': '''
                Message for passing the result of executing a tool back to a model.
                
                ToolMessages contain the result of a tool invocation. Typically, the result
                is encoded inside the `content` field.
                
                Example: A ToolMessage representing a result of 42 from a tool call with id
                
                    .. code-block:: python
                
                        from langchain_core.messages import ToolMessage
                
                        ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
                
                
                Example: A ToolMessage where only part of the tool output is sent to the model
                    and the full output is passed in to artifact.
                
                    .. versionadded:: 0.2.17
                
                    .. code-block:: python
                
                        from langchain_core.messages import ToolMessage
                
                        tool_output = {
                            "stdout": "From the graph we can see that the correlation between x and y is ...",
                            "stderr": None,
                            "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
                        }
                
                        ToolMessage(
                            content=tool_output["stdout"],
                            artifact=tool_output,
                            tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
                        )
                
                The tool_call_id field is used to associate the tool call request with the
                tool call response. This is useful in situations where a chat model is able
                to request multiple tool calls in parallel.
              ''',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'artifact': dict({
                  'title': 'Artifact',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'status': dict({
                  'default': 'success',
                  'title': 'Status',
                }),
                'tool_call_id': dict({
                  'title': 'Tool Call Id',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'tool',
                  'default': 'tool',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'tool_call_id',
              ]),
              'title': 'ToolMessage',
              'type': 'object',
            }),
            'ToolMessageChunk': dict({
              'additionalProperties': True,
              'description': 'Tool Message chunk.',
              'properties': dict({
                'additional_kwargs': dict({
                  'title': 'Additional Kwargs',
                  'type': 'object',
                }),
                'artifact': dict({
                  'title': 'Artifact',
                }),
                'content': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'items': dict({
                        'anyOf': list([
                          dict({
                            'type': 'string',
                          }),
                          dict({
                            'type': 'object',
                          }),
                        ]),
                      }),
                      'type': 'array',
                    }),
                  ]),
                  'title': 'Content',
                }),
                'id': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Id',
                }),
                'name': dict({
                  'anyOf': list([
                    dict({
                      'type': 'string',
                    }),
                    dict({
                      'type': 'null',
                    }),
                  ]),
                  'default': None,
                  'title': 'Name',
                }),
                'response_metadata': dict({
                  'title': 'Response Metadata',
                  'type': 'object',
                }),
                'status': dict({
                  'default': 'success',
                  'title': 'Status',
                }),
                'tool_call_id': dict({
                  'title': 'Tool Call Id',
                  'type': 'string',
                }),
                'type': dict({
                  'const': 'ToolMessageChunk',
                  'default': 'ToolMessageChunk',
                  'title': 'Type',
                  'type': 'string',
                }),
              }),
              'required': list([
                'content',
                'tool_call_id',
              ]),
              'title': 'ToolMessageChunk',
              'type': 'object',
            }),
            'UsageMetadata': dict({
              'description': '''
                Usage metadata for a message, such as token counts.
                
                This is a standard representation of token usage that is consistent across models.
                
                Example:
                
                    .. code-block:: python
                
                        {
                            "input_tokens": 350,
                            "output_tokens": 240,
                            "total_tokens": 590,
                            "input_token_details": {
                                "audio": 10,
                                "cache_creation": 200,
                                "cache_read": 100,
                            },
                            "output_token_details": {
                                "audio": 10,
                                "reasoning": 200,
                            }
                        }
                
                .. versionchanged:: 0.3.9
                
                    Added ``input_token_details`` and ``output_token_details``.
              ''',
              'properties': dict({
                'input_token_details': dict({
                  '$ref': '#/$defs/InputTokenDetails',
                }),
                'input_tokens': dict({
                  'title': 'Input Tokens',
                  'type': 'integer',
                }),
                'output_token_details': dict({
                  '$ref': '#/$defs/OutputTokenDetails',
                }),
                'output_tokens': dict({
                  'title': 'Output Tokens',
                  'type': 'integer',
                }),
                'total_tokens': dict({
                  'title': 'Total Tokens',
                  'type': 'integer',
                }),
              }),
              'required': list([
                'input_tokens',
                'output_tokens',
                'total_tokens',
              ]),
              'title': 'UsageMetadata',
              'type': 'object',
            }),
          }),
          'anyOf': list([
            dict({
              'type': 'string',
            }),
            dict({
              'oneOf': list([
                dict({
                  '$ref': '#/$defs/AIMessage',
                }),
                dict({
                  '$ref': '#/$defs/HumanMessage',
                }),
                dict({
                  '$ref': '#/$defs/ChatMessage',
                }),
                dict({
                  '$ref': '#/$defs/SystemMessage',
                }),
                dict({
                  '$ref': '#/$defs/FunctionMessage',
                }),
                dict({
                  '$ref': '#/$defs/ToolMessage',
                }),
                dict({
                  '$ref': '#/$defs/AIMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/HumanMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/ChatMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/SystemMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/FunctionMessageChunk',
                }),
                dict({
                  '$ref': '#/$defs/ToolMessageChunk',
                }),
              ]),
            }),
          ]),
          'title': 'RunnableParallel<as_list,as_str>Input',
        }),
        'id': 3,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'properties': dict({
            'as_list': dict({
              'items': dict({
                'type': 'string',
              }),
              'title': 'As List',
              'type': 'array',
            }),
            'as_str': dict({
              'title': 'As Str',
            }),
          }),
          'required': list([
            'as_list',
            'as_str',
          ]),
          'title': 'RunnableParallel<as_list,as_str>Output',
          'type': 'object',
        }),
        'id': 4,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'output_parsers',
            'list',
            'CommaSeparatedListOutputParser',
          ]),
          'name': 'CommaSeparatedListOutputParser',
        }),
        'id': 5,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'title': 'conditional_str_parser_input',
          'type': 'string',
        }),
        'id': 6,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'title': 'conditional_str_parser_output',
        }),
        'id': 7,
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'output_parser',
            'StrOutputParser',
          ]),
          'name': 'StrOutputParser',
        }),
        'id': 8,
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain_core',
            'output_parsers',
            'xml',
            'XMLOutputParser',
          ]),
          'name': 'XMLOutputParser',
        }),
        'id': 9,
        'type': 'runnable',
      }),
    ]),
  })
# ---
# name: test_graph_sequence_map[mermaid-simple]
  '''
  graph TD;
  	PromptInput --> PromptTemplate;
  	PromptTemplate --> FakeListLLM;
  	Parallel_as_list_as_str_Input --> CommaSeparatedListOutputParser;
  	CommaSeparatedListOutputParser --> Parallel_as_list_as_str_Output;
  	conditional_str_parser_input --> StrOutputParser;
  	StrOutputParser --> conditional_str_parser_output;
  	conditional_str_parser_input --> XMLOutputParser;
  	XMLOutputParser --> conditional_str_parser_output;
  	Parallel_as_list_as_str_Input --> conditional_str_parser_input;
  	conditional_str_parser_output --> Parallel_as_list_as_str_Output;
  	FakeListLLM --> Parallel_as_list_as_str_Input;
  
  '''
# ---
# name: test_graph_sequence_map[mermaid]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	PromptInput([PromptInput]):::first
  	PromptTemplate(PromptTemplate)
  	FakeListLLM(FakeListLLM)
  	Parallel_as_list_as_str_Input(Parallel<as_list,as_str>Input)
  	Parallel_as_list_as_str_Output([Parallel<as_list,as_str>Output]):::last
  	CommaSeparatedListOutputParser(CommaSeparatedListOutputParser)
  	conditional_str_parser_input(conditional_str_parser_input)
  	conditional_str_parser_output(conditional_str_parser_output)
  	StrOutputParser(StrOutputParser)
  	XMLOutputParser(XMLOutputParser)
  	PromptInput --> PromptTemplate;
  	PromptTemplate --> FakeListLLM;
  	Parallel_as_list_as_str_Input --> CommaSeparatedListOutputParser;
  	CommaSeparatedListOutputParser --> Parallel_as_list_as_str_Output;
  	conditional_str_parser_input --> StrOutputParser;
  	StrOutputParser --> conditional_str_parser_output;
  	conditional_str_parser_input --> XMLOutputParser;
  	XMLOutputParser --> conditional_str_parser_output;
  	Parallel_as_list_as_str_Input --> conditional_str_parser_input;
  	conditional_str_parser_output --> Parallel_as_list_as_str_Output;
  	FakeListLLM --> Parallel_as_list_as_str_Input;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_graph_single_runnable[ascii]
  '''
  +----------------------+   
  | StrOutputParserInput |   
  +----------------------+   
              *              
              *              
              *              
     +-----------------+     
     | StrOutputParser |     
     +-----------------+     
              *              
              *              
              *              
  +-----------------------+  
  | StrOutputParserOutput |  
  +-----------------------+  
  '''
# ---
# name: test_graph_single_runnable[mermaid]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	StrOutputParserInput([StrOutputParserInput]):::first
  	StrOutputParser(StrOutputParser)
  	StrOutputParserOutput([StrOutputParserOutput]):::last
  	StrOutputParserInput --> StrOutputParser;
  	StrOutputParser --> StrOutputParserOutput;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_parallel_subgraph_mermaid[mermaid]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	outer_1(outer_1)
  	outer_2(outer_2)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> outer_1;
  	inner_1_inner_2 --> outer_2;
  	inner_2_inner_2 --> outer_2;
  	outer_1 --> inner_1_inner_1;
  	outer_1 --> inner_2_inner_1;
  	outer_2 --> __end__;
  	subgraph inner_1
  	inner_1_inner_1(inner_1)
  	inner_1_inner_2(inner_2<hr/><small><em>__interrupt = before</em></small>)
  	inner_1_inner_1 --> inner_1_inner_2;
  	end
  	subgraph inner_2
  	inner_2_inner_1(inner_1)
  	inner_2_inner_2(inner_2)
  	inner_2_inner_1 --> inner_2_inner_2;
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_single_node_subgraph_mermaid[mermaid]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	__end__([<p>__end__</p>]):::last
  	__start__ --> sub_meow;
  	sub_meow --> __end__;
  	subgraph sub
  	sub_meow(meow)
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc
  
  '''
# ---
# name: test_trim
  dict({
    'edges': list([
      dict({
        'source': '__start__',
        'target': 'ask_question',
      }),
      dict({
        'source': 'ask_question',
        'target': 'answer_question',
      }),
      dict({
        'conditional': True,
        'source': 'answer_question',
        'target': 'ask_question',
      }),
      dict({
        'conditional': True,
        'source': 'answer_question',
        'target': '__end__',
      }),
    ]),
    'nodes': list([
      dict({
        'data': '__start__',
        'id': '__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'output_parser',
            'StrOutputParser',
          ]),
          'name': 'ask_question',
        }),
        'id': 'ask_question',
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'output_parser',
            'StrOutputParser',
          ]),
          'name': 'answer_question',
        }),
        'id': 'answer_question',
        'type': 'runnable',
      }),
      dict({
        'data': '__end__',
        'id': '__end__',
        'type': 'schema',
      }),
    ]),
  })
# ---
