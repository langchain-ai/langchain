# v1.1_改动4_支持 SystemMessage + 分块／metadata.md

## 过去

过去，在使用 `create_agent` 时，`system_prompt` 参数只能接受一个字符串（`string`），例如：

```python
agent = create_agent(
    model,
    tools,
    system_prompt="You are a helpful assistant."
)
```

LangChain 会将该字符串作为 system-level 指令（system prompt），直接添加在消息历史开头，形成一条 `system` 角色的消息。

## 新版本（v1.1.0）

现在，`system_prompt` 参数不仅可以接受字符串，也支持直接传递 `SystemMessage` 对象。

```python
from langchain.schema import SystemMessage

system_msg = SystemMessage(content="You are a helpful assistant.")
agent = create_agent(
    model,
    tools,
    system_prompt=system_msg
)
```

更重要的是，借助 `SystemMessage`，你不仅可以传递纯文本系统提示，还可以灵活构造**结构化/分块（content blocks）**的 system prompt。比如，可以将 prompt 拆分为多个 block，每个块可以包含不同的 metadata（如 cache-control、行为 hint、工具调用提示等）。LangChain 会根据这些 blocks 及 metadata，自动拼接出最终传递给 LLM 的实际 prompt 请求。

```py
from langchain.schema import SystemMessage
from langchain.agents import create_agent

# 构造 content_blocks — 多块 (blocks) + metadata (如 cache_control)
blocks = [
    {"type": "text", "text": "You are a top-level data extraction assistant."},
    {"type": "text", "text": "Your task: when user gives unstructured text, always output JSON according to schema XYZ.", "cache_control": {"type": "ephemeral"}},
    {"type": "text", "text": "Do not output any extra commentary."}
]

system_msg = SystemMessage(content=blocks)

agent = create_agent(
    model=model,
    tools=tools,
    system_prompt=system_msg
)

# “cache_control” metadata (譬如 "ephemeral" / “persistent”) 可用于指示某些系统 prompt 块 (block) 内容是否可以缓存 (对于支持 cache 的模型 / provider) —— 这对长 prompt / 大背景特别有用，因为你不用每次都发所有内容
# 多块分割 (blocks) 的 prompt，比把所有内容拼到一个长字符串里更结构化 — 可读性好、易管理、可维护。你可以分别写 背景说明 / 角色设定 / 输出格式 / 规范 / 注意事项 / 工具说明 / … 每块分开

# 对于某些 provider /模型 (例如支持 content-blocks 的 provider)，LangChain 会根据提供者能力 (provider-native) 来把这些 block 转换成该 provider 所需格式 (例如 Anthropic 的 block-based prompt)，从而兼容性更好。
```


## 这个改动带来的新能力

- **Cache control**  
  对于部分 provider/模型（如 Anthropic 等支持该机制者），你可以为 system prompt 的内容块设置 `cache-control` metadata。这使得大型、固定的 system prompt 或 context（如助手背景、说明文档、长期资料等）支持缓存（ephemeral/persistent cache 等机制），避免每次请求重复发送，非常节省 token 和调用成本。这就是所谓的 “cache-control blocks”。

- **Structured content blocks**  
  system prompt 不再局限于单一字符串。你可以将其拆分为多个块（block），每块可为 text / code / markdown / 引用 / 特殊指令等。这样可以更灵活地构造 system prompt，非常适合复杂任务（如分别设定背景说明、角色/行为规范、输出模板等）。LangChain 会根据 provider 是否支持 content_blocks 自动组织这些块，比简单拼接长字符串更清晰、可靠。

- **Richer system-level instructions**  
  借助 content-blocks + metadata + 结构化 prompt，你能传递比单行文本更复杂、更加精细化的系统指令。例如可要求回答中包含引用、sources、reason trace、JSON output、总结、特定格式、遵守规则、注意隐私等。这种方式比传统 string prompt 更具可控性，也便于维护和复用。

- **更好兼容不同 provider/模型**  
  一些 LLM provider（如 Anthropic）原生支持 content-block 风格的 system prompt 及 cache control/message metadata。允许 SystemMessage + content-blocks 后，你可充分利用 provider 的这些高级特性，不受限于基础的字符串 prompt。


## 对项目的意义

- **节省 token & 延迟**：当 system prompt 中包含长期不变的背景上下文（如角色设定、使用规则、文档、任务说明、安全约束、性能规范等），可以使用 `SystemMessage` 结合 cache-control blocks，减少重复传输这些内容。
- **结构化与可维护性**：如果 system prompt 本身比较复杂（如包含背景说明、行为规范、数据规范、输出要求、工具调用说明等多部分），用 content blocks 代替拼接超长字符串会更加清晰易维护，方便变更、复用和版本管理。
- **兼容多 provider**：对于需适配不同 provider（有的支持高级特性、有的基础支持），SystemMessage + blocks 更为灵活、通用。
- **强化系统级指令**：在有结构化输出、工具调用、多阶段流程或复杂 agent 的项目中，这一机制让系统级指令（prompt）更强大、可控，也更利于调试和后期维护。





## 什么是 `cache_control` metadata

在你为大模型（LLM）构造 system prompt（或 message）时，如果其中某条 message 或某个 block 被标记了 `cache_control`，这意味着这一部分内容被视为**可缓存（cacheable）/ 可复用（reuse-friendly）**的前缀（prefix）。

当你使用支持 prompt caching 的模型或 API（如部分 Anthropic 模型）时，这个 metadata 会被后端识别。被标记为 cacheable 的前缀会缓存在服务端或 provider 端（cache store）——下一次请求只要前缀（内容 + 顺序 + metadata）完全一致，后端就能直接复用缓存，而无需重新对这部分 prompt 进行 tokenize/encode/处理。

> **注**：通常只有足够长（token 数量多/内容复杂度高）的 prompt 前缀才会被缓存（需达到模型或服务要求的最小缓存长度）；短小或经常变动的前缀即使加了 `cache_control`，也可能不会被实际缓存。

---

### ✅ `cache_control` 的作用 —— 为何能节省 token 并降低延迟？

假设你的场景是「聊天 / agent / 多轮调用 + 相似 system prompt/背景设定」，实际工作中 system prompt + 背景设定 + 工具定义等部分，往往很久都不会变。

- **传统做法**（无 cache_control）：每次请求都需要把这些内容连同用户输入一起发给模型，导致**每次都要 tokenize + encode＋网络传输**全部内容，浪费 token 并增加延迟。
- **使用 cache_control**：只对“不变/可复用的前缀部分”加上标记，后端会缓存这段 prefix。只要下次请求时前缀内容完全没变（exact match），[LLM 服务提供者 (provider) / 模型 API 服务端]即可**直接复用缓存**，不需重新 tokenize/encode，大幅下降输入 token 数、处理时延和 API 费用。

**形象比喻：**

- **传统方式（无 cache）**：每次都「发送 + 编码」`背景 + 规则 + user 消息`
- **支持 cache_control**：第一次发送「背景 + 规则 + user 消息」并缓存前缀，后续仅需发「user 消息 + 少量指明如何与前缀拼接说明」，省去重复发送背景/规则所产生的 token 和延迟。

```py
import os, requests, json

api_key = os.getenv("ANTHROPIC_API_KEY")
url = "https://api.anthropic.com/v1/messages"

headers = {
    "Content-Type": "application/json",
    "x-api-key": api_key,
    "anthropic-version": "2023-06-01",
}

data = {
    "model": "claude-opus-4-20250514",
    "max_tokens": 1024,
    "system": [
        {
            "type": "text",
            "text": "You are an AI assistant specialized in summarizing documents.",
        },
        {
            "type": "text",
            "text": "<very long static background/document here>",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    "messages": [
        {"role": "user", "content": "Summarize the background document briefly."}
    ]
}

resp = requests.post(url, headers=headers, data=json.dumps(data))
print(resp.json())

```