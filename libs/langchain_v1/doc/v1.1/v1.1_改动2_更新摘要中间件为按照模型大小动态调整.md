# v1.1_改动2_更新摘要中间件为按照模型大小动态调整

## 历史摘要触发

消息数量 (message count)”或“token 数 (absolute token count)

## 现在摘要触发

模型本身的上下文窗口大小 (context window / max input tokens) —— 即读取模型的 .profile 属性 —— 来做动态判断。

- 当对话快要超过模型允许的最大上下文 (context window) 时，就触发摘要

## 新版解释

- 新版 (v1.1.0) 中，middleware 能读取模型的 profile，了解这个模型支持多少上下文 (最大 token / context window 大小)，然后以相对 (fraction of window) 或绝对 (token count) 的方式作为 trigger 条件。这样触发摘要变得模型-敏感 (model-aware)、更加动态与灵活。
- 名词解释：profile 指模型的规格描述（如最大可接受 token 数，即 context window）；fraction 表示历史内容已占最大 context 的比例（如 0.8 表示 80%）；token count 指当前历史消息累计的 token 总数。

- trigger 条件支持多种形式 (fraction / tokens / messages)，以及组合逻辑 (AND / OR) — 因此 summarization 更灵活，不再是写死的 “每 N 条消息就 summarize” 的粗暴规则

- middleware 的行为会根据当前所使用模型 (及其 capacity) 自适应 — 对轻量 model／小 context window，可能更早 summarize；对大模型／大 window，则延迟或减少 summarize

## 为什么这个改动有意义 / 带来了什么好处

- 更准确避免 context overflow：不再依赖经验值 (固定阈值)，而是基于模型 capacity 自动判断何时 summary，减少因为 context 太长导致模型出错或性能下降的风险。

- 兼容多模型 & 多 provider：当你切换模型 (例如从小模型切换到大模型)， middleware 的行为会自动适配 — 不需要手动调整阈值。对于使用多种 LLM / provider 的系统非常有用。

- 更灵活、更健壮：trigger 机制灵活 (fraction / tokens / messages /组合)，适应不同使用场景 (聊天机器人 / 多轮 Agent / 长对话 /短对话)，提升中间件通用性。

- 减少手动维护 / 配置：开发者不必硬编码或试错那些阈值，只需依赖模型 profile + 合适配置，就能让 summarization 按需执行。

## 使用示例

```py
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="openai:gpt-4o",  # 例如 "openai:gpt-4o"
    tools=[ ... ],
    middleware=[
        SummarizationMiddleware(
            model="openai:gpt-4o",
            # 当历史内容达到模型最大 context 的 80% 或消息数达到 50 时触发摘要
            trigger=[("fraction", 0.8), ("messages", 50)],
            keep=("messages", 20),  # 摘要后保留最近 20 条消息
        ),
    ],
)

```

- trigger 可以用 "fraction" — 表示当对话历史占模型最大 context 的 80% 时触发 summarization。
- 也可以用 "tokens" (绝对 token 数) 或 "messages" (消息条数)；如果模型 profile 中没有 context-window 数据，middleware 会退回到这些更传统的条件。 
LangChain 文档
+1
- keep 定义 summarization 后要保留多少最近内容 (messages / tokens / fraction)

> 如果你用的是自定义模型、或 profile 数据不完整，也可以显式给模型指定 profile (如 max_input_tokens) —— 否则 middleware 可能无法正确判断

## 启示

- 构建一个长期对话 agent (或者多人同时对话 /多轮任务)，使用 SummarizationMiddleware + model profile 可以更稳健地管理上下文，不用担心 context overflow
- 对于对成本、性能敏感 (token 数 /模型调用次数) 的场景，可以通过调整 trigger / keep 来精细控制 —— 而且切换模型不需要修改逻辑
- 需要兼容多个模型 (大模型、小模型、本地模型、远端模型)，profile-aware summarization 能让你的代码更具通用性和适应性