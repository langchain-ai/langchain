"""Baseline vulnerability tests - verify models ARE susceptible without middleware.

These tests verify that models trigger tool calls from injection payloads when
middleware is NOT applied. This proves the middleware provides real protection.

A test PASSES if the model IS vulnerable (triggers the target tool).
"""

import pytest

from .conftest import INJECTION_TEST_CASES, check_vulnerability


class TestBaselineOpenAI:
    """Verify OpenAI models are vulnerable without middleware."""

    @pytest.mark.requires("langchain_openai")
    @pytest.mark.parametrize("payload,tools,_tool_name,target_tools", INJECTION_TEST_CASES)
    def test_vulnerability(
        self, openai_model, payload, tools, _tool_name, target_tools
    ):
        vulnerable, triggered = check_vulnerability(openai_model, tools, payload, target_tools)
        print(f"\n{openai_model.model_name}: vulnerable={vulnerable}, triggered={triggered}")
        assert vulnerable, f"{openai_model.model_name} not vulnerable to this attack"


class TestBaselineAnthropic:
    """Verify Anthropic models are vulnerable without middleware."""

    @pytest.mark.requires("langchain_anthropic")
    @pytest.mark.parametrize("payload,tools,_tool_name,target_tools", INJECTION_TEST_CASES)
    def test_vulnerability(
        self, anthropic_model, payload, tools, _tool_name, target_tools
    ):
        vulnerable, triggered = check_vulnerability(anthropic_model, tools, payload, target_tools)
        print(f"\n{anthropic_model.model}: vulnerable={vulnerable}, triggered={triggered}")
        assert vulnerable, f"{anthropic_model.model} not vulnerable to this attack"


class TestBaselineOllama:
    """Verify Ollama models are vulnerable without middleware."""

    @pytest.mark.requires("langchain_ollama")
    @pytest.mark.parametrize("payload,tools,_tool_name,target_tools", INJECTION_TEST_CASES)
    def test_vulnerability(
        self, ollama_model, payload, tools, _tool_name, target_tools
    ):
        vulnerable, triggered = check_vulnerability(ollama_model, tools, payload, target_tools)
        print(f"\n{ollama_model.model}: vulnerable={vulnerable}, triggered={triggered}")
        assert vulnerable, f"{ollama_model.model} not vulnerable to this attack"
