"""Wrapper around Google VertexAI models."""
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, root_validator

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens
from langchain.utilities.vertexai import (
    init_vertexai,
    is_tuned_model,
    raise_vertex_import_error,
)


class _VertexAICommon(BaseModel):
    client: Any = None  #: :meta private:
    model_name: Optional[str] = "text-bison"
    """Model name to use."""
    temperature: float = 0.7
    """Sampling temperature, it controls the degree of randomness in token selection."""
    max_output_tokens: int = 256
    "Token limit determines the maximum amount of text output from one prompt."
    top_p: float = 1
    "Tokens are selected from most probable to least until the sum of their "
    "probabilities equals the top-p value."
    top_k: int = 40
    """How the model selects tokens for output, the next token is selected from "
    "among the top-k most probable tokens."""
    project: Optional[str] = None
    """The default GCP project to use when making Vertex API calls."""
    location: Optional[str] = None
    """The default location to use when making API calls."""
    credentials_json_path: Optional[str] = None
    """The default staging bucket to use to stage artifacts when making API calls."""

    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling OpenAI API."""
        base_params = {
            "temperature": self.temperature,
            "max_output_tokens": self.max_output_tokens,
            "top_k": self.top_p,
            "top_p": self.top_k,
        }
        return {**base_params}

    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:
        res = self.client.predict(prompt, **self._default_params)
        return self._enforce_stop_words(res.text, stop)

    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:
        if stop:
            return enforce_stop_tokens(text, stop)
        return text

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "vertexai"

    @classmethod
    def _try_init_vertexai(cls, values: Dict) -> None:
        allowed_params = ["project", "location", "credentials_json_path"]
        params = {k: v for k, v in values.items() if v in allowed_params}
        init_vertexai(**params)
        return None


class VertexAI(_VertexAICommon, LLM):
    """Wrapper around Google Vertex AI large language models."""

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that the python package exists in environment."""
        cls._try_init_vertexai(values)
        try:
            from vertexai.preview.language_models import TextGenerationModel
        except ImportError:
            raise_vertex_import_error()
        is_tuned = is_tuned_model(values["model_name"])
        if is_tuned:
            values["client"] = TextGenerationModel.get_tuned_model(values["model_name"])
        else:
            values["client"] = TextGenerationModel.from_pretrained(values["model_name"])
        return values

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
    ) -> str:
        """Call out to Vertex AI's create endpoint.
        Args:
            prompt: The prompt to pass into the model.
            stop: A list of stop words (optional).
        Returns:
            The string generated by the model.
        """
        return self._predict(prompt, stop)
